# 1 自定义InputFormat合并小文件

## 1.1 需求
无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案

## 1.2 分析
小文件的优化无非以下几种方式：
- 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS
- 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并
- 在mapreduce处理时，可采用combineInputFormat提高效率

## 1.3 实现
本节实现的是上述第二种方式

程序的核心机制：
- 自定义一个InputFormat
- 改写RecordReader，实现一次读取一个完整文件封装为KV
- 在输出时使用SequenceFileOutPutFormat输出合并文件

### 1.3.1 自定义InputFromat
``` 
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import java.io.IOException;
public class UnionFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {
    @Override
    public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
        UnionFileRecordReader recordReader = new UnionFileRecordReader();
        recordReader.initialize(inputSplit,taskAttemptContext);
        return recordReader;
    }
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {// 小文件不用切割文件了
        return false;
    }
}
```

### 1.3.2 自定义RecordReader
``` 
import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import java.io.IOException;
public class UnionFileRecordReader  extends RecordReader<NullWritable, BytesWritable> {
    private Configuration configuration = null;
    private FileSplit fileSplit = null;
    private BytesWritable bytesWritable = new BytesWritable();
    private boolean processed = false;
    FileSystem fileSystem = null;
    FSDataInputStream inputStream = null;
    @Override
    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
        configuration = taskAttemptContext.getConfiguration();
        fileSplit = (FileSplit) inputSplit;
    }
    @Override
    public boolean nextKeyValue() throws IOException, InterruptedException {
        if(!processed){
            fileSystem = FileSystem.get(configuration);
            inputStream = fileSystem.open(fileSplit.getPath());
            int length = (int) fileSplit.getLength();
            byte[] bytes = new byte[length];
            IOUtils.read(inputStream,bytes);
            bytesWritable.set(bytes,0,length);
            processed = true;
            return true;//不直接return会无限循环
        }
        return false;
    }
    @Override
    public NullWritable getCurrentKey() throws IOException, InterruptedException {
        return NullWritable.get();
    }
    @Override
    public BytesWritable getCurrentValue() throws IOException, InterruptedException {
        return bytesWritable;
    }
    @Override
    public float getProgress() throws IOException, InterruptedException {
        return 0;
    }
    @Override
    public void close() throws IOException {
        inputStream.close();
        fileSystem.close();
    }
}
```

### 1.3.3 Mapper类
``` 
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import java.io.IOException;
public class UnionFileMapper extends Mapper<NullWritable, BytesWritable, Text,BytesWritable> {
    @Override
    protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException {
        FileSplit fileSplit = (FileSplit) context.getInputSplit();
        String filename = fileSplit.getPath().getName();
        context.write(new Text(filename),value);
    }
}
```

### 1.3.4 主类
``` 
import com.main.constant.HadoopConf;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
public class UnionFileMain extends Configured implements Tool {
    @Override
    public int run(String[] strings) throws Exception {
        Job job = Job.getInstance(super.getConf(), UnionFileMain.class.getSimpleName());
        job.setInputFormatClass(UnionFileInputFormat.class);
        UnionFileInputFormat.addInputPath(job,new Path(HadoopConf.FILE_PATH+"/file/input/in"));
        job.setMapperClass(UnionFileMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(BytesWritable.class);
        job.setOutputKeyClass(Text.class);// 没有reducer也要设置
        job.setOutputValueClass(BytesWritable.class);
        job.setOutputFormatClass(SequenceFileOutputFormat.class);
        SequenceFileOutputFormat.setOutputPath(job,new Path(HadoopConf.FILE_PATH+"/file/input/out"));
        boolean b = job.waitForCompletion(true);
        return b?0:1;
    }
    public static void main(String[] args) throws Exception {
        int run = ToolRunner.run(new Configuration(), new UnionFileMain(), args);
        System.exit(run);
    }
}
```

