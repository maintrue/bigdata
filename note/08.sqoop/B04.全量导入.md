# 1 全量导入

> 注意：
> - CommunicationsException: Communications link failure
> - 如果报这个错可能是mysql的连接问题

## 1.1 全量导入mysql表数据到HDFS

下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。
``` 
bin/sqoop import \
--connect jdbc:mysql://localhost:3306/userdb \
--username root \
--password root \
--delete-target-dir \
--target-dir /sqoopresult \
--table emp --m 1
```


> 注意事项：
> - --target-dir可以用来指定导出数据存放至HDFS的目录
> - 如果HDFS中目录不存指定的目录HDFS会自己创建
> - 如果指定的目录存在并且有文件会覆盖原有的文件
> - 在HDFS上默认用逗号分隔
> - --fields-terminated-by '\t'来指定分隔符


## 1.2 全量导入mysql表数据到HIVE

1 将关系型数据的表结构复制到hive中
``` 
bin/sqoop create-hive-table \
--connect jdbc:mysql://localhost:3306/tags_dat \
--table tbl_goods \
--username root \
--password root \
--hive-table tags_data.tbl_goods
```

> 注意事项
> - hive中数据库不存在的话会报错：Hive exited with status 88
> - hive中数据库中的表不存在的话会创建
> - 只是复制表结构
> - 要给hive的家目录开放权限要不会报错


2 再导入数据
``` 
bin/sqoop import \
--connect jdbc:mysql://localhost:3306/tags_dat \
--username root \
--password root \
--table tbl_goods \
--hive-table tags_data.tbl_goods \
--hive-import \
--m 1
```


>注意事项：重复导入会追加数据


3 直接复制表结构和数据到hive中
``` 
bin/sqoop import \
--connect jdbc:mysql://localhost:3306/userdb \
--username root \
--password root \
--table emp_conn \
--hive-import \
--m 1 \
--hive-database myhive;
```

> 注意事项：重复导入会追加数据

