# 1 将数据集中的 IP 转为地域信息

## 1.1 IP 转换工具介绍

### 1.1.1 进行 IP 转换的一些办法
一般第三方库会有一些数据结构上的优化, 查找速度比二分法会更快一些, 例如 BTree 就特别适合做索引, 常见的方式有
- GeoLite
- 纯真数据库
- ip2region

<table class="tableblock frame-all grid-all stretch">
    <colgroup>
        <col style="width: 25%;">
        <col style="width: 25%;">
        <col style="width: 25%;">
        <col style="width: 25%;">
    </colgroup>
    <thead>
        <tr>
            <th class="tableblock halign-left valign-top">工具</th>
            <th class="tableblock halign-left valign-top">数据结构支持</th>
            <th class="tableblock halign-left valign-top">中文支持</th>
            <th class="tableblock halign-left valign-top">经纬度支持</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>GeoLite</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">有</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">无</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">有</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">纯真</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">无</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">有</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">无</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>ip2region</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">有</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">有</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">无</p>
            </td>
        </tr>
    </tbody>
</table>

### 1.1.2 ip2region 查找省市名称
1.引入 ip2region
- 复制 IP 数据集 ip2region.db 到工程下的 dataset 目录
- 在 Maven 中增加如下代码
``` 
<dependency>
    <groupId>org.lionsoul</groupId>
    <artifactId>ip2region</artifactId>
    <version>1.7.2</version>
</dependency>
```

2.ip2region 的使用
``` 
val searcher = new DbSearcher(new DbConfig(), "dataset/ip2region.db")
val data = searcher.btreeSearch(ip)
println(data.getRegion)
```

### 1.1.3 选用 GeoLite 确定经纬度
1.引入 GeoLite
- 将 GeoLite 的数据集 GeoLiteCity.dat 拷贝到工程中 dataset 目录下
- 在 pom.xml 中添加如下依赖
```
<dependency>
    <groupId>com.maxmind.geoip</groupId>
    <artifactId>geoip-api</artifactId>
    <version>1.3.0</version>
</dependency>

<dependency>
    <groupId>com.maxmind.geoip2</groupId>
    <artifactId>geoip2</artifactId>
    <version>2.12.0</version>
</dependency>
```

2.GeoLite 的使用方式
``` 
val lookupService = new LookupService("dataset/GeoLiteCity.dat", LookupService.GEOIP_MEMORY_CACHE)
val location = lookupService.getLocation("121.76.98.134")
println(location.latitude, location.longitude)
```

## 1.2 挑战和结构

### 1.2.1 实现步骤

1.读取数据集 pmt.json  
2.将 IP 转换为省市  
3.设计 Kudu 表结构, 创建 Kudu 表  
4.存入 Kudu 表

### 1.2.2 挑战和结构
现在的任务本质上是把一个 数据集 A 转为 数据集 B, 数据集 A 可能不够好, 数据集 B 相对较好, 然后把 数据集 B 落地到 Kudu, 作为 ODS 层, 以供其它功能使用

但是如果 数据集 A 转为 数据集 B 的过程中需要多种转换呢?

所以, 从面向对象的角度上来说, 需要一套机制, 能够组织不同的功能协同运行

所以我们可以使用一个名为 PmtETLRunner 的类代表针对 数据集 A 到 数据集 B 的转换, 然后抽象出单位更小的负责具体某一个转换步骤的节点, 集成到 PmtETLRunner 中, 共同完成任务

## 1.3 参数配置
1.为了让程序行为更可控制, 所以一般会在编写程序之前先大致计划以下程序中可能使用到的一些参数
- Kudu 的表名
- Kudu 表的复制因子
- ODS 层的表名

2.规划好以后, 着手创建配置文件 resource/kudu.conf
``` 
# Server properties
kudu.common.master="192.168.169.101:7051,192.168.169.102:7051,192.168.169.103:7051"
kudu.common.factor=1

# Table name
kudu.name.pmt_ods="ODS_"
```

## 1.4 Kudu 的工具
为了方便 Kudu 的使用, 所以要创建一个 Kudu 的 Helper, 大致需求如下
<table class="tableblock frame-all grid-all stretch">
    <colgroup>
        <col style="width: 33.3333%;">
        <col style="width: 33.3333%;">
        <col style="width: 33.3334%;">
    </colgroup>
    <thead>
        <tr>
            <th class="tableblock halign-left valign-top">需求</th>
            <th class="tableblock halign-left valign-top">原始调用方式</th>
            <th class="tableblock halign-left valign-top">理想调用方式</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">创建表</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>KuduContext.createTable()</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>SparkSession.createKuduTable()</code></p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">读取表</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">
                    <code>spark.read.option("kudu.master", KUDU_MASTERS).option("kudu.table", tableName).kudu</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>SparkSession.readKuduTable(tableName)</code></p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">通过 <code>DataFrame</code> 将数据保存到 <code>Kudu</code> 表</p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>DataFrame.write.options(…​).kudu</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>DataFrame.saveAsKuduTable</code></p>
            </td>
        </tr>
    </tbody>
</table>

### 1.4.1 KuduHelper 的设计
KuduHelper 的设计思路两句话可以总结
- 尽可能的不在处理类中读取配置文件
- 尽可能的提供易于调用的接口

``` 
import com.typesafe.config.ConfigFactory
import org.apache.commons.lang3.StringUtils
import org.apache.kudu.Schema
import org.apache.kudu.client.CreateTableOptions
import org.apache.kudu.spark.kudu.KuduContext
import org.apache.spark.sql.{DataFrame, Dataset, SaveMode, SparkSession}

// 主题设计思路就是将 SparkSession 或者 DataFrame 隐式转换为 KuduHelper, 在 KuduHelper 中提供帮助方法
class KuduHelper {
  private var spark: SparkSession = _
  private var dataset: Dataset[_] = _
  private val config = ConfigFactory.load("kudu")
  private val KUDU_MASTERS = config.getString("kudu.common.master")

  // 在 Helper 内部读取配置文件, 创建 KuduContext
  private var kuduContext: KuduContext = _

  // 将 SparkSession 转为 KuduHelper 时调用
  def this(spark: SparkSession) = {
    this()
    this.spark = spark
    // 可以设置超时时间
    this.kuduContext = new KuduContext(KUDU_MASTERS, spark.sparkContext,Some(900000))
  }

  // 将 Dataset 转为 KuduHelper 时调用
  def this(dataset: Dataset[_]) = {
    this(dataset.sparkSession)
    this.dataset = dataset
  }

  def createKuduTable(tableName: String, schema: Schema,
                      // 此方法就是设计目标 SparkSession.createKuduTable(tableName) 中被调用的方法
                      partitionKey: Seq[String]): Unit = {
    if (kuduContext.tableExists(tableName)) {
      throw new RuntimeException("kuduContext.tableExists is true, Please check.")
//      kuduContext.deleteTable(tableName)
    }

    import scala.collection.JavaConverters._
    val options = new CreateTableOptions()
      .setNumReplicas(config.getInt("kudu.common.factor"))
      .addHashPartitions(partitionKey.asJava, 6)

    kuduContext.createTable(tableName, schema, options)
  }

  // 此方法就是设计目标 DataFrame.saveToKudu(tableName) 中被调用的方法
  def saveToKudu(tableName: String): Unit = {
    if (dataset == null) {
      throw new RuntimeException("请在 DataFrame 上调用 saveToKudu")
    }

    import org.apache.kudu.spark.kudu._

    dataset.write
      .option("kudu.table", tableName)
      .option("kudu.master", KUDU_MASTERS)
      .mode(SaveMode.Append)
      .kudu
  }

  // 此方法就是设计目标 SparkSession.readKuduTable(tableName) 中被调用的方法
  def readKuduTable(tableName: String): Option[DataFrame] = {
    if (StringUtils.isBlank(tableName)) {
      throw new RuntimeException("请传入合法的表名")
    }

    import org.apache.kudu.spark.kudu._

    if (kuduContext.tableExists(tableName)) {
      val dataFrame = spark.read
        .option("kudu.master", KUDU_MASTERS)
        .option("kudu.table", tableName)
        .kudu

      Some(dataFrame)
    } else {
      None
    }
  }

}

object KuduHelper {

  // 将 SparkSession 转为 KuduHelper
  implicit def sparkToKuduContext(spark: SparkSession): KuduHelper = {
    new KuduHelper(spark)
  }

  // 将 Dataset 转为 KuduHelper
  implicit def datasetToKuduContext(dataset: Dataset[_]): KuduHelper = {
    new KuduHelper(dataset)
  }

}
```

## 1.5 ETL 代码编写

### 1.5.1 创建 PmtETLRunner 类
PmtETLRunner 类负责整个 ETL 过程, 但是不复杂中间过程中具体的数据处理, 具体数据如何转换, 要做什么事情由具体的某个 Converter 类负责
```
object PmtETLRunner {

  def main(args: Array[String]): Unit = {
    import com.itheima.dmp.utils.SparkConfigHelper._
    import com.itheima.dmp.utils.KuduHelper._

    // 创建 SparkSession
    val spark = SparkSession.builder()
      .master("local[6]")
      .appName("pmt_etl")
      .loadConfig()
      .getOrCreate()

    import spark.implicits._

    // 读取数据
    val originDataset = spark.read.json("dataset/pmt.json")

  }
}
```

### 1.5.2 创建 IPConverter 类处理 IP 转换的问题
IPConverter 主要解决如下问题
- 原始数据集中有一个 ip 列, 要把 ip 这一列数据转为五列, 分别是 ip, Longitude, latitude, region, city, 从而扩充省市信息和经纬度信息
- 将新创建的四列数据添加到原数据集中
``` 
object IPConverter {

  def process(dataset: Dataset[Row]): Dataset[Row] = {
    val dataConverted: RDD[Row] = dataset
      .rdd
      // 通过 mapPartitions 算子, 对每一个分区数据调用 convertIPtoLocation 进行转换, 需要注意的是, 这个地方已经被转为 RDD 了, 而不是 DataFrame, 因为 DataFrame 在转换中不能更改 Schema
      .mapPartitions(convertIPtoLocation)                                  

    val schema = dataset.schema
      .add("region", StringType)
      .add("city", StringType)
      .add("longitude", DoubleType)
      .add("latitude", DoubleType)

    // 新的 row 对象的 RDD 结合 被扩充过的 Schema, 合并生成新的 DataFrame 返回给 Processor
    val completeDataFrame = dataset
      .sparkSession
      .createDataFrame(dataConverted, schema)                              

    completeDataFrame
  }

  // convertIPtoLocation 主要做的事情是扩充原来的 row, 增加四个新的列
  def convertIPtoLocation(iterator: Iterator[Row]): Iterator[Row] = {      
    val searcher = new DbSearcher(new DbConfig(), "dataset/ip2region.db")

    val lookupService = new LookupService(
      "dataset/GeoLiteCity.dat",
      LookupService.GEOIP_MEMORY_CACHE)

    iterator.map(row => {
      val ip = row.getAs[String]("ip")
      //  获取省市中文名
      val regionData = searcher.btreeSearch(ip).getRegion.split("\\|")     
      val region = regionData(2)
      val city = regionData(3)

      // 获取经纬度
      val location = lookupService.getLocation(ip)                         
      val longitude = location.longitude.toDouble
      val latitude = location.latitude.toDouble

      // 根据原始 row 生成新的 row, 新的 row 中包含了省市和经纬度信息
      val rowSeq = row.toSeq :+ region :+ city :+ longitude :+ latitude
      Row.fromSeq(rowSeq)                                                  
    })
  }
}
```

### 1.5.3 在 PmtETLRunner 中调用 IPConverter
``` 
object PmtETLRunner {

  def main(args: Array[String]): Unit = {
    ...

    val originDataset = spark.read.json("dataset/pmt.json")

    // 调用 IPConverter, 传入 originDataset, 生成包含经纬度和省市的 DataFrame
    val ipConvertedResult = IPConverter.process(originDataset)

    // 要 Select 的列们, 用于组织要包含的结果集中的数据
    // 因为太多, 不再此处展示, 若要查看, 请移步代码工程
    val selectRows: Seq[Column] = Seq('sessionid, 'advertisersid, 'adorderid, 'adcreativeid, 'adplatformproviderid,
      'sdkversion, 'adplatformkey, 'putinmodeltype, 'requestmode, 'adprice, 'adppprice,
      'requestdate, 'ip, 'appid, 'appname, 'uuid, 'device, 'client, 'osversion, 'density,
      'pw, 'ph, 'longitude, 'latitude, 'region, 'city, 'ispid, 'ispname, 'networkmannerid,
      'networkmannername, 'iseffective, 'isbilling, 'adspacetype, 'adspacetypename,
      'devicetype, 'processnode, 'apptype, 'district, 'paymode, 'isbid, 'bidprice, 'winprice,
      'iswin, 'cur, 'rate, 'cnywinprice, 'imei, 'mac, 'idfa, 'openudid, 'androidid,
      'rtbprovince, 'rtbcity, 'rtbdistrict, 'rtbstreet, 'storeurl, 'realip, 'isqualityapp,
      'bidfloor, 'aw, 'ah, 'imeimd5, 'macmd5, 'idfamd5, 'openudidmd5, 'androididmd5,
      'imeisha1, 'macsha1, 'idfasha1, 'openudidsha1, 'androididsha1, 'uuidunknow, 'userid,
      'reqdate, 'reqhour, 'iptype, 'initbidprice, 'adpayment, 'agentrate, 'lomarkrate,
      'adxrate, 'title, 'keywords, 'tagid, 'callbackdate, 'channelid, 'mediatype, 'email,
      'tel, 'age, 'sex)

    // 选中相应的列
    ipConvertedResult.select(selectRows:_*).show()
  }
```

### 1.5.4 将已经处理好的数据存入 Kudu 表中
1.思路
- 创建 Kudu 表 (需要表名)
- 向 Kudu 表中插入数据 (需要表名和数据)

2.在 KuduHelper 伴生对象中增加日期获取方法
``` 
object KuduHelper {

  ...

  def formattedDate(): String = {
    FastDateFormat.getInstance("yyyyMMdd").format(new Date)
  }
}
```

3.在 PmtETLRunner 中获取结果并存储到 Kudu 中
``` 
object PmtETLRunner {
  val ODS_TABLE_NAME: String = "ODS_" + KuduHelper.formattedDate()

  def main(args: Array[String]): Unit = {

    ...

    val selectRows: Seq[Column] = Seq(
      ...
    )

    // 创建 Kudu 表
    spark.createKuduTable(ODS_TABLE_NAME, schema, Seq("uuid"))
    // 将 DataFrame 的数据落地到 Kudu
    ipConvertedResult.select(selectRows:_*).saveToKudu(ODS_TABLE_NAME)
  }

  import scala.collection.JavaConverters._

  lazy val schema = new Schema(List(
    ...
  ))

}
```