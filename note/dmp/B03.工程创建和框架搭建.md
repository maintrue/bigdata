# 1 工程创建和框架搭建

## 1.1 创建工程

### 1.1.1 创建步骤
已经到最后一个阶段了, 不再详细说工程如何创建了, 看一下步骤即可
1. 在 IDEA 创建 Maven 工程, 选择存储位置
2. 工程命名为 dmp, 注意工程名一般小写, 大家也可以采用自己喜欢的命名方式, 在公司里, 要采用公司的习惯
3. 导入 Maven 依赖
4. 创建 Scala 代码目录
5. 创建对应的包们

### 1.1.2 创建需要的包
<table class="tableblock frame-all grid-all stretch">
    <colgroup>
        <col style="width: 30%;">
        <col>
    </colgroup>
    <thead>
        <tr>
            <th class="tableblock halign-left valign-top">包名</th>
            <th class="tableblock halign-left valign-top">描述</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>com.itheima.dmp.etl</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">放置数据转换任务</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>com.itheima.dmp.report</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">放置报表任务</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>com.itheima.dmp.tags</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">放置标签有关的任务</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>com.itheima.dmp.utils</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">放置一些通用公用的类</p>
            </td>
        </tr>
    </tbody>
</table>

### 1.1.3 导入需要的依赖
在 DMP 中, 暂时先不提供完整的 Maven POM 文件, 在一开始只导入必备的, 随着项目的进程, 用到什么再导入什么, 以下是必备的
- Spark 全家桶
- Kudu 一套
- Scala 依赖
- SLF4J 日志依赖
- Junit 单元测试
- Java 编译插件
- Scala 编译插件
- Uber Jar 编译插件 Shade
- CDH Repo 仓库, 需要一个 CDH 的 Maven 仓库配置是因为用到 CDH 版本的 Kudu

```
<properties>
    <scala.version>2.11.8</scala.version>
    <spark.version>2.2.0</spark.version>
    <hadoopo.version>2.6.1</hadoopo.version>
    <kudu.version>1.7.0-cdh5.16.0</kudu.version>
    <maven.version>3.5.1</maven.version>
    <junit.version>4.12</junit.version>
</properties>

<dependencies>
    <!-- Spark -->
    <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>${scala.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.11</artifactId>
        <version>${spark.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.11</artifactId>
        <version>${spark.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_2.11</artifactId>
        <version>${spark.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoopo.version}</version>
    </dependency>

    <!-- Kudu client -->
    <dependency>
        <groupId>org.apache.kudu</groupId>
        <artifactId>kudu-client</artifactId>
        <version>1.7.0-cdh5.16.1</version>
    </dependency>

    <!-- Kudu Spark -->
    <dependency>
        <groupId>org.apache.kudu</groupId>
        <artifactId>kudu-spark2_2.11</artifactId>
        <version>1.7.0-cdh5.16.1</version>
    </dependency>

    <!-- Logging -->
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>1.7.12</version>
    </dependency>

    <!-- Unit testing -->
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>${junit.version}</version>
        <scope>provided</scope>
    </dependency>
</dependencies>

<build>
    <sourceDirectory>src/main/scala</sourceDirectory>
    <testSourceDirectory>src/test/scala</testSourceDirectory>

    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>${maven.version}</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>

        <plugin>
            <groupId>net.alchim31.maven</groupId>
            <artifactId>scala-maven-plugin</artifactId>
            <version>3.2.0</version>
            <executions>
                <execution>
                    <goals>
                        <goal>compile</goal>
                        <goal>testCompile</goal>
                    </goals>
                    <configuration>
                        <args>
                            <arg>-dependencyfile</arg>
                            <arg>${project.build.directory}/.scala_dependencies</arg>
                        </args>
                    </configuration>
                </execution>
            </executions>
        </plugin>

        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>2.4</version>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

<repositories>
    <repository>
        <id>cdh.repo</id>
        <name>Cloudera Repositories</name>
        <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
        <snapshots>
            <enabled>false</enabled>
        </snapshots>
    </repository>
</repositories>
```

## 1.2 框架搭建
编写 Spark 程序的时候, 往往不需要一个非常复杂的框架, 只是对一些基础内容的抽象和封装即可, 但是也要考虑如下问题

### 1.2.1 有哪些任务是要执行的
在做一个项目的时候, 尽量从全局的角度去看, 要考虑到周边的一些环境, 例如说回答自己如下几个问题

1.这个应用有几个入口?
- 这个程序的入口数量是不确定的, 随着工作的进展而变化, 但是至少要有两个入口, 一个是生成报表数据, 一个是处理用户的标签数据

2.这个应用会放在什么地方执行?
- 分为测试和生产, 测试可以直接使用 IDEA 执行, 生成需要打包并发送到集群执行

3.这个应用如何调度?
- 这个应用包含了不止一个任务, 最终会由 Oozie, Azkaban, AirFlow 等工具去调度执行

### 1.2.2 有哪些操作可能会导致重复代码过多
其实无论是 Spring, 还是 Vue, 还是 Spark, 这些框架和工具, 最终的目的都是帮助我们消除一些重复的和通用的代码

所以既然我们无需在 Spark 的应用中搭建复杂的项目框架, 但是对于重复的代码还是要消除的, 初步来看可能会有如下重复的代码点
- 各个数据库的访问
- 配置的读取

## 1.3 建立配置读取工具

### 1.3.1 了解配置文件和读取框架
数据读取部分, 有一个比较好用的工具, 叫做 lightbend/config, 它可以读取一种叫做 HOCON 的配置文件

1.HOCON 全称叫做 Human-Optimized Config Object Notation, 翻译过来叫做 为人类优化的配置对象表示法
- HOCON 是一种类似于 Properties 的配置文件格式, 并包含 JSON 的语法格式, 比较易于使用, 其大致格式如下
```
foo: {
  bar: 10,
  baz: 12
}

foo {
  bar = 10,
  baz = 12
}

foo.bar=10
foo.baz=10

以上三种写法是等价的, 其解析结果都是两个字段, 分别叫做 foo.bar 和 foo.baz
```
2.读取 HOCON 文件格式需要使用 lightbend/config, 它的使用非常的简单
- 当配置文件被命名为 application.conf 并且被放置于 resources 时, 可以使用如下方式直接加载
```
val config: Config = ConfigFactory.load()
val bar = config.getInt("foo.bar")
val baz = config.getInt("foo.baz")
```

### 1.3.2 创建配置文件
创建配置文件 resource/spark.conf, 并引入如下内容
```
# Worker 心跳超时时间
spark.worker.timeout="500"

# RPC 请求等待结果的超时时间
spark.rpc.askTimeout="600s"

# 所有网络操作的等待时间, spark.rpc.askTimeout 默认值等同于这个参数
spark.network.timeoout="600s"

# 最大使用的 CPU 核心数
spark.cores.max="10"

# 任务最大允许失败次数
spark.task.maxFailures="5"

# 如果开启推测执行, 开启会尽可能的增快任务执行效率, 但是会占用额外的运算资源
spark.speculation="true"

# Driver 是否允许多个 Context
spark.driver.allowMutilpleContext="true"

# Spark 序列化的方式, 使用 Kryo 能提升序列化和反序列化的性能
spark.serializer="org.apache.spark.serializer.KryoSerializer"

# 每个页缓存, Page 指的是操作系统的内存分配策略中的 Page, 一个 Page 代表一组连续的内存空间
# Spark 在引入钨丝计划以后, 使用 Java 的 Unsafe API 直接申请内存, 其申请单位就是 Page
# 如果 Page 过大, 有可能因为操作系统的策略无法分配而拒绝这次内存申请, 从而报错
# 简单来说, 这个配置的作用是一次申请的内存大小
spark.buffer.pageSize="6m"
```

以上的配置列成表如下

<table class="tableblock frame-all grid-all stretch">
    <colgroup>
        <col style="width: 50%;">
        <col style="width: 50%;">
    </colgroup>
    <thead>
        <tr>
            <th class="tableblock halign-left valign-top">配置项目</th>
            <th class="tableblock halign-left valign-top">描述</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.worker.timeout</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">如果超过了这个配置项指定的时间, <code>Master</code> 认为 <code>Worker</code> 已经跪了</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.network.timeout</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">因为 <code>Spark</code> 管理一整个集群, 任务可能运行在不同的节点上, 后通过网络进行通信, 一次网络通信有可能因为要访问的节点实效而一直等待,
                    这个配置项所配置的便是这个等待的超时时间</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.cores.max</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>Spark</code> 整个应用最大能够申请的 <code>CPU</code> 核心数</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.task.maxFailures</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>Spark</code> 本身是支持弹性容错的, 所以不能因为某一个 <code>Task</code> 失败了, 就认定整个
                    <code>Job</code> 失败, 一般会因为相当一部分 <code>Task</code> 失败了才会认定 <code>Job</code> 失败, 否则会进行重新调度, 这个参数的含义是,
                    当多少个 <code>Task</code> 失败了, 可以认定 <code>Job</code> 失败</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.speculation</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">类似 <code>Hadoop</code>, <code>Spark</code> 也支持推测执行, 场景是有可能因为某台机器的负载过高, 或者其它原因,
                    导致这台机器运行能力很差, <code>Spark</code> 会根据一些策略检测较慢的任务, 去启动备用任务执行, 使用执行较快的任务的结果, 但是推测执行有个弊端,
                    就是有可能一个任务会执行多份, 浪费集群资源</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.driver.allowMutilpleContext</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">很少有机会必须一定要在一个 <code>Spark Application</code> 中启动多个 <code>Context</code>,
                    所以这个配置项意义不大, 当必须要使用多个 <code>Context</code> 的时候, 开启此配置即可</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.serializer</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>Spark</code> 将任务分发到集群中执行, 所以势必涉及序列化, 这个配置项配置的是使用什么序列化器, 默认是 <code>JDK</code>
                    的序列化器, 可以指定为 <code>Kyro</code> 从而提升性能, 但是如果使用 <code>Kyro</code> 的话需要序列化的类需要被先注册才能使用</p>
            </td>
        </tr>
        <tr>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock"><code>spark.buffer.pageSize</code></p>
            </td>
            <td class="tableblock halign-left valign-top">
                <p class="tableblock">每个页缓存, <code>Page</code> 指的是操作系统的内存分配策略中的 <code>Page</code>, 一个 <code>Page</code>
                    代表一组连续的内存空间, <code>Spark</code> 在引入钨丝计划以后, 使用 <code>Java</code> 的 <code>Unsafe API</code> 直接申请内存,
                    其申请单位就是 <code>Page</code>, 如果 <code>Page</code> 过大, 有可能因为操作系统的策略无法分配而拒绝这次内存申请, 从而报错, 简单来说,
                    这个配置的作用是一次申请的内存大小, 一般在报错的时候修改这个配置, 减少一次申请的内存</p>
            </td>
        </tr>
    </tbody>
</table>

### 1.3.3 导入配置读取的工具依赖
1.在 pom.xml 中的 properties 段增加如下内容
``` 
<config.version>1.3.4</config.version>
```
2.在 pom.xml 中的 dependencites 段增加如下内容
``` 
<!-- Config reader -->
<dependency>
    <groupId>com.typesafe</groupId>
    <artifactId>config</artifactId>
    <version>${config.version}</version>
</dependency>
```

### 1.3.4 配置工具的设计思路
在设计一个工具的时候, 第一步永远是明确需求, 我们现在为 SparkSession 的创建设置配置加载工具, 其需求如下
- 配置在配置文件中编写
- 使用 typesafe/config 加载配置文件
- 在创建 SparkSession 的时候填写这些配置

前两点无需多说, 已经自表达, 其难点也就在于如何在 SparkSession 创建的时候填入配置, 大致思考的话, 有如下几种方式
- 加载配置文件后, 逐个将配置的项设置给 SparkSession
``` 
spark.config("spark.worker.timeout", config.get("spark.worker.timeout"))
```
- 加载配置文件后, 通过隐式转换为 SparkSession 设置配置
``` 
spark.loadConfig().getOrCreate()
```
毫无疑问, 第二种方式更为方便

### 1.3.5 创建配置工具类
看代码之前, 先了解一下设计目标
- 加载配置文件 spark.conf
- 无论配置文件中有多少配置都全部加载
- 为 SparkSession 提供隐式转换自动装载配置

下面是代码, 以及重点解读
``` 
class SparkConfigHelper(builder: SparkSession.Builder) {

  // 加载配置文件
  private val config: Config = ConfigFactory.load("spark")           

  def loadConfig(): SparkSession.Builder = {
    import scala.collection.JavaConverters._

    for (entry <- config.entrySet().asScala) {
      val value = entry.getValue
      val valueType = value.valueType()
      // 因为 Config 工具会自动的加载所有的系统变量, 需要通过 Origin 来源判断, 只接收来自于文件的配置
      val valueFrom = value.origin().filename()                       
      // 判断: 1. 是 String 类型, 2. 来自于某个配置文件
      if (valueType == ConfigValueType.STRING && valueFrom != null) { 
        val value = value.unwrapped().asInstanceOf[String]
        //为 SparkSession 设置参数
        builder.config(entry.getKey, value)                           
      }
    }

    builder
  }
}

// 提供伴生对象的意义在于两点: 1. 更方便的创建配置类, 2. 提供隐式转换, 3. 以后可能需要获取某个配置项
object SparkConfigHelper {                                            

  def apply(builder: SparkSession.Builder): SparkConfigHelper = {
    new SparkConfigHelper(builder)
  }

  // 提供隐式转换, 将 SparkSession 转为 ConfigHelper 对象, 从而提供配置加载
  implicit def setSparkSession(builder: SparkSession.Builder) = {     
    SparkConfigHelper(builder)
  }
}
```






