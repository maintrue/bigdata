# 1 抽取公共类
## 1.1 目标和步骤
目标
- 所有的报表统计基本上都是从一个表中读取数据, 经过处理, 落地到另外一张表中, 所以可以抽取出框架
- 面向对象是一个非常重要的技能
- 通过这个小节的学习, 抽取一个简单的框架, 对于以后查看源码和重构优化代码有非常大的帮助

步骤
- 分析现有代码逻辑
- 框架设计
- 修改现有代码

## 1.2 分析现有代码逻辑
公共代码抽取和设计的步骤
1. 详细的观察代码, 从中找到流程, 找到重复的东西
2. 从流程中, 找到可能会有变化的点
3. 抽取这些变化点
4. 抽取流程
``` 
object RegionReportProcessor {

  def main(args: Array[String]): Unit = {
    import com.itheima.dmp.utils.SparkConfigHelper._
    import com.itheima.dmp.utils.KuduHelper._
    import org.apache.spark.sql.functions._

    val spark = SparkSession.builder()
      .master("local[6]")
      .appName("pmt_etl")
      .loadConfig()
      .getOrCreate()

    import spark.implicits._

    val origin = spark.readKuduTable(ORIGIN_TABLE_NAME)

    if (origin.isEmpty) return

    val result = origin.get.groupBy($"region", $"city")
      .agg(count($"*") as "count")
      .select($"region", $"city", $"count")

    spark.createKuduTable(TARGET_TABLE_NAME, schema, keys)
    result.saveToKudu(TARGET_TABLE_NAME)
  }

  private val ORIGIN_TABLE_NAME = PmtETLRunner.ODS_TABLE_NAME
  private val TARGET_TABLE_NAME = "ANALYSIS_REGION_" + KuduHelper.formattedDate()

  import scala.collection.JavaConverters._
  private val schema = new Schema(List(
      new ColumnSchemaBuilder("region", Type.STRING).nullable(false).key(true).build,
      new ColumnSchemaBuilder("city", Type.STRING).nullable(true).key(true).build(),
      new ColumnSchemaBuilder("count", Type.STRING).nullable(true).key(false).build()
    ).asJava
  )

  private val keys = Seq("region", "city")
}
```

### 1.2.1 可变点
1.要读取的原始表
``` 
spark.readKuduTable(ORIGIN_TABLE_NAME)
```

2.数据处理流程, 和要落地的数据
``` 
val result = origin.groupBy($"region", $"city")
.agg(count($"*") as "count")
.select($"region", $"city", $"count")
```

3.要落地的表名
``` 
spark.createKuduTable(TARGET_TABLE_NAME, schema, keys)
result.saveToKudu(TARGET_TABLE_NAME)
```

4.要落地的表结构
``` 
private val schema = new Schema(List(
new ColumnSchemaBuilder("region", Type.STRING).nullable(false).key(true).build,
new ColumnSchemaBuilder("city", Type.STRING).nullable(true).key(true).build(),
new ColumnSchemaBuilder("count", Type.STRING).nullable(true).key(false).build()
).asJava
)
```

5.要落地的表分区 Key
```
private val keys = Seq("region", "city")
```

### 1.2.2 公共流程
1.创建 SparkSession 对象
``` 
val spark = SparkSession.builder()
.master("local[6]")
.appName("pmt_etl")
.loadConfig()
.getOrCreate()
```

2.读取表, 虽然名字不同, 但是读取的动作所有的报表统计中都要有
``` 
spark.readKuduTable(ORIGIN_TABLE_NAME)
```

3.处理数据, 虽然数据处理后生成的表数据不同, 但是数据都要经过处理
``` 
val result = origin.groupBy($"region", $"city")
.agg(count($"*") as "count")
.select($"region", $"city", $"count")
```

4.创建落地表, 虽然落地表的结构, 表名, 分区 Key 不同, 但是都要创建这样一张表
``` 
spark.createKuduTable(TARGET_TABLE_NAME, schema, keys)
```

5.落地表数据, 虽然要落地的表名不同, 但是都要落地
``` 
result.saveToKudu(TARGET_TABLE_NAME)
```

## 1.3 框架设计

### 1.3.1 可变点 Processor
需要有一个对象, 表示不同的报表如何统计, 有如下要求
- 这个对象能够提供数据处理的过程
- 这个对象能够提供源表的名称
- 这个对象能够提供目标表的名称
- 这个对象能够提供目标表的结构信息
- 这个对象能够提供目标表的分区 key

``` 
trait ReportProcessor {

  def process(origin: DataFrame): DataFrame

  def sourceTableName(): String

  def targetTableName(): String

  def targetTableSchema(): Schema

  def targetTableKeys(): List[String]
}
```

### 1.3.2 公共流程 DailyReportRunner
每一个报表都有自己的独特信息, 但是所有的报表统计流程却是比较相似, 都有如下流程
1. 创建 SparkSession
2. 读取源表
3. 处理数据
4. 创建目标表
5. 保存数据到目标表中

另外, 还需要一个集合保存所有的报表特质
``` 
object DailyReportRunner {

  def main(args: Array[String]): Unit = {
    import com.itheima.dmp.utils.SparkConfigHelper._
    import com.itheima.dmp.utils.KuduHelper._

    val spark = SparkSession.builder()
      .master("local[6]")
      .appName("pmt_etl")
      .loadConfig()
      .getOrCreate()

    val processors: List[ReportProcessor] = List(
      NewRegionReportProcessor
    )

    for (processor <- processors) {
      val origin = spark.readKuduTable(processor.sourceTableName())

      if (origin.isDefined) {
        val result = processor.process(origin.get)
        result.createKuduTable(processor.targetTableName(), processor.targetTableSchema(), processor.targetTableKeys())
        result.saveToKudu(processor.targetTableName())
      }
    }
  }
}
```

### 1.3.3 修改现有代码
可以根据现有的地域统计报表处理类, 改造为一个新的 Processor, 符合整体框架流程
``` 
object NewRegionReportProcessor extends ReportProcessor {

  override def process(origin: DataFrame): DataFrame = {
    import origin.sparkSession.implicits._

    origin.groupBy($"region", $"city")
      .agg(count($"*") as "count")
      .select($"region", $"city", $"count")
  }

  override def sourceTableName(): String = {
    PmtETLRunner.ODS_TABLE_NAME
  }

  override def targetTableName(): String = {
    "ANALYSIS_REGION_" + KuduHelper.formattedDate()
  }

  override def targetTableSchema(): Schema = {
    import scala.collection.JavaConverters._
    new Schema(List(
      new ColumnSchemaBuilder("region", Type.STRING).nullable(false).key(true).build,
      new ColumnSchemaBuilder("city", Type.STRING).nullable(false).key(true).build(),
      new ColumnSchemaBuilder("count", Type.INT64).nullable(false).key(false).build()
    ).asJava
    )
  }

  override def targetTableKeys(): List[String] = {
    List("region", "city")
  }
}
```

## 1.4 总结

### 1.4.1 框架抽取的好处
- 减少重复的代码
- 在编写新功能的时候, 有一个指引, 不需要所有的事情都由开发者操心

其实这也和 Spark, Hadoop 这样的计算框架很像, 这些计算框架不需要我们考虑分布式的细节, 而是自己把脏活累活都干了, 这其实也是一般框架在设计的时候的目的所在

### 1.4.2 框架抽取流程, 学会举一反三
- 不一定所有的框架都这么抽取, 但是对于流程性的东西, 这样做确实比较科学
- 抽取其实就是要找到两个东西: 第一, 什么是变的, 特有的东西. 第二, 什么是不变的, 共有的东西
- 根据每个不同的功能, 编写特质 Trait
- 根据特质, 编写流程

### 1.4.3 改善现有代码
改善后, 现有代码减少了一半
- 原有代码中, 有 25 行左右的有效代码
- 改善后, 有 12 行左右的有效代码
- 并且经过改善后整体流程更加清晰
