# 机器学习入门

* 目标
  * 理解机器学习的一般步骤, 以及如何使用 Spark 进行机器学习
* 步骤
  1. 学习型算法的不同
  2. 什么是机器学习
  3. Zeppelin
  4. 房价预测实现



## 1. 学习型算法

* 目标
  * 理解学习型算法和普通的算法区别
* 步骤
  1. 冒泡排序
  2. 学习型算法



### 1.1. 冒泡排序



* 按照一定得顺序执行某一些操作, 最终实现某些功能
* 演绎法



```javascript
function bubbleSort(arr) {
    var len = arr.length;
    for (var i = 0; i < len; i++) {
        for (var j = 0; j < len - 1 - i; j++) {
            if (arr[j] > arr[j+1]) {       // 相邻元素两两对比
                var temp = arr[j+1];       // 元素交换
                arr[j+1] = arr[j];
                arr[j] = temp;
            }
        }
    }
    return arr;
}
```



![img](assets/849589-20171015223238449-2146169197.gif)



### 1.2. 机器学习



* 归纳法
* 步骤
  1. 处理数据, 生成数据集
  2. 使用学习型算法, 从数据集中学习规律, 这个规律就是模型



![](assets/004.png)



![](assets/5bd6672a0001276f13820792.png)



## 2. 什么是机器学习

两类

* 监督学习
* 无监督学习



监督式机器学习的定义如下



- 机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测



**标签**



**标签**是我们要预测的事物, 即简单线性回归中的 `y` 变量. 标签可以是小麦未来的价格, 图片中显示的动物品种, 音频剪辑的含义或任何事物



**特征**

**特征**是输入变量, 即简单线性回归中的 `x` 变量. 简单的机器学习项目可能会使用单个特征, 而比较复杂的机器学习项目可能会使用数百万个特征, 按如下方式指定:


$$
x1,x2,...xN
$$


**样本**

**样本**是指数据的特定实例: **x**

- 有标签样本
- 无标签样本



**有标签样本**同时包含特征和标签

```
labeled examples: {features, label}: (x, y)
```



我们使用有标签样本来**训练**模型

* 表就是数据集
* 列是特征
* 标签就是要预测的结果
* 每一行数据叫做样本

| 住房平均年龄 | 房间数 | 卧室数 | 房屋价格 |
| :----------- | :----- | :----- | :------- |
| 15           | 5612   | 1283   | 66900    |
| 19           | 7650   | 1901   | 80100    |
| 17           | 720    | 174    | 85700    |
| 14           | 1501   | 337    | 73400    |
| 20           | 1454   | 326    | 65500    |



**无标签样本**包含特征, 但不包含标签

```
unlabeled examples: {features, ?}: (x, ?)
```



我们要预测无标签样本的标签



| 住房平均年龄 | 房间 | 卧室数 |
| :----------- | :--- | :----- |
| 42           | 1686 | 361    |
| 34           | 1226 | 180    |
| 33           | 1077 | 271    |



**模型**

模型定义了特征与标签之间的关系. 例如, 垃圾邮件检测模型可能会将某些特征与标签 "垃圾邮件" 紧密联系起来. 

- **训练**是指创建或**学习**模型. 也就是说, 向模型展示有标签样本, 让模型逐渐学习特征与标签之间的关系
- **预测**是指将训练后的模型应用于无标签样本



**回归与分类**

**回归**模型可预测连续值. 例如, 回归模型做出的预测可回答如下问题

- 加利福尼亚州一栋房产的价值是多少?
- 用户点击此广告的概率是多少?

**分类**模型可预测离散值. 例如, 分类模型做出的预测可回答如下问题

- 某个指定电子邮件是垃圾邮件还是非垃圾邮件?
- 这是一张狗, 猫还是仓鼠图片?



个人理解

* 机器学习非常难, 因为机器学习是一个学术学科, 不是一个工程学科
* 机器学习难以入门
* 机器学习掌握比较深的人并不是搞开发的
* 不应该把精力放在对算法和理论的研究上, 因为一般的功能和任务, 现在的算法都能很好的完成, 掌握常见算法的使用方式即可
* 更重要的是, 了解数据如何处理, 如何进行特征工程



## 3. Zeppelin

* 目标
  * 能够在 Zeppelin 中编写代码, 从而在后面能够使用 Zeppelin 进行机器学习的可视化数据分析
* 步骤
  1. 安装
  2. Zeppelin 的作用
  3. Zeppelin 入门使用



### 3.1. 安装



1. 下载 Zeppelin

   在 `Files/setup` 中已经准备好了压缩包

2. 上传到 master01 中

3. 解压缩 `zeppelin-0.8.2-bin-all.tgz`

   ```shell
   cd /home/vagrant
   mkdir /opt/zeppelin
   tar -xzvf zeppelin-0.8.2-bin-all.tgz -C /opt
   cd /opt
   mv zeppelin-0.8.2-bin-all zeppelin
   ```

4. 创建 Zeppelin 的环境配置

   ```shell
   cd /opt/zeppelin/conf
   cp zeppelin-env.sh.template zeppelin-env.sh
   cp zeppelin-site.xml.template zeppelin-site.xml
   ```

5. 修改 Zeppelin 配置文件, 在 `zeppelin/conf/zeppelin-env.sh` 中加入如下环境变量

   ```properties
   export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera
   export HADOOP_CONF_DIR=/etc/hadoop/conf:/etc/hive/conf
   export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
   export SPARK_HOME=/opt/cloudera/parcels/CDH/lib/spark
   export MASTER=yarn-client
   export ZEPPELIN_LOG_DIR=/var/log/zeppelin
   export ZEPPELIN_PID_DIR=/var/run/zeppelin
   export ZEPPELIN_WAR_TEMPDIR=/var/tmp/zeppelin
   ```

6. 修改 Zeppelin 配置文件, 在 `zeppelin/conf/zeppelin-site.xml` 中修改 IP 和端口

   ```xml
   <property>
     <name>zeppelin.server.addr</name>
     <value>0.0.0.0</value>
     <description>Server binding address</description>
   </property>
   
   <property>
     <name>zeppelin.server.port</name>
     <value>8099</value>
     <description>Server port.</description>
   </property>
   ```

7. 启动 Zeppelin

   ```shell
   cd /opt/zeppelin/bin
   ./zeppelin-daemon.sh start
   ```

   

### 3.2. Zeppelin的作用



* Zeppelin 是一个多用途笔记本工具, 其每一个笔记可以包含如下的条目

  * Markdown
  * Spark 程序
  * 在某个数据库上执行的 SQL
  * Flink 程序
  * 等...

* Zeppelin 可以对数据进行可视化

  ![“zeppelin 是什么”的图片搜索结果](assets/3cdafpcyk2.png)

* Zeppelin 中的图表可以嵌入到网页中

  ![](assets/20150401111832223)

* 所以 Zeppelin 可以做这些事
  * 数据分析的协作
  * 作为 BI 使用
  * 机器学习中的数据探索



### 3.3. Zeppelin 入门使用



使用 Zeppelin 创建一个测试笔记本, 并编写 Spark 程序, 在开始之前, 需要在 HDFS 的目录 `/user/admin/dataset/` 中上传我们所需要的数据集 `Files/dataset/house_price_train.csv`



1. 创建笔记本

   <img src="assets/image-20200229141911248.png" alt="image-20200229141911248" style="zoom:50%;" />

2. 编写一行 Markdown, 使用 `shift + enter` 运行显示

   ![image-20200229142201621](assets/image-20200229142201621.png)

3. 编写一个 Spark 程序并运行

   ![image-20200229144956055](assets/image-20200229144956055.png)

4. 注册为临表并使用 SQL 可视化

   ![image-20200229145129488](assets/image-20200229145129488.png)



## 4. 房价预测详细思路

* 目标
  * 能够完成简单的机器学习任务
* 步骤
  1. 绘制房价直方图, 审视数据分布
  2. 绘制散点图, 审视数据和结果之间的关系
  3. 选择模型
  4. 特征处理之空值处理
  5. 特征处理之组合特征
  6. 特征处理之特征编码
  7. 简要实现模型



### 4.1. 绘制房价直方图

> 通过绘制房价的直方图, 就可以了解 **我们要预测的值的分布情况**
>
> 通过了解目标的分布情况, 有助于我们理解我们要做的事情, 为算法选择和结果验证提供支撑



* 需求
  * 根据数据集, 预测房价



1. 读取数据

   ```scala
   %spark
   val source = spark.read
       .option("header", "true")
       .option("delimiter", ",")
       .csv("/user/admin/dataset/house_prices_train.csv")
   ```

2. 查看元数据

   ```scala
   %spark
   source.printSchema
   ```

3. 查看房价概况

   ```scala
   %spark
   import spark.implicits._
   
   source.describe("SalePrice").show
   ```

4. 为房价直方图准备数据

   * 简单来说, 直方图就是二维坐标系中, x 轴为房价, y 轴为某一个房价出现的次数

     <img src="assets/histogramofvectordataexample_01_zh_CN.png" style="zoom: 80%;" />

   * 直方图一般把数据分组放置于 x 轴, 组中数据的个数映射到 y 轴, 数据准备如下

     ```scala
     %spark
     import org.apache.spark.ml.feature.QuantileDiscretizer
     import spark.implicits._
     import org.apache.spark.sql.types.LongType
     import org.apache.spark.sql.functions._
     
     val salePrice = source.select('SalePrice cast(LongType))
     val qd = new QuantileDiscretizer()
       .setInputCol("SalePrice")
       .setOutputCol("Bin")
       .setNumBuckets(60)
     qd.fit(salePrice)
         .transform(salePrice)
         .groupBy('Bin)
         .agg(avg('SalePrice) as "avg_price", count('Bin) as "count")
         .registerTempTable("sale_price_hist")
     ```

5. 绘图

   ```sql
   %spark.sql
   select * from sale_price_hist
   ```



通过这张图, 我们可以总结出来数据的大致情况



* 呈偏态分布
* 有明显尖峰



### 4.2. 绘制数据和房价之间的散点图

> 绘制散点图有助于我们理解 **标签(要预测的值)和某一维度数据之间的关系**
>
> 如果连哪一列数据更重要, 就谈不上模型



1. 注册 Source 为临时表

   ```scala
   %spark
   source.registerTempTable("house_price")
   ```

2. 绘制房屋质量和房价之间的关系

   ```sql
   %spark.sql
   select YearBuilt, SalePrice from house_price 
   ```

3. 绘制居住面积和房价之间的关系

   ```sql
   %spark.sql
   select GrLivArea, SalePrice from house_price 
   ```

4. 绘制地下室面积和房价之间的关系

   ```sql
   %spark.sql
   select TotalBsmtSF, SalePrice from house_price 
   ```



![image-20200229221821279](assets/image-20200229221821279.png)



通过这些图, 我们可以确定一些事情



* 这三个属性和结果之间都是正相关的
* 这三个属性和结果之间并非简单的线性关系
* 通过散点图, 还可以观察到一些异常值, 考虑去掉无关模型的异常值有助于提高模型准确率



### 4.3. 选择模型

* 学习特征中的规律, 使用算法生成模型
* 把新的输入模型, 产生预测结果
* 其实很难通过经验和特征准确的预知哪个模型效果更好



![img](assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTgwMjA5MTExMTMzODY5.jfif)



### 4.4. 空值处理

> 当我们使用算法 "学习" 数据中的规律时, 无意义的值会影响学习效果



* 决定模型优劣的主因还是特征的处理
* 大致超过八成的时间是处理数据的
* 空值类型
  * null
  * NaN
  * "NA"
  * 异常值



1. 识别缺失值

   * 缺失值有三种, 一种是 `null`, 一种是 `NaN`, 一种是 `NA` 这样的字符串
   * 前两种比较好处理, Spark 提供有 `dropNa` 和 `dropNaN` 之类的方法可以处理
   * 第三种缺失值处理的第一步要先识别缺失值
   * 极端情况下, 可以如下做法, 收集数据集中可能的缺失值

   ```scala
   %spark
   import org.apache.spark.sql.functions._
   
   source.columns.map(c => {
     source.groupBy(col(c)).agg(first(col(c)) as "first").show()
   })
   
   // 有缺失值的列为 Alley(小路), MasVnrType(表层砌体), BsmtQual(地下室高度) 等...
   // 而且整个数据集字符串的缺失值类型都是 NA
   ```

2. 过滤缺失值的方式

   * 如果一列数据有太多缺失值, 删除此列
   * 使用均值, 中值, 众数, 随机数等替代缺失值, 但是不推荐, 因为这就等于认为添加噪声, 降低数据质量
   * 拆值, 比如说性别有男, 女, 缺失, 三种状态, 拆成三列, 是否男, 是否女, 是否缺失, 这样最为精确

   ```scala
   %spark
   import spark.implicits._
   import org.apache.spark.sql.functions._
   import org.apache.spark.sql.types.IntegerType
   
   source.agg(sum(('Alley === "NA").cast(IntegerType)) / count('Alley)).show
   
   // 发现 Alley 这一列的 NA 数量高达 93%
   ```



### 4.5. 顺序特征编码

> 人类可以理解装修质量的低中高是有顺序的, 但是以文字形式出现的话, 计算机是无法理解的
>
> 有一些特征, 存在明显的顺序性, 比如说身高分三档, 矮, 中, 高, 就明显具备这种顺序性, 如果将文字形式的特征表示为具备顺序性的 123 时, 会提升 "学习" 效果, 让计算机也可以识别我们所理解的顺序
>
> ExterQual 的含义是 外部质量, 取值由低到高为 (Ex, Gd, TA, Fa), 我们选择这一列, 示例如何顺序编码



```scala
%spark
import spark.implicits._
import org.apache.spark.sql.functions._

def to_level(q: String): Int = {
    q match {
      case "Ex" => 1
      case "Gd" => 2
      case "TA" => 3
      case "Fa" => 4
      case _ => 0
    }
}

val to_level_udf = udf(to_level _)

source.select(to_level_udf('ExterQual)).show
```



### 4.6. 组合新特征

> 房屋面积对房价的影响不可谓不大, 如果能增加一个关于房屋面积的列, 会对模型有正面的影响
>
> 数据集中关于房屋面积的列有三个, 地下室面积 TotalBsmtSF, 一楼面积 1stFlrSF, 二楼面积 2ndFlrSF
>
> 可以增加一个地下地上总面积的列



```scala
import spark.implicits._
source.select(('TotalBsmtSF + source.col("1stFlrSF") + source.col("2ndFlrSF")) as "TotalSF").show
```



### 4.7. 特征编码

> 很多算法无法对枚举类型的字符串进行处理, 需要对其转为可以处理的数值类型



1. 在数据集中有很多列的数据是字符串枚举, 算法无法直接处理

   ```scala
   %spark
   val cols = Array(
     "BsmtFinType1", "MasVnrType", "Foundation",
     "HouseStyle", "Functional", "BsmtExposure",
     "GarageFinish", "Street", "ExterQual",
     "PavedDrive", "ExterCond", "KitchenQual",
     "HeatingQC", "BsmtQual", "FireplaceQu",
     "GarageQual", "PoolQC"
   )
   
   source.select("BsmtFinType1", cols:_*).show
   ```

2. 可以将这些文本数据转为算法可以理解的数值型, 就可以看到这些字符串的数据已经是数值的形式了

   ```scala
   %spark
   import org.apache.spark.ml.feature.StringIndexer
   import org.apache.spark.sql.DataFrame
   
   val cols = Array(
     "BsmtFinType1", "MasVnrType", "Foundation",
     "HouseStyle", "Functional", "BsmtExposure",
     "GarageFinish", "Street", "ExterQual",
     "PavedDrive", "ExterCond", "KitchenQual",
     "HeatingQC", "BsmtQual", "FireplaceQu",
     "GarageQual", "PoolQC"
   )
   
   var indexerDF: DataFrame = null
   
   for (col <- cols) {
       val stringIndexer = new StringIndexer()
       .setInputCol(col)
       .setOutputCol(s"${col}_index")
   
       if (indexerDF == null) {
           indexerDF = stringIndexer.fit(source).transform(source)
       } else {
           indexerDF = stringIndexer.fit(indexerDF).transform(indexerDF)
       }
   }
   
   val indexCols = cols.map(col => s"${col}_index")
   indexerDF.select(indexCols.head, indexCols:_*).show()
   ```

3. 通过这些数值型的数据, 可以将这些数据进行 OneHot 编码

   * 为了提高算法能力, 只是转成数值还不行, 因为数值之间是有大小的
   * 举个例子, 如果红转成1, 黄转成2, 蓝转成3, 那么黄就变成红和蓝的中位数了, 这在很多算法的计算里是不合适的

   ![](assets/0_T5jaa2othYfXZX9W_.jfif)

   ```scala
   %spark
   import org.apache.spark.ml.feature.OneHotEncoderEstimator
   
   val oneHotCols = cols.map(col => s"${col}_onehot")
   val oneHotEncoderEstimator = new OneHotEncoderEstimator()
     .setInputCols(indexCols)
     .setOutputCols(oneHotCols)
   
   val oneHotDF = oneHotEncoderEstimator.fit(indexerDF).transform(indexerDF)
   oneHotDF.select(oneHotCols.head, oneHotCols:_*)
     .show()
   ```

4. 对于很多算法而言, 需要将所有的特征集合起来, 放入一个列中

   * 需要注意的是, 在这一步骤中, 需要加入所有传入算法的特征

   ```scala
   %spark
   import org.apache.spark.ml.feature.VectorAssembler
   
   val vectorAssembler = new VectorAssembler()
     //.setInputCols(oneHotCols :+ "TotalBsmtSF" :+ "1stFlrSF" :+ "2ndFlrSF")
     .setInputCols(oneHotCols)
     .setOutputCol("features")
   
   val vectorDF = vectorAssembler.transform(oneHotDF)
   vectorDF.select("features")
     .show()
   ```



### 4.8. 简要实现模型



为了实现一个机器学习算法, 一般会遵循如下步骤



1. 数据分析, 查看数据的情况, 例如分布情况, 组成情况等, 从而为后面的特征处理和模型训练奠定基础
2. 数据处理, 空值处理, 顺序编码, 组合特征等
3. 特征提取, 例如一些文本列, 要通过一些特定的方式, 抽取出来特征
4. 特征合并, 某些算法需要我们把特征合并起来
5. 特征转换, 特征中有一部分是非数字形式的, 要通过一些方式转为数字形式, 例如独热或者编码
6. 模型训练, 训练模型
7. 交叉验证, 验证模型指标
8. 新数据预测



接下来用户画像标签的实现中, 直接选择特征, 进行合并转换, 训练模型

在工作中, 如果模型不够好, 要优化模型, 要从特征的角度着手, 认真梳理和处理特征



```scala
%spark
import org.apache.spark.ml.regression.RandomForestRegressor
import org.apache.spark.sql.types.DoubleType
import spark.implicits._

val regressor = new RandomForestRegressor()
  .setMaxDepth(5)
  .setImpurity("variance")
  .setFeaturesCol("features")
  .setLabelCol("SalePrice")
  .setPredictionCol("prediction")

regressor.fit(vectorDF.select('features, 'SalePrice cast DoubleType))
  .transform(vectorDF)
  .select('SalePrice, 'prediction)
  .show()
```

