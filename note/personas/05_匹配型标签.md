# 匹配型标签

* 目标
  * 能够完成匹配型标签的计算
* 步骤
  1. SHC 介绍
  2. 性别标签
  3. HBase 工具类
  4. 职业标签
  5. 基类抽取
  6. 国籍标签
  7. 政治面貌标签



## 1. SHC 介绍

* 目标
  * 理解 SHC 是什么, 以及如何使用
* 步骤
  1. 基本使用
  2. JSON 格式化



### 1.1. SHC 的基本使用



直到 2.3 版本开始, HBase 才提供了 Spark 的原生连接器, 所以如果需要使用 Spark 访问 HBase, 有两种选择



* 自己编写连接器, 通过 `newApiHadoop` 来操作 HBase
* 使用第三方的, 目前看来第三方最好的还是 Hortonworks 的 SHC(Spark HBase Connector)



使用 SHC 读取 HBase



1. 安装 SHC 最新版
   1. MVN 配置
   2. 在 `/Code/shc-master` 中执行 `mvn install --DskipTests`
   3. Maven pom.xml -> Local repo 读取本地的 Maven 缓存 -> 远端仓库
   4. 如果想要使用 MVN 命令, 需要配置 Maven 到 Path 中, 同时需要确定有 JAVA_HOME 这个环境变量
2. 编写代码



```scala
def catalog = s"""{
     |"table":{"namespace":"default", "name":"tbl_users"},
     |"rowkey":"id",
     |"columns":{
       |"id":{"cf":"rowkey", "col":"id", "type":"string"},
       |"username":{"cf":"default", "col":"username", "type":"string"}
     |}
   |}""".stripMargin

val spark = SparkSession.builder()
  .appName("shc test")
  .master("local[10]")
  .getOrCreate()

spark.read
  .option(HBaseTableCatalog.tableCatalog, catalog)
  .format("org.apache.spark.sql.execution.datasources.hbase")
  .load()
  .show()
```



使用 SHC 写入 HBase



```scala
    def catalogRead = s"""{
         |"table":{"namespace":"default", "name":"tbl_users_test"},
         |"rowkey":"id",
         |"columns":{
           |"id":{"cf":"rowkey", "col":"id", "type":"string"},
           |"username":{"cf":"default", "col":"username", "type":"string"}
         |}
       |}""".stripMargin

    val spark = SparkSession.builder()
      .appName("shc test")
      .master("local[10]")
      .getOrCreate()

    val readDF = spark.read
      .option(HBaseTableCatalog.tableCatalog, catalogRead)
      .format("org.apache.spark.sql.execution.datasources.hbase")
      .load()

    def catalogWrite = s"""{
         |"table":{"namespace":"default", "name":"tbl_users_test"},
         |"rowkey":"id",
         |"columns":{
           |"id":{"cf":"rowkey", "col":"id", "type":"string"},
           |"username":{"cf":"default", "col":"username", "type":"string"}
         |}
       |}""".stripMargin

    readDF.write
      .option(HBaseTableCatalog.tableCatalog, catalogWrite)
      .option(HBaseTableCatalog.newTable, "5")
      .format("org.apache.spark.sql.execution.datasources.hbase")
      .save()
```



这段程序如果在本机执行的话, 会出现一个异常



```text
shc Pathname xx from xx is not a valid DFS filename
```



这个异常并不会影响数据的写入, 是因为本机的临时文件问题, 放在集群跑就没问题了



### 1.1. 生成 JSON



因为直接使用字符串去拼接 JSON 格式的 Catalog 会非常麻烦, 所以我们可以通过 JSON 对象来简化这个步骤



1. 根据 Catalog 的对象格式, 生成对应的样例类
2. 创建样例类对象
3. 通过 JSON4S 将样例类对象转为 JSON 字符串
4. 访问和保存 HBase



```scala
object ShcJsonTest {

  def main(args: Array[String]): Unit = {
    val rowkeyField = "id"
    val columnFamily = "default"
    val tableName = "tbl_users"

    val columns: mutable.HashMap[String, HBaseField] = mutable.HashMap.empty[String, HBaseField]
    columns += rowkeyField -> HBaseField("rowkey", rowkeyField, "string")
    columns += "username" -> HBaseField(columnFamily, "username", "string")

    val hbaseCatalog = HBaseCatalog(HBaseTable("default", tableName), rowkeyField, columns.toMap)

    import org.json4s._
    import org.json4s.jackson.Serialization
    import org.json4s.jackson.Serialization.write
    implicit  val formats: AnyRef with Formats = Serialization.formats(NoTypeHints)

    val catalog = write(hbaseCatalog)

    val spark = SparkSession.builder()
      .appName("shc test")
      .master("local[10]")
      .getOrCreate()

    val readDF = spark.read
      .option(HBaseTableCatalog.tableCatalog, catalog)
      .format("org.apache.spark.sql.execution.datasources.hbase")
      .load()

    readDF.show()
  }

  case class HBaseCatalog(table: HBaseTable, rowkey: String, columns: Map[String, HBaseField])

  case class HBaseTable(namespace: String, name: String)

  case class HBaseField(cf: String, col: String, `type`: String)
}
```



同时, 可以将这个过程抽取为一个方法, 简化开发



```scala
object ShcJsonTest {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("shc test")
      .master("local[10]")
      .getOrCreate()

    val readDF = spark.read
      .option(HBaseTableCatalog.tableCatalog, generateCatalog("id", "default", "tbl_users"))
      .format("org.apache.spark.sql.execution.datasources.hbase")
      .load()

    readDF.show()
  }

  def generateCatalog(rowkeyField: String, columnFamily: String, tableName: String): String = {
    val columns: mutable.HashMap[String, HBaseField] = mutable.HashMap.empty[String, HBaseField]
    columns += rowkeyField -> HBaseField("rowkey", rowkeyField, "string")
    columns += "username" -> HBaseField(columnFamily, "username", "string")

    val hbaseCatalog = HBaseCatalog(HBaseTable("default", tableName), rowkeyField, columns.toMap)

    import org.json4s._
    import org.json4s.jackson.Serialization
    import org.json4s.jackson.Serialization.write
    implicit  val formats: AnyRef with Formats = Serialization.formats(NoTypeHints)

    write(hbaseCatalog)
  }

  case class HBaseCatalog(table: HBaseTable, rowkey: String, columns: Map[String, HBaseField])

  case class HBaseTable(namespace: String, name: String)

  case class HBaseField(cf: String, col: String, `type`: String)
}
```





## 2. 性别标签

* 目标
  * 理解标签计算的全流程
* 步骤
  1. 读取 MySQL 元数据
  2. 处理元数据
  3. 读取源表
  4. 计算性别标签
  5. 写入画像表



### 2.1. 读取 MySQL 中的标签信息和标签元数据



* Spark 任务是否需要知道标签的信息
  * 四级标签通过ID可以从 Metadata 表中读取对应的元数据
* 四级标签和五级标签的rule字段是不同的含义
  * 四级标签的 rule 的含义是连接信息, 元数据
  * 五级标签的 rule 是要匹配的值
* 计算过程
  1. 通过四级标签读取连接信息, 拿到源表
  2. 通过五级标签的数据, 匹配标签, 例如说一个人gender是1, 五级标签的rule也是1, 匹配成功, 这个人打上男标签
  3. 打上标签过后, 将这个人的信息, 存入用户画像表



1. 编写配置

   ```properties
   jdbc.mysql.basic_tag.url="jdbc:mysql://"${jdbc.mysql.host}":"${jdbc.mysql.port}"/"${jdbc.mysql.basic_tag.db}"?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC&user="${jdbc.mysql.user}"&password="${jdbc.mysql.pass}
   jdbc.mysql.basic_tag.db="tags"
   jdbc.mysql.basic_tag.table="tbl_basic_tag"
   jdbc.mysql.basic_tag.tag_name_field="name"
   
   jdbc.mysql.host="master01"
   jdbc.mysql.port="3306"
   jdbc.mysql.user="root"
   jdbc.mysql.pass="itcastmysqlroot"
   ```

   

2. 读取四级标签

3. 读取五级标签



```scala
def readBasicTag(): (Tag, Array[Tag]) = {
  import org.apache.spark.sql.functions._
  import spark.implicits._

  val jdbcUrl = config.getString("jdbc.mysql.basic_tag.url")
  val jdbcTable = config.getString("jdbc.mysql.basic_tag.table")
  val tagNameField = config.getString("jdbc.mysql.basic_tag.tag_name_field")

  val sourceDF = spark.read
    .jdbc(jdbcUrl, jdbcTable, new Properties())

  val fourLevelTag = sourceDF.where(col(tagNameField) === TAG_NAME)
    .as[Tag]
    .collect()(0)

  val fiveLevelTags = sourceDF.where('pid === fourLevelTag.id)
    .as[Tag]
    .collect()

  (fourLevelTag, fiveLevelTags)
}
```



### 2.2. 处理元数据



1. 根据元数据表, 创建对应的元数据对象

   ```scala
   case class MetaData(in_type: String, driver: String, url: String, username: String, password: String, db_table: String,
                       in_path: String, sperator: String, in_fields: String, cond_fields: String, out_fields: String,
                       out_path: String, zk_hosts: String, zk_port: String, hbase_table: String, family: String,
                       select_field_names: String, where_field_names: String, where_field_values: String) {
   
     def isHBase: Boolean = {
       in_type.toLowerCase == "hbase"
     }
   
     def isRdbms: Boolean = {
       in_type.toLowerCase == "rdbms" || in_type.toLowerCase == "mysql" ||
       in_type.toLowerCase == "postgresql" || in_type.toLowerCase() == "oracle"
     }
   
     def toHBase: HBaseMeta = {
       val common = CommonMeta(hbase_table, in_fields.split(","), out_fields.split(","))
       HBaseMeta(common, "id", family, zk_hosts, zk_port)
     }
   
     def toRdbms: RdbmsMeta = {
       val common = CommonMeta(db_table, in_fields.split(","), out_fields.split(","))
       RdbmsMeta(common, driver, url, username, password)
     }
   }
   
   case class CommonMeta(schema: String, inFields: Array[String], outFields: Array[String])
   
   case class HBaseMeta(common: CommonMeta, rowKey: String, family: String, zkHosts: String, zkPort: String)
   
   case class RdbmsMeta(common: CommonMeta, driver: String, url: String, user: String, password: String)
   ```

   

2. 配置元数据表

   ```scala
   jdbc.mysql.tag_meta_data.url="jdbc:mysql://"${jdbc.mysql.host}":"${jdbc.mysql.port}"/"${jdbc.mysql.tag_meta_data.db}"?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC&user="${jdbc.mysql.user}"&password="${jdbc.mysql.pass}
   jdbc.mysql.tag_meta_data.db="tags"
   jdbc.mysql.tag_meta_data.table="tbl_metadata"
   ```

   

3. 读取元数据

   ```scala
   def extraMetaData(tagId: Long): MetaData = {
     import spark.implicits._
   
     val jdbcUrl = config.getString("jdbc.mysql.tag_meta_data.url")
     val jdbcTable = config.getString("jdbc.mysql.tag_meta_data.table")
   
     val metaData = spark.read
       .jdbc(jdbcUrl, jdbcTable, new Properties())
       .where('tag_id === tagId)
       .as[MetaData]
       .collect()(0)
   
     metaData
   }
   ```

   

### 2.3. 创建源表 DataFrame



1. 处理 Catalog
2. 读取源表



```scala
def createSparkSource(metaData: MetaData): (DataFrame, CommonMeta) = {
  if (metaData.isHBase) {
    val hbaseMeta = metaData.toHBase

    val columns: mutable.HashMap[String, HBaseField] = mutable.HashMap.empty[String, HBaseField]
    columns += hbaseMeta.rowKey -> HBaseField("rowkey", hbaseMeta.rowKey, "string")
    for (field <- hbaseMeta.common.inFields) {
      columns += field -> HBaseField(hbaseMeta.family, field, "string")
    }

    val hbaseCatalog = HBaseCatalog(HBaseTable("default", hbaseMeta.common.schema), hbaseMeta.rowKey, columns.toMap)

    import org.json4s._
    import org.json4s.jackson.Serialization
    import org.json4s.jackson.Serialization.write
    implicit  val formats: AnyRef with Formats = Serialization.formats(NoTypeHints)

    val hbaseCatalogJson = write(hbaseCatalog)

    val source = spark.read
      .option(HBaseTableCatalog.tableCatalog, hbaseCatalogJson)
      .format("org.apache.spark.sql.execution.datasources.hbase")
      .load()

    return(source, hbaseMeta.common)
  }

  if (metaData.isRdbms) {

  }

  null
}
```



### 2.4. 计算性别匹配



1. 根据五级标签, 生成 Spark 条件
2. 处理



```scala
def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  var conditions: Column = null
  for (tag <- fiveTags) {
    conditions = if (conditions != null)
      conditions.when('gender === tag.rule, tag.id)
    else
      when('gender === tag.rule, tag.id)
  }

  if (conditions != null) {
    conditions = conditions.as(outFields.head)
  }

  val result = source.select('id as "uid", conditions)

  result.show()

  result
}
```



### 2.5. 保存数据



1. 处理 Catalog
2. 保存数据



```scala
def save(result: DataFrame, commonMeta: CommonMeta): Unit = {
  val RESULT_ROWKEY_FIELD = "uid"
  val RESULT_CF = "default"
  val RESULT_TABLE = "user_profile"

  val columns: mutable.HashMap[String, HBaseField] = mutable.HashMap.empty[String, HBaseField]
  columns += RESULT_ROWKEY_FIELD -> HBaseField("rowkey", RESULT_ROWKEY_FIELD, "string")
  for (field <- commonMeta.outFields) {
    columns += field -> HBaseField(RESULT_CF, field, "string")
  }

  val hbaseCatalog = HBaseCatalog(HBaseTable("default", RESULT_TABLE), RESULT_ROWKEY_FIELD, columns.toMap)

  import org.json4s._
  import org.json4s.jackson.Serialization
  import org.json4s.jackson.Serialization.write
  implicit  val formats: AnyRef with Formats = Serialization.formats(NoTypeHints)

  val resultCatalog = write(hbaseCatalog)

  result.write
    .option(HBaseTableCatalog.tableCatalog, resultCatalog)
    .option(HBaseTableCatalog.newTable, "5")
    .format("org.apache.spark.sql.execution.datasources.hbase")
    .save()
}
```



## 3. 优化点1: 优化 HBase 的读取和写入



整体处理步骤



1. 读取四级标签和五级标签
   1. 读取四级标签
   2. 通过速记标签的id取得它旗下的五级标签们
2. 通过四级标签的id, 取得它的metadata
3. 把 元数据 打散, 分类
4. 根据元数据的指示, 读取对应的数据源
   1. HBase √ 拼接 Catalog
   2. RDBMS
   3. HDFS 文件
5. 标签匹配
6. 标签存储
   1. 拼接 Catalog



优化点: 将HBase的存取, 编写一个工具类



## 4. 职业标签开发



1. 测试环境中开发
   1. 编辑标签数据
      1. 通过 SQL 在 MySQL 中, 创建四级标签和五级标签
      2. 同步的去创建元数据, 写入 Metadata 表中
   2. 确定计算标签所需要的数据
   3. 具体的标签开发
   4. 修改 Spark 任务
2. 上线进入生产环境
   1. 打包 Spark 任务
   2. 直接使用线上的 Web 系统, 创建标签, Web 后端会同步的解析元数据存储
      * 同时, 上传 Spark 任务
   3. 审核
   4. 计算标签, 写入画像表



* 编写 职业标签 的同时, 把和性别标签相似的方法抽取到 BasicModel 中
  * 极限编程, 不断重构



* JobModel

  ```scala
  object JobModel extends BasicModel {
    val TAG_NAME = "职业"
  
    val spark = SparkSession.builder()
      .appName("job model")
      .master("local[5]")
      .getOrCreate()
  
    val config = ConfigFactory.load()
  
    def main(args: Array[String]): Unit = {
      // 1. 读取标签数据
      val (fourTag, fiveTags) = readBasicTag(TAG_NAME)
  
      // 2. 读取元信息
      val metaData = readMetaData(fourTag.id)
  
      // 3. 读取源表数据
      val (source, commonMeta) = createSource(metaData)
  
      // 4. 计算标签
      val result = process(source, fiveTags, commonMeta.outFields)
      result.show()
  
      // 5. 保存画像数据
  //    saveUserProfile(result, commonMeta.outFields)
    }
  
    def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
      import org.apache.spark.sql.functions._
      import spark.implicits._
  
      var conditions: Column = null
  
      for (tag <- fiveTags) {
        conditions = if (conditions == null)
          when('job === tag.rule, tag.id)
        else
          conditions.when('job === tag.rule, tag.id)
      }
      conditions = conditions.as(outFields.head)
      source.select('id, conditions)
    }
  }
  ```



* BasicModel

  ```scala
  class BasicModel {
  
    /**
     * 读取标签数据
     *
     * @param tagName 要读取的四级标签名
     * @return 四级标签和与之关联的五级标签
     */
    def readBasicTag(tagName: String): (Tag, Array[Tag]) = {
      val url = config.getString("jdbc.basic_tag.url")
      val table = config.getString("jdbc.basic_tag.table")
  
      val source: DataFrame = spark.read.jdbc(url, table, new Properties())
  
      import spark.implicits._
  
      val fourTag = source.where('name === tagName)
        .as[Tag]
        .collect()
        .head
  
      val fiveTag = source.where('pid === fourTag.id)
        .as[Tag]
        .collect()
  
      (fourTag, fiveTag)
    }
  
    /**
     * 根据四级标签获取元数据
     *
     * @param fourTagID 四级标签ID
     * @return 元数据
     */
    def readMetaData(fourTagID: String): MetaData = {
      import spark.implicits._
      import org.apache.spark.sql.functions._
  
      val url = config.getString("jdbc.basic_tag.url")
      val table = config.getString("jdbc.meta_data.table")
      val matchColumn = config.getString("jdbc.meta_data.match_column")
  
      val metaData = spark.read
        .jdbc(url, table, new Properties())
        .where(col(matchColumn) === fourTagID)
        .as[MetaData]
        .collect()
        .head
  
      metaData
    }
  
    /**
     * 读取源表数据
     * 根据 MetaData, 判断源表在什么库中
     *
     * @param metaData 元数据
     * @return
     */
    def createSource(metaData: MetaData): (DataFrame, CommonMeta) = {
      if (metaData.isHBase) {
        val hbaseMeta = metaData.toHBaseMeta
        val source = ShcUtils.read(hbaseMeta.tableName, hbaseMeta.commonMeta.inFields, hbaseMeta.columnFamily, spark)
        return (source, hbaseMeta.commonMeta)
      }
  
      if (metaData.isRDBMS) {
  
      }
  
      if (metaData.isHDFS) {
  
      }
  
      (null, null)
    }
  
    /**
     * 保存画像数据
     *
     * @param result 计算过后的标签
     * @param outFields 计算后的标签列名
     */
    def saveUserProfile(result: DataFrame, outFields: Array[String]): Unit = {
      ShcUtils.writeToHBase(outFields, result, "5")
    }
  }
  ```

  



## 5. 基类抽取



* 具体的某一个 Model 的计算是一个 Worker 的形式
  * 元数据之类的处理
  * 流程控制之类的代码
  * 只做标签计算



整个标签的计算还可以进一步的抽取



* 将 Spark 的声明和 Config 的声明, 移动到基类中
* 将 main 方法移动到基类中
* 将基类变为 trait, 同时提供抽象方法获取标签名称和 process 过程, 给子类重写
* 基类调用抽象方法实现 main 方法的逻辑



代码如下



* JobModel

  ```scala
  object JobModel extends BasicModel {
  
    override def tagName(): String = "职业"
  
    override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
      import org.apache.spark.sql.functions._
      import spark.implicits._
  
      var conditions: Column = null
  
      for (tag <- fiveTags) {
        conditions = if (conditions == null)
          when('job === tag.rule, tag.id)
        else
          conditions.when('job === tag.rule, tag.id)
      }
      conditions = conditions.as(outFields.head)
      source.select('id, conditions)
    }
  }
  ```



* BasicModel

  ```scala
  trait BasicModel {
    val spark: SparkSession = SparkSession.builder()
      .appName("job model")
      .master("local[5]")
      .getOrCreate()
  
    val config: Config = ConfigFactory.load()
  
    def main(args: Array[String]): Unit = {
      // 1. 读取标签数据
      val (fourTag, fiveTags) = readBasicTag(tagName())
  
      // 2. 读取元信息
      val metaData = readMetaData(fourTag.id)
  
      // 3. 读取源表数据
      val (source, commonMeta) = createSource(metaData)
  
      // 4. 计算标签
      val result = process(source, fiveTags, commonMeta.outFields)
  
      // 5. 保存画像数据
      saveUserProfile(result, commonMeta.outFields)
    }
  
    /**
     * 抽象方法, 交由自类实现
     * 声明要计算的标签名
     */
    def tagName(): String
  
    /**
     * 抽象方法, 交由自类实现
     * 声明此标签对应的计算
     */
    def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame
  
    /**
     * 读取标签数据
     *
     * @param tagName 要读取的四级标签名
     * @return 四级标签和与之关联的五级标签
     */
    def readBasicTag(tagName: String): (Tag, Array[Tag]) = {
      val url = config.getString("jdbc.basic_tag.url")
      val table = config.getString("jdbc.basic_tag.table")
  
      val source: DataFrame = spark.read.jdbc(url, table, new Properties())
  
      import spark.implicits._
  
      val fourTag = source.where('name === tagName)
        .as[Tag]
        .collect()
        .head
  
      val fiveTag = source.where('pid === fourTag.id)
        .as[Tag]
        .collect()
  
      (fourTag, fiveTag)
    }
  
    /**
     * 根据四级标签获取元数据
     *
     * @param fourTagID 四级标签ID
     * @return 元数据
     */
    def readMetaData(fourTagID: String): MetaData = {
      import org.apache.spark.sql.functions._
      import spark.implicits._
  
      val url = config.getString("jdbc.basic_tag.url")
      val table = config.getString("jdbc.meta_data.table")
      val matchColumn = config.getString("jdbc.meta_data.match_column")
  
      val metaData = spark.read
        .jdbc(url, table, new Properties())
        .where(col(matchColumn) === fourTagID)
        .as[MetaData]
        .collect()
        .head
  
      metaData
    }
  
    /**
     * 读取源表数据
     * 根据 MetaData, 判断源表在什么库中
     *
     * @param metaData 元数据
     * @return
     */
    def createSource(metaData: MetaData): (DataFrame, CommonMeta) = {
      if (metaData.isHBase) {
        val hbaseMeta = metaData.toHBaseMeta
        val source = ShcUtils.read(hbaseMeta.tableName, hbaseMeta.commonMeta.inFields, hbaseMeta.columnFamily, spark)
        return (source, hbaseMeta.commonMeta)
      }
  
      if (metaData.isRDBMS) {
  
      }
  
      if (metaData.isHDFS) {
  
      }
  
      (null, null)
    }
  
    /**
     * 保存画像数据
     *
     * @param result 计算过后的标签
     * @param outFields 计算后的标签列名
     */
    def saveUserProfile(result: DataFrame, outFields: Array[String]): Unit = {
      ShcUtils.writeToHBase(outFields, result, "5")
    }
  }
  ```

  



## 6. 国籍标签



* 思路
  * 有 BasicModel 之前, 应该思考第一步做什么, 然后第二步做什么, 流程化
  * 有 BasicModel 之后, 应该思考的就不是流程, 应该从做什么事上思考
    * 通过什么名字查询数据
    * 怎么计算匹配
* 测试开发阶段
  1. 检查元数据
     1. 标签是否存在, `tags.tbl_basic_tag`
     2. 元数据是否存在, 是否有问题, `tags.tbl_metadata`
  2. 检查源表数据是否正确
     1. 检查源表数据是否完整, 比如说元数据中, 标注说读取 nationality, 坚持这个字段在用户表中是否存在
     2. mysql tag_dat -> hive -> hbase
     3. 真正执行的是hbase 的数据, 但是可以通过 mysql 中的 tags_dat 数据库查看表结构



```scala
object CitizenModel extends BasicModel {
  /**
   * 抽象方法, 交由自类实现
   * 声明要计算的标签名
   */
  override def tagName(): String = "国籍"

  /**
   * 抽象方法, 交由自类实现
   * 声明此标签对应的计算
   */
  override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
    import spark.implicits._
    import org.apache.spark.sql.functions._

    var conditions: Column = null

    for (tag <- fiveTags) {
      conditions = if (conditions == null)
        when('politicalface === tag.rule, tag.id)
      else
        conditions.when('politicalface === tag.rule, tag.id)
    }

    conditions = conditions.as(outFields.head)

    source.select('id, conditions)
  }
}
```

## 7. 政治面貌标签



```scala
object PoliticalModel extends BasicModel {
  /**
   * 抽象方法, 交由自类实现
   * 声明要计算的标签名
   */
  override def tagName(): String = "政治面貌"

  /**
   * 抽象方法, 交由自类实现
   * 声明此标签对应的计算
   */
  override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
    import spark.implicits._
    import org.apache.spark.sql.functions._

    var conditions: Column = null

    for (tag <- fiveTags) {
      conditions = if (conditions == null)
        when('politicalface === tag.rule, tag.id)
      else
        conditions.when('politicalface === tag.rule, tag.id)
    }

    conditions = conditions.as(outFields.head)

    source.select('id, conditions)
  }
}
```

