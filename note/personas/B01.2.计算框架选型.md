## 用户画像的计算框架选型

****
目标::
理解用户画像项目的几种实现方式

步骤::
1. 计算方式
2. 存储方式
****

#### 离线数仓

****
目标::
回顾离线数仓表结构

步骤::
1. 需求介绍
2. 技术演进
3. 总结
****

需求介绍::
+
****
* **需求介绍**
+
====
* **有一家公司做收银系统**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209225808.png[]
--

* **有一个需求是要统计 每个城市的店铺 每个月销售额 的走向**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209231145.png[width=600]
--
====

* **业务分析**
+
====
* 统计关系: 店铺每个月的销售额, 按照城市区分
* 店铺信息在店铺表中
* 城市信息在收货地址表中
* 订单时间在订单表中
* 店铺表和城市表和订单表关联
====

* **业务表**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209231535.png[]
====
****

技术演进::
+
****
* **直接在 MySQL 上查询**
+
====
1. 查询时会应用线上业务运行
2. 查询语句过于繁琐, 并且多个分析之间是有共性的, 中间层没有保存
3. 用于业务数据库的模型一般特别细节, 分析不方便
====

* **维度建模**
+
====
* **解决问题 2, 维度建模**
+
--
1. ODS, 贴源层, 做数据的存储, 当出现问题的时候不再二次抽取
2. DW,  数仓层, 维度建模, 简化查询
3. DM,  集市层, 小型数仓, 为每个部分服务
4. ADS, 应用层, 对应数据应用的需求, 例如便于报表访问等

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209231535.png[]
--

* **解决问题 3, 拉宽**
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209232303.png[]

* **表结构**
+
--
* ODS
** 订单表
** 订单明细表
** 用户表
** 店铺表
** 收货地址表
* DW
** 订单事实表
** 用户维度表
** 店铺维度表
** 地址维度表
** 时间维度表
--
====

* **架构升级, 解决问题 1**
+
====
* **数据抽取**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209234014.png[]
--

* **只能在 Hive 上做定制查询, 没办法做即席查询, 报表导出太慢**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209231145.png[width=600]

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200209234425.png[]
--
====
****

总结::
+
****
* 搭建离线数仓的好处
** 可以保留数据的中间计算, 避免浪费
** 数据以星型模型维度建模, 方便查询
* 需要理解业务处理过程
** DW层有: 订单事实, 用户维度, 店铺维度, 商品维度等
****

#### 实时数仓

****
目标::
回顾实时数仓, 理解实时数仓的架构模式

步骤::
1. 需求介绍
2. 技术演进
3. 总结
****

需求介绍::
+
****
* **大屏展示**
+
====
例如我们要做一个大屏, 实时的展示当前的业务情况, 其中有一个需求和离线数仓部分的需求类似

* **实时统计 每个区域订单占比**

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200210001858.png[]
====

****

技术演进::
+
****
* **直接使用计算工具从 Kafka 中读取数据计算结果**
+
====
* **架构**
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200210003524.png[]

* **缺陷**
+
--
* 计算会特别复杂
* 没有保留中间计算过程
--
====

* **搭建实时数仓**
+
====
* **架构**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200210011407.png[]
--

* **表结构**
+
--
总体上来说, 分层的思路和离线数仓是一致的, 大致如下

* ODS, 表以 Kafka Topic 的形式存在于 Kafka 中
** 订单表
** 收货地址表
* DW
** 订单事实明细, 在 Kafka 中
** 地区维度信息, 存在于 Redis 中
--

* **挑战**
+
--
* 维度数据如何从 Kafka 中同步到 Redis 中, 或者是否能够使用离线数仓的维度数据?
* 数据应用不止一种, 很多时候需要对 ADS 进行查询, OLAP 是否支持实时数据的摄入或者插入?
* 实时系统是不间断运行的, 如何保证高可用?
--
====
****

总结::
+
****
* 实时数仓和离线数仓的设计思路基本一致
* 需要理解业务处理过程
** ODS 层全部以 Topic 的形式存在于 Kafka, 其中有 订单 Topic, 收货地址 Topic
** DW 层的事实明细表存在于 Kafka 中, 其中有 订单 Topic
** DW 层的维度信息存在于 Redis 中, 其中有 收货地址信息
****

#### 离线画像

****
目标::
理解离线的用户画像如何实现

步骤::
1. 需求说明
2. 实现方式一: 使用数仓的方式
3. 实现方式二: 每个标签对应一个 Spark Job
4. 总结
****

需求说明::
+
****
* **需求分析**
+
====
* 一家电商公司的用户画像, 其中一个标签是客单价
* 客单价有四个级别
** 1-999
** 1000-2999
** 3000-4999
** 4000-5999
====

* **数据源分析**
+
====
* 用户表, 取得用户 ID, 相关联的订单数据
* 订单表, 取得订单价格

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200210180646.png[]
====

* **目标结果**
+
====
|===
| uid | avg_price

| 10001 | 128
| 10002 | 100
| 10003 | 50
|===
====
****

实现方式一: 使用数仓的方式::
+
****
* **思路**
+
====
* **假定数仓已经存在**
+
--
* ODS
** tbl_order, 订单表, 通过外键关联到用户表
** tbl_user, 用户表
* DW
** dw_ord_detail, 订单事实表
** dw_ord_user, 用户维度表

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200210194042.png[]
--

* **分析**
+
--
1. 关联查询订单和用户
2. 按照用户分组
3. 求得每一个用户组的所有订单加个均值
--
====

* **实现**
+
====
[source, sql]
----
SELECT 
    u.id AS uid, avg(amount) AS avg_price 
FROM dw_ord_detail o JOIN dw_ord_user u 
    ON o.buyer_id = u.id GROUP BY o.buyer_id
----
====

* **再加一个标签**
+
====
* **需求: 单笔最高 标签**

* **SQL**
+
[source, sql]
----
SELECT 
    u.id AS uid, 
    avg(amount) AS avg_price,
    max(amount) AS max_price
FROM dw_ord_detail o JOIN dw_ord_user u 
    ON o.buyer_id = u.id GROUP BY o.buyer_id
----
====

* **再加一个**
+
====
* **需求: 性别 标签**

* **SQL**
+
[source, sql]
----
SELECT 
    u.id AS uid, 
    avg(amount) AS avg_price,
    max(amount) AS max_price,
    u.gender AS gender
FROM dw_ord_detail o JOIN dw_ord_user u 
    ON o.buyer_id = u.id GROUP BY o.buyer_id
----
====

* **总结**
+
====
在 Hive 上做用户画像有一些固定的套路

* 从哪里取数据?
** ODS 层
** DW 层
* 如何计算?
** 使用 SQL, 加一个标签, 就加一个标签的查询
====

* **但是, 当标签复杂了以后...**
+
====
比如说有两个标签

* 客单价, 对用户所有的订单进行统计, 求得平均价格
* 当月购买频次, 对用户当月的订单进行统计, 求得次数

这两个标签的统计范式是不一样的, 这样的 SQl 写起来就比较复杂了, 一般有两种写法

* 使用窗口函数, 比如统计当月的, 就可以开一个窗口来统计
* 使用并集, 比如对不同统计范围的数据分开统计, 最终合并起来
* 使用临时表, 和使用并集的思路一样, 但是是把数据暂存起来

所以, 使用 SQL 固然是简单的, 但是也有一些局限性
====
****

实现方式二: 每个标签对应一个 Spark Job::
+
****
* **分析**
+
====
1. 使用 Spark 从 Hive 中的 ODS 或者 DW 取数据
2. 使用 Spark DSL 计算客单价
3. 存入画像表
====

* **实现**
+
====
[source, scala]
----
val spark = SparkSession
      .builder()
      .appName("Spark SQL on hive")
      .master("spark://192.168.4.4:7077")
      .config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()

val source = spark.sql("SELECT u.id AS uid, o.amount AS amount FROM tbl_order o JOIN tbl_user u on o.uid = u.id")
val result = source.select('id, avg('amount))

result.write.xxx.save()
----

可以非常容易的想到, 如果一个标签对应一个 Job 的话, 会有两个好处

* 每个标签独立计算, 每个 Job 都不会很难写
* 每个标签独立计算, 不存在临时表之类的麻烦事, 好管理一些
====

* **缺点**
+
====
这种方式也有两个很明显的缺点

1. 每个标签独立计算, 会增加很多工作量
2. 每个标签独立计算, 有多少标签就要有多少个 Job, 全量计算的时候, 集群可能会有几百个 Spark job 的负载

问题1 基本上无解, 想少写点代码就必然会写的很复杂, 但是 问题2 可以用以下的办法解决

* 按量设计 Spark Job 的资源占用, 比如说某些任务计算不复杂, 可以少设置一些资源
* 在合理范围内, 尽可能的减少更新, 当然这样会影响画像的准确性
* 多花点钱整个大集群[手动憨笑]
====
****

总结::
+
****
* 无论使用 Hive 的方式还是 Spark 的方式, 数据来源都是数仓中的 ODS 层或者 DW 层
* 使用什么方式没有正确答案, 每个公司有自己的选择
** 使用 Hive 会不好管理, 并且遇到口径和时间范围不同的统计则可能会比较麻烦
** 使用 Spark 会多占用一些资源, 并且多写点代码
** 使用 Hive 还有一个很大的问题, 画像一般是需要进行机器学习的处理的, 因为你要挖掘用户的喜好, 基于 Hive 是没办法机器学习的, 需要引入其他的工具, 而 Spark 本身便有机器学习的库
****

#### 实时画像

实时画像部分不再详细的说了, 有如下几个原因

* 本部分主要说明的是如何处理, 而讲实时更多的是讲架构
* 实时画像和实时数仓的处理流程几乎一致, 所遇到的问题也类似

### 用户画像的存储选型

****
目标::
理解画像系统如何选择存储系统

步骤::
1. 画像应用的访问特点
2. MySQL 的特点
3. HBase 的特点
4. ES 的特点
5. 总结
****

#### 画像应用的访问特点

****
目标::
+
理解画像应用对存储的要求是什么, 从而理解如何选型

步骤::
+
1. 画像的应用场景
2. 画像对存储的要求
3. 选型
****

画像的应用场景::
+
****
* **假设用户画像表如下**
+
====
|===
| 用户 ID | 购买品类 | 浏览品类 | 所在商圈

| 10001 | 男装 | 女装 | 西单
| 10002 | 数码 | 图书 | 东单
| 10003 | 家居 | 家居 | 王府井
|===
====

* **分群营销**
+
====
例如公司现在新上了一批男装, 希望找到经常购买男装的用户, 发送短信营销, 我应该使用如下的查询方式找到这些用户

[source, sql]
----
SELECT id FROM up_buying WHERE 购买品类=男装;
----

* 如果有多个查询条件, 这就是一个多字段组合查询
====

* **产品优化**
+
====
现在要做一个新版本的程序, 上线一个功能是和线下合作的, 为了更精准的定义产品风格, 产品经理想看看我们的用户哪些商圈的比较多

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200211223847.png[width=450]

[source, sql]
----
SELECT id, 商圈, count(商圈) AS cnt FROM up_loc GROUP BY 所在商圈;
----

* 这是一个统计聚合
====
****

画像对存储的要求::
+
****
* **画像表比较稀疏**
+
====
* 一般一个用户画像的项目, 标签至少也是几十上百
* 如果一个画像项目, 有大量用户被打上了所有标签, 这个画像项目是非常不合格的, 质量很差, 所以应该是绝大部分用户都只有一部分标签
* 得出一个规律, 画像表应该很稀疏
====

* **多见匹配查找, 例如通过 ID 找用户标签, 通过标签找用户群体**
+
====
无论按照标签匹配还是按照 ID 匹配, 都是非常简单的查询, 几乎所有数据都都可满足
====

* **多见聚合查询, 例如查看某个标签的占比**
+
====
聚合也是一个非常简单的查询方式, 大多数数据库都可以完成
====

* **所以, 可以得出画像对存储的要求如下**
+
====
* 稀疏表的存储不应占用太多资源, 因为很多数据库即使某个位置没有数据, 也依然会分配存储空间
* 查询种类多样化
** 按 Key 查询
** 多条件组合查询
** 统计聚合
** 嵌套查询
====
****

#### MySQL 的特点

****
目标::
+
理解 MySQL 最底层的数据结构, 从而理解 MySQL 这个数据库的特点

步骤::
+
1. B树
2. B+树
3. MySQL 的特性
****

B树::
+
****
* **什么是树**
+
====
最简单的树肯定当之无愧是二叉树

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212012102.png[width=260]

缺点: 在这棵树里, 似乎想查找某个值不太容易啊
====

* **查找二叉树**
+
====
再看看这棵树, 是不是查找就更容易一点了?

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212012319.png[width=260]
====

* **节点有相对顺序的树的作用**
+
====
* 在查找一颗节点之间有顺序的树时, 可以使用类似于折半查找的方式查找, 时间复杂度为 stem:[Log_2N], 这已经是搜索算法的最低时间复杂度了
* 而数据库中的索引, 就是为了增快查询速度的
* 综上所述, 数据库中的 **索引** 的数据结构就可以使用类似于查找二叉树这样有相对顺序的树
====

* **假设每次查找都要读取磁盘, 当查找元素 6 的位置时, 有多少次 IO**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212014325.png[width=350]

* 由上图可知, 四次
* 树的高度是多少? 四次
* 可得知, 每次查找的次数, 是整棵树的高度
====

* **有一个性能问题**
+
====
* 完全二叉树的高度 `h` 和节点个数 `c` 之间的公式是: stem:[c = 2^h - 1]
* 一千万条数据, 深度大概为 三千层 上下
* 也就是说, 查找一条数据, 最大可能要 IO 三千次
* CPU 执行一条执行的时间大概是 `0.5ns`, 而访问一次机械硬盘的寻址时间大概是 `10ms = 10000ns`, 两万倍, 感受一下这个速度
* 所幸, 操作系统不会那么傻每次都去读, 而是一次读一个页(`page`), 如果数据在硬盘上存储位置相邻, 则不会有那么多 IO

如何解决这个问题呢? 让层级变少即可
====

* **B树 的结构**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212045019.png[width=400]

如上是一个 `2-3 树`, 是一个 B树 的形式, 含义有两个, 在这里是: "每个节点有两个数据元素, 每个节点有三个子节点, 每个叶子节点有两个数据元素"

无论是什么形式的 B树, 都具备以下定理, 这四个定理也是保证 B树 插入和删除能够平衡的原因

1. 根节点至少两个子节点
2. 每个中间节点都包含 `m` 个孩子, 每个中间节点都包含 `m - 1` 个数据元素
3. **最底层的节点称之为叶子节点**, 所有叶子节点都位于同一层
4. 所有节点中的数据元素按照大小排列, 所有子节点按照数据元素的大小排列, 父节点的数据元素恰好是子节点数据元素的值域划分点

使用伪代码表示每个元素如下

[source, scala]
----
class BTree[Key, Value] {
    val m: Int = 3  // 阶, 节点元素最大个数为树的阶, 例如 2-3树 就是一个 3阶 的树

    /**
     * 节点, 每个节点可以包含多个数据元素和多个子节点
     */
    class Node {
        val maxChildren: Int = m      // 最大节点数
        val maxEntry: Int = m - 1     // 最大数据元素数

        val firstEntry: Entry = null  // 数据元素链表的第一个位置
        val children: List[Node] = List.fill(maxChildren)(null) // 孩子节点
    }

    /**
     * 数据元素
     */
    class Entry {
        val next: Node = null   // 链表中下一个位置

        val key: Key = null     // 用于比较的 Key
        val value: Value = null // 值 Value
    }
}
----
====

* **B树 的查找**
+
====
按照定理 4, 查找也类似于折半, 比如说查找 5

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212045225.png[width=400]

值得注意一点的是: B树 的查找, 其时间复杂度并不比等量的二叉树小
====

* **B树 插入的规则**
+
====
* 如果当前节点未满, 插入
* 如果当前节点已满, 分裂节点, 中间大小的值提升, 直到插入根节点
* 如果根节点也已满, 插入节点成为新的根节点, 层级 +1
====

* **B树 的插入**
+
====
比如说我要插入一个节点 `4`, 流程如下

1. 从 root 向下查找, 找到 `4` 的位置
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212045410.png[width=400]

2. 当前节点 `3,5` 已满, 按照规则, 需要把节点 `3,5` 拆开, 并且 `4` 向上提升
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212045908.png[width=400]

3. 当前节点 `2,6` 已满, 按照规则, 节点 `2,6` 也需要拆开, 并且 `4` 再次向上提升
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212050306.png[width=400]

4. 刚好根节点未满, 插入根节点
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212050356.png[width=400]
====
****

B+ 树::
+
****
* **B树 的问题 1 - 性能不稳定**
+
====
* **原因**
+
--
因为 B树 中所有节点都可携带数据元素, 所以导致性能不稳定, 例如

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212045019.png[width=400]

* 查找 Key 为 6 的数据, 在第二层找到, IO 两次
* 查找 Key 为 1 的数据, 在第三层找到, IO 三次
--

* **解决方案**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212152320.png[width=400]

* 除了叶子节点以外, 其它节点不再携带数据元素, 这样所有的查找都会落到叶子节点上
* 这样会导致一个新的问题
** 外部向 BTree 中添加元素的时候是给定 Key 和 Value 添加的
+
[source, scala]
----
class BTree[Key, Value] {
    ...

    def insert(key: Key, value: Value) = {

    }
}
----
** 也就是说, 如果中间节点不再携带数据元素就要自己生成了, 如何生成?
--

* **解决方案 2**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212153743.png[width=400]

* 让叶子节点包含父节点
* 这样叶子节点就包含了所有数据元素, 而中间节点则只有一个作用, 就是划分叶子节点的值域
--
====

NOTE: 遗留问题: 中间节点不再携带数据, 则无法查找, 需要自己生成

* **B树 的问题 2 - 范围查找效率太低**
+
====
* **原因**
+
--
在 B树 中, 查找一个范围的话, 需要使用树的中序查找, 例如在树中查找 `select ... from ... where x >= 3 and x <= 11`, 大概的步骤有 9步, 如下

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212155745.png[width=400]
--

* **解决方案**
+
--
在 问题1 的解决方案基础之上, 让叶子节点成为一张链表

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212154831.png[width=400]

这样再执行 `select ... from ... where x >= 3 and x <= 11` 时候, 步骤变为 8步, 并且范围涉及的中间越多, 差距就越明显

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212155528.png[width=400]
--
====

* **这样的树, 我们称之为 B+ 树**
+
====
* **B+树**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212160052.png[width=400]
--

* **B+树 的特性**
+
--
* 有 k 个子树的中间节点, 就可以存放 K 个数据元素(比 B树 多一个)
* 中间节点不保存数据, 只用来索引, 划分子树值域, 所有数据元素都以卫星的形式和叶子节点关联
* 叶子节点本身按照 Key 有序
* 所有中间节点的元素都存在于子节点
--

* **B+树 的优点**
+
--
* 单一节点存储更多的元素, IO 次数变少
* 所有查询都要查找到叶子节点, 看起来每次都是都是最差情况, 但是三层的 B+树 可以存放一百万条数据, 通常 B+树 都很低很宽
* 所有叶子节点是形成有序链表, 范围查询性能极强
--
====

* **B+树 和 MySQL 的关系**
+
====
* **从存储的脚上来看, 索引的类型**
+
--
* 聚集索引
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212164520.png[width=400]

* 非聚集索引
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212164737.png[width=400]
--

* **MySQL 的索引类型**
+
--
在 MySQL 中, 有两个引擎, 如下

* MyISAM, 早期的引擎, 事务支持很差, 极少使用
* InnoDB, 优化的引擎, 事务支持完备, 最为多见

InnoDB 有如下特点

* 任何一张表的数据都自带一个聚集索引
* 默认情况下, 建表必须有主键, 默认的聚集索引以主键为 Key
--

* **B+树 和 MySQL 的关系**
+
--
无论是否聚集, MySQL 中的索引通通都是 B+树 结构
--
====
****

MySQL 的特性::
+
****
* 随着数据的增多, 插入性能递减
+
====
根据 B+树 的特性可以知道, 每次在插入的时候都比较复杂, 当数据量增多的时候, 性能衰减会非常明显
====

* 查找延迟低
+
====
B+树 是查找树, 其节点之间是有序的, 当需要搜索的时候, 时间复杂度和折半查找一样, 只有 stem:[Log_2N]
====

* 范围查询优势明显, 可以实现复杂的查询
+
====
B+树 的叶子节点构成了一个类似链表的结构, 所以进行范围查找的时候, 不需要回到父节点, 可以直接在子节点中进行, 所以在进行一些复杂查询的时候比较方便范围取数据
====

* 完整存储所有数据
+
====
因为 MySQL 的主要目的是 OLTP, OLTP 更强调每次操作一条或者多条数据, 所以 MySQL 是行存储的形式, 行存储为了对齐所有的列, 即使某列为 Null, 也依然会有按照数据类型的占位
====
****

#### HBase 的特点

****
目标::
+
通过 HBase 的核心数据结构, 理解 HBase 的特点

步骤::
+
1. MySQL 的问题
2. LSM Tree
3. HBase 的特点
****

MySQL 的问题::
+
****
* **插入性能会随着树的复杂度而递减**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200212160052.png[width=400]

* 如果数据太多, 是不是这个树会非常宽, 甚至有几千万的量, 这个树就不只三层了
* 这个时候插入一条数据的话, 要不断的对比, 导致复杂度升高
* 所以随着数据量的增大, 这棵树的插入性能会下降
====

* **行存储**
+
====
|===
| id | name | school | age

| 1 | 张三 | MIT | 10
| 2 | 李四 | null | 10
| 3 | 王五 | 清华 | null
|===

* 如上, 有一些行中有空值
* 但是 Schema 已经定义了, 对应的列是有数据类型的, 为了对齐, 所以即使这个地方没有数据, 也依然需要占用对应数据类型的空间
====
****

LSM Tree::
+
****
* **解决问题: 插入效率低**
+
====
* 多颗小树
* 不写磁盘, 内存效率更高

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213003534.png[width=400]
====

* **新的问题: 内存不持久啊**
+
====
* 把数分级
* 一级存盘, 一级内存

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213004847.png[width=600]
====

* **新的问题: 不能内存和磁盘数据不同步呀**
+
====
* 使用查找树, 这样树的节点之间是有序的
* 两颗有序的树可以使用归并排序, 时间复杂度很低
* 这个过程叫做 Flush, Level 0 级别的树达到容量阈值的时候, Flush 到 Level 1 的树

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213011528.png[width=600]
====

* **归并排序**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213010712.png[width=300]
====

* **新的问题: 如果数据量太大, 即使是归并, 也是需要耗时的, 还能不能进一步快?**
+
====
* 内存大小毕竟有限, 阈值可能比较小, 所以刷新到磁盘中是经常发生的事情
* 如果一次性把整棵树先刷新到磁盘, 在特定的时间合并这些小树, 就可以进一步的提升速度

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213011825.png[width=800]
====

* **这个朴素的思想, 就叫做 LSM Tree, 日志合并树**
+
====
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213012423.png[width=500]

* 数据的结构为 B树 比较多见, 但是不同的数据库有不同的实现, HBase 中每一颗小树都类似于 B树
* 插入性能及其优良, 大概比 B+树 快几十倍
* 查询性能比较差, 因为要扫描三个级别的存储, 比 B+树 要慢几十倍
====
****

HBase 的特点::
+
****
* **HBase 和 LSM 树**
+
====
* **HBase 的一个表有多个 Region 分布在多个 RegionServer 上, 一个 RegionServer 有多个 Region**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213031242.png[width=500]
--

* **每个 Region 又分为 Memstore 和 DiskStore, 其实就是 LSM树**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213031445.png[width=500]
--

* **HBase 的存储结构是 Key-Value**
+
--
虽然 HBase 对外提供的看起来好像一种表, 但其实在 Region 中, 数据以 KV 的形式存在

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213034740.png[width=500]
--
====

* **哈希函数**
+
====
* **哈希函数**
+
--
哈希函数就是将一个不固定长度的数据转为固定长度的数据的算法, 很多时候也叫做摘要算法

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213023013.png[width=500]
--

* **哈希冲突**
+
--
因为一般情况下, 哈希是把长度比较长的数据转为比较短的形式, 所以可能存在多个数据的哈希结果是一样的, 这种现象称之为冲突

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213023241.png[width=500]
--

* **比较常见的哈希函数**
+
--
* MD5, 产生 128位 哈希结果
* SHA1, 产生 160位 哈希结果
* SHA256, 产生 256位 哈希结果
--
====

* **布隆过滤器**
+
====
* **需求: 在一个非常大的数据集合中, 快速的确认某个元素是否存在?**

* **思路 1: 使用哈希表**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213025525.png[width=500]
--

* **问题 1: 虽然查的快了, 但是这个哈希表一定很大**

* **思路 2: 映射为固定长度的位数组**
+
--
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213030027.png[width==500]
--

* **问题 2: 看起来好像会产生大量哈希冲突, 起不到应有的作用**

* **思路 3: 多次哈希, 这样产生冲突的概率就小多了**
+
--
* 放入 baidu
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213030201.png[width=500]

* 放入 tencent
+
image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213030250.png[width=500]
--

* **思路 3 就是我们所说的布隆过滤器**
+
--
如果希望判断一个值是否在一个很大的数据集中, 可以生成一个很小的布隆过滤器, 然后把要查询的值哈希一次, 代入布隆过滤器

* 如果对应位置的值为 0, 则这个值一定不存在
* 如果对应位置为 1, 则这个值有可能存在
--
====

* **优化 1 : 使用布隆过滤器提升查询性能**
+
====
* **HBase 的 LSM 结构中, Level 2 和 Level 3 都是 HFile**
+
--
HFile 是 HBase 自定义的一种文件格式

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213032340.png[width=500]
--

* **每个 HFile 都包含一个 布隆过滤器**
+
--
* 在 HBase 中查找数据时候, 会扫描整张表的所有 Region, 以及 Region 的 Memstore 和 HFile
* 每个 HFile 都包含一个布隆过滤器, 则可以快速的跳过一部分 Region, 提升查找速度

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213032500.png[width=500]
--
====

* **优化 2 : HFile 的合并**
+
====
* **小合并: Minor Compaction**
+
--
* 小合并一般发生在 Memstore 刷写为 HFile 时, 达到阈值则产生一次合并, 常见如果刷写的 HFile 太小则合并

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213032833.png[width=500]
--

* **全合并, Major Compaction**
+
--
* 全合并一般周期性发生, 例如 24 小时, 合并期间会导致集群 IO 被大量占用, 影响 HBase 的响应时间

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213033233.png[width=500]
--

* **总结**
+
--
* 对照 LSM树, Memstore 是 Level 0, Memstore 刷写的 HFile 就是 Level 1, Major Compaction 后是 Level 2
* 通过把合并分为两种, 会将 IO 分散在不同的时间片, 让 HBase 的运行更加流畅, 性能也更加好
--
====

* **优化 3 : 读优化**
+
====
* **一级缓存: BlockCache**
+
--
* MySQL 的 B+树 并不是把数据直接存放在树中, 而是把数据组成 页(Page) 然后再存入 B+树, MySQL 中最小的数据存储单元是 Page
* HBase 也一样, 其最小的存储单元叫做 Block, Block 会被缓存在 BlockCache 中, 读数据时, 优先从 BlockCache 中读取
* BlockCache 是 RegionServer 级别的
* BlockCache 叫做读缓存, 因为 BlockCache 缓存的数据是读取返回结果给客户端时存入的

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213214539.png[width=500]
--

* **二级缓存: 当查找数据时, 会先查内存, 后查磁盘, 然后汇总返回**
+
--
* 因为写是写在 Memstore 中, 所以从 Memstore 就能立刻读取最新状态
* Memstore 没有的时候, 扫描 HFile, 通过布隆过滤器优化读性能

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213031918.png[width=500]
--
====

* **总结**
+
====
* HBase 是 LSM树 的一种开源实现, 类似的还有 LevelDB, RocketDB 等
* HBase 无论是批量写还是实时写, 性能都超过 MySQL 不少
* HBase 的查询只有一种, 就是扫描, Get 也是扫描的一种特殊情况, 所以 HBase 的查询能力不强
* HBase 以 KV 的形式存储数据, 所以如果某一单元数据为 Null 则不存, 所以 HBase 适合存储比较稀疏的表
====
****

#### 选型

****
目标::
+
最终确认选型方案

步骤::
+
1. 画像类型系统访问特点
2. MySQL 的特点
3. HBase 的特点
4. 选型
****

画像类型系统的访问特点::
****
* 画像表比较稀疏
* 多见匹配查找, 例如通过 ID 找用户标签, 通过标签找用户群体
* 多见聚合查询, 例如查看某个标签的占比
****

MySQL 的特点::
+
****
* 随着数据的增多, 插入性能递减
* 查找延迟低
* 范围查询优势明显, 可以实现复杂的查询
* 完整存储所有数据, 不适合稀疏表
****

HBase 的特点::
+
****
* HBase 无论是批量写还是实时写, 性能都超过 MySQL 不少
* HBase 的查询只有一种, 就是扫描, Get 也是扫描的一种特殊情况, 所以 HBase 的查询能力不强
* HBase 以 KV 的形式存储数据, 所以如果某一单元数据为 Null 则不存, 所以 HBase 适合存储比较稀疏的表
****

ES 的特点::
+
****
虽然没有单独去说基于 Lucene 的 ElasticSearch 和 Solr, 但是也总结一下他们的特点

* 适合一次索引, 多次搜索的场景, 没有真正的更新能力
* 不如 HBase 的数据存储能力强, 适合只存储索引数据
* 所以一般使用 ES或Solr + HBase 的方式, ES或Solr 做搜索, HBase 做存储和简单查询
****

选型::
+
****
* **投票**
+
====
1. 从存储形式上来看, 选 HBase, HBase 是 KV 型数据库, 是不需要提前预设 Schema 的, 添加新的标签时候比较方便
2. 从使用方式上来看, 选 MySQL 似乎更好, 但是 HBase 也可以, 因为并没有太多复杂查询
3. 从写入方式上来看, 选 HBase, 因为画像的数据一般量也不小, HBase 可以存储海量数据, 而 MySQL 不太适合集群部署

最终选择 HBase, 同时可选的方案还有 Hive, ES 等
====

* **优化**
+
====
可以看到 HBase 的唯一劣势就是查询能力较差, 所以, 可以选择使用 ES 或者 RowKey 作为 HBase 的二级索引, 这个部分后面还会详细说明

image::https://doc-1256053707.cos.ap-beijing.myqcloud.com/20200213035849.png[width=500]
====
****