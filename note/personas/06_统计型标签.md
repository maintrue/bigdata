# 统计型标签

* 目标
  * 理解统计型标签的计算和匹配方式
* 步骤
  1. 年龄段标签
  2. 支付方式标签
  3. 最近一次的支付方式
  4. 消费周期标签



## 1. 年龄段标签



1. 需求整理
   1. 和业务方沟通需求
   2. 创建标签
      1. 创建四级标签, 业务标签, 年龄段
      2. 创建五级标签, 年龄段的取值范围, 50后, 60后, 70后...
   3. 整理思路
      1. 用户直接填写生日, 或者年龄
         1. 拿到每一个用户的生日
         2. 对比每一个标签的范围, 进行匹配
      2. 用户没有填写
         1. 通过用户的行为, 购买了某些产品, 特定的行为特点
         2. 学习型算法和普通算法, 最大的区别, 就是学习型算法能够在不断运行的过程中, 优化性能
2. 代码实现
   1. 检查元数据
   2. 处理数据
   3. 标签匹配



```scala
override def process... = {
  import org.apache.spark.sql.functions._
  import spark.implicits._

  var conditions: Column = null

  for (tag <- fiveTags) {
    val timeSplit = tag.rule.split("-")
    val start = timeSplit(0).toInt
    val end = timeSplit(1).toInt
    val birthday = regexp_replace('birthday, "-", "").cast(IntegerType)

    if (conditions == null) {
      conditions = when(birthday.between(start, end), tag.id)
    } else {
      conditions = conditions.when(birthday.between(start, end), tag.id)
    }
  }

  conditions = conditions.as(outFields.head)

  source.select('id, conditions)
}
```





## 2. 支付方式标签(最多)



* 需求

  * 统计每个用户最多的支付方式, 打上支付标签
  * 每个人会有多个订单
  * 每个订单的支付方式不同
  * 如果要为一个用户打上支付方式的标签, 那么应该怎么打, 针对哪个订单打支付方式的标签
    * 所有订单中最多的支付方式, 标签
    * 最近一笔订单所使用的支付方式

* 步骤

  * 查看数据

  1. 计算每个用户的不同支付方式的数量
  2. 统计数量最多的支付方式
  3. 打上标签



1. 计算得到 **每一个用户** 的 **所有支付方式** 的数量

   ```scala
   override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
     import spark.implicits._
     import org.apache.spark.sql.functions._
   
     source.groupBy('memberid, 'paymentcode)
       .agg(count('paymentcode) as "count")
   }
   ```

   <img src="assets/image-20200227224454774.png" alt="image-20200227224454774"  />

2. 错误的方式: 基于 **用户** 分组, 计算得到 **每一个用户** 的 **最大数量** 的 **支付方式**

   1. 规律: groupby 的结果集中, 只有 groupby 的内容, 和 agg 的内容列
   2. 要想清楚在组中取一个值的话, 取得是哪一个? 在分组中, 无论取哪个值, 都是聚合操作, 如果没有聚合, 取得就是第一个值

   ```sql
   # 这是一个错误的说明
   select 
          s.memberId, s.paymentCode, max(s.count)
   from (
             select 
                    memberId, paymentCode, count(paymentCode) as "count"
             from tbl_orders o
             group by memberId, paymentCode
         ) s
   group by memberId;
   ```

   ```scala
   override def process... = {
     import spark.implicits._
     import org.apache.spark.sql.functions._
   
     // 报错: cannot resolve '`paymentcode`' given input columns: [memberid, max(count)]
     // 原因: 没有 paymentcode 列, 为什么没有? 详见迷思1
     source.groupBy('memberid, 'paymentcode)
       .agg(count('paymentcode) as "count")
       .groupBy('memberid)
       // 迷思1: 如果这里在按照用户 ID 分组的一个组中取得最大的 count
       .agg(max('count))
       // 迷思1: 那么这里的 paymentcode 又是取的这个组中哪一个值?
       .select('memberid as "id", 'paymentcode)
   }
   ```

3. 正确的方式: 使用窗口

   ```scala
   override def process... = {
     import spark.implicits._
     import org.apache.spark.sql.functions._
   
     source.groupBy('memberid, 'paymentcode)
       .agg(count('paymentcode) as "count")
       // 按照用户分组, 在组内按照count排序, 并生成序号
       .withColumn("row_num", 
                   row_number() over Window.partitionBy('memberid).orderBy('count desc))
       .where('row_num === 1)
       
     var conditions: Column = null
     for (tag <- fiveTags) {
       if (conditions == null) {
         conditions = when('paymentcode === tag.rule, tag.id)
       } else {
         conditions = conditions.when('paymentcode === tag.rule, tag.id)
       }
     }
     stated.select('id, conditions)
   }
   ```

   



## 3. 支付方式标签(最近)



* 需求
  * 统计最近一次支付方式标签
  * 需要根据时间, 取时间最大的标签



```scala
override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val stated = source.select(
      'memberid,
      row_number() over Window.partitionBy('memberid).orderBy('paytime desc) as "rn",
      'paymentcode
    )
    .where('rn === 1)
    .select('memberid as "id", 'paymentcode)

  var conditions: Column = null
  for (tag <- fiveTags) {
    if (conditions == null) {
      conditions = when('paymentcode === tag.rule, tag.id)
    } else {
      conditions = conditions.when('paymentcode === tag.rule, tag.id)
    }
  }
  stated.select('id, conditions)
}
```



## 4. 消费周期



* 需求
  * 最近一次消费在什么时候



```scala
override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val stated = source.select('memberid as "id", 'finishtime.cast(LongType) as "finishtime")
    .groupBy('id)
    .agg(max('finishtime) as "finishtime")
    .select('id, datediff(current_timestamp(), from_unixtime('finishtime)) as "dayPass")

  stated.show()

  var conditions: Column = null
  for (tag <- fiveTags) {
    val dayArray = tag.rule.split("-")
    val start = dayArray(0)
    val end = dayArray(1)

    if (conditions == null) {
      conditions = when('dayPass.between(start, end), tag.id)
    } else {
      conditions = conditions.when('dayPass.between(start, end), tag.id)
    }
  }
  conditions = conditions.as(outFields.head)

  stated.select('id, conditions)
}
```

