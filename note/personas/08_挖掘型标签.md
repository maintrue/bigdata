# 挖掘型标签

* 目标
  * 理解用户画像中常见的挖掘型标签计算方式
* 步骤
  1. RFM 用户价值标签
  2. PSM 价格敏感度标签
  3. 购物性别



## 1. RFM 用户价值模型

* 目标
  * 能够独立完成价值标签的开发
* 步骤
  1. 需求
  2. RFM 是什么
  3. RFM 的实际应用
  4. 实现方式
  5. 聚类
  6. 模型训练
  7. 预测和排序
  8. RFE



### 1.1. 需求



假设我是一个市场营销者, 在做一次活动之前, 我可能会思考如下问题



* 谁是我比较有价值的客户?
* 谁是比较有潜力成为有价值的客户?
* 谁快要流失了?
* 谁能够留下来?
* 谁会关心这次活动?



其实上面这些思考, 都围绕一个主题 **价值**

`RFM` 是一个最常见的用来评估价值的和潜在价值的工具



### 1.2. RFM 是什么



RFM 通过最后一次消费距今时间, 单位时间内的消费频率, 平均消费金额来评估一个人对公司的价值, 可以理解为 RFM 是一个集成的值, 如下



`RFM  = Rencency(最后一次消费时间),  Frequency(消费频率), Monetary(消费金额)`



RFM 模型可以说明如下事实



* 最近一次购买时间越近, 用户对促销越有感
* 购买频率越高, 对我们满意度就越高
* 消费金额越大, 越有钱, 越是高消费人群



<img src="assets/image-20200301173934148.png" alt="image-20200301173934148" style="zoom: 67%;" />



虽然看起来 RFM 只是三个维度, 但其组合起来可以完成诸多需求



![](assets/yunying1-1540648250.jpeg)



### 1.3. RFM的实际应用



![image-20200301182821173](assets/image-20200301182821173.png)



### 1.4. 实现方式

> * RFM 本质上就是按照 R, F, M 三个维度对用户打分
> * 但是我们的需求是用户分群, 所以打分过后, 需要使用一定的分类算法对用户进行分类



1. 计算 RFM

   | User ID | R 最近消费 | F 消费频率 | M 消费金额 |
   | ------- | ---------- | ---------- | ---------- |
   | 1       | 2019-11-01 | 5          | 1000       |
   | 2       | 2019-11-02 | 40         | 800        |

2. 通过打分统一量纲

   * **R**: 1-3天=5分，4-6天=4分，7-9天=3分，10-15天=2分，大于16天=1分
   * **F**: ≥200=5分，150-199=4分，100-149=3分，50-99=2分，1-49=1分
   * **M**: ≥20w=5分，10-19w=4分，5-9w=3分，1-4w=2分，<1w=1分

3. 统一量纲

   | User ID | R    | F    | M    |
   | ------- | ---- | ---- | ---- |
   | 1       | 5    | 1    | 2    |
   | 2       | 1    | 1    | 1    |

4. 聚类, 把用户数据根据 RFM 值, 自动的分组



### 1.5. 聚类



* 聚类算法的本质

  ![image-20200301183024389](assets/image-20200301183024389.png)

* 聚类的实现

  1. 选择 K 个点作为初始中点
  2. 计算每个中点到相近点的距离, 将相近的点聚在一类(簇)
     * 欧式距离
  3. 重新计算每个簇的中点
  4. 重复迭代上面步骤, 直至不再发生变化

  ![image-20200301183321338](assets/image-20200301183321338.png)



http://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/



### 1.6. 模型训练



* 一般我们会将模型训练和预测分开, 这样模型训练和预测可以以不同的节奏调度
* 模型训练对应一个 Job, 预测对应一个 Job
* 要使用聚类, 是因为比较 RFM 值是否相近, 应该同时考虑三个值, 需要放在高维空间做向量对比



```scala
object RFMModel extends BasicModel {
  val MODEL_PATH = "/models/rfm/kmeans"

  def main(args: Array[String]): Unit = {
    startFlow()
  }

  override def tagName(): String = "客户价值"

  override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
    val assembled = assembleDataFrame(source)

    val regressor = new KMeans()
      .setK(7)
      .setSeed(10)
      .setMaxIter(10)
      .setFeaturesCol("features")
      .setPredictionCol("predict")

    regressor.fit(assembled).save(MODEL_PATH)

    null
  }

  def assembleDataFrame(source: DataFrame): DataFrame = {
    import spark.implicits._
    import org.apache.spark.sql.functions._

    // 1. 求得 RFM
    val rCol = datediff(date_sub(current_timestamp(), 190), from_unixtime(max('finishtime))) as "r"
    val fCol = count('ordersn) as "f"
    val mCol = sum('orderamount) as "m"

    val rfm = source.groupBy('memberid)
      .agg(rCol, fCol, mCol)

    // 2. 为 RFM 打分
    // R: 1-3天=5分，4-6天=4分，7-9天=3分，10-15天=2分，大于16天=1分
    // F: ≥200=5分，150-199=4分，100-149=3分，50-99=2分，1-49=1分
    // M: ≥20w=5分，10-19w=4分，5-9w=3分，1-4w=2分，<1w=1分
    val rScore = when('r >= 1 and 'r <= 3, 5)
      .when('r >= 4 and 'r <= 6, 4)
      .when('r >= 7 and 'r <= 9, 3)
      .when('r >= 10 and 'r <= 15, 2)
      .when('r >= 16, 1)
      .as("r_score")

    val fScore: Column = when('f >= 200, 5)
      .when(('f >= 150) && ('f <= 199), 4)
      .when((col("f") >= 100) && (col("f") <= 149), 3)
      .when((col("f") >= 50) && (col("f") <= 99), 2)
      .when((col("f") >= 1) && (col("f") <= 49), 1)
      .as("f_score")

    val mScore: Column = when(col("m") >= 200000, 5)
      .when(col("m").between(100000, 199999), 4)
      .when(col("m").between(50000, 99999), 3)
      .when(col("m").between(10000, 49999), 2)
      .when(col("m") <= 9999, 1)
      .as("m_score")

    val scores = rfm.select('memberid as "id", rScore, fScore, mScore)

    val assembled = new VectorAssembler()
      .setInputCols(Array("r_score", "f_score", "m_score"))
      .setOutputCol("features")
      .setHandleInvalid("skip")
      .transform(scores)

    assembled
  }
}
```



### 1.7. 预测并排序



* 计算步骤
  1. RFMTrainModel 训练模型, 保存模型到 HDFS 中, 调度周期, 一个月执行一次
  2. RFMPredictModel 预测模型, 从 HDFS 中读取聚类模型, 对整个数据集进行预测, 每天一次



* 注意点: 使用聚类生成的 predict 列是无序的, 但是其大小顺序和质心的数组顺序是相同的, 可以对质心排序, 从而将序号替换 predict 组号



```scala
object RFMPredictModel extends BasicModel {

  def main(args: Array[String]): Unit = {
    startFlow()
  }

  override def tagName(): String = "客户价值"

  override def process(source: DataFrame, fiveTags: Array[Tag], outFields: Array[String]): DataFrame = {
    import spark.implicits._
    import org.apache.spark.sql.functions._

    val assembled = RFMModel.assembleDataFrame(source)

    val kmeans = KMeansModel.load(RFMModel.MODEL_PATH)
    val predicted = kmeans.transform(assembled)

//    predicted.groupBy('predict)
//      .agg(max('r_score + 'f_score + 'm_score) as "max_score", min('r_score + 'f_score + 'm_score) as "min_score")
//      .sort('predict)
//      .show()

    // 找到 kmeans 生成的组号和 rule 之间的关系
    val sortedCenters = kmeans.clusterCenters.indices
      .map(i => (i, kmeans.clusterCenters(i).toArray.sum))
      .sortBy(c => c._2).reverse

    val centerIndex = sortedCenters.indices.map(i => (i + 1, sortedCenters(i)._1)).toDF("index", "center")

//    centerIndex.foreach(println(_))

    // 将组号替换为序号
    predicted.join(centerIndex, predicted.col("predict") === centerIndex.col(("center")))
      .select(predicted.col("id"), centerIndex.col("index"))
      .show()
    null
  }
}

```



### 1.8. RFE 活跃度

类似 RFM, 我们使用 RFE 计算用户的活跃度



```
RFE = R (最近一次访问时间) + F (特定时间内访问频率) + E (活动数量)

R = datediff(date_sub(current_timestamp(),60), max('log_time))
F = count('loc_url)
E = countDistinct('loc_url)

R:0-15天=5分，16-30天=4分，31-45天=3分，46-60天=2分，大于61天=1分
F:≥400=5分，300-399=4分，200-299=3分，100-199=2分，≤99=1分
E:≥250=5分，230-249=4分，210-229=3分，200-209=2分，1=1分
```



## 2. PSM 价格敏感度模型



* PSM 用于统计用户的价格敏感度
* 对于不同级别价格敏感的用户可以实行不同程度的营销



### 2.1. 计算 PSM



```
PSM Score = 优惠订单占比 + (平均优惠金额 / 平均每单应收) + 优惠金额占比
```



* 优惠订单占比
  * 优惠订单 / 总单数
  * 优惠订单 = 优惠的订单数量 / 总单数
  * 未优惠订单 = 未优惠的订单数量 / 总单数
* 平均优惠金额
  * 总优惠金额 / 优惠单数
* 平均每单应收
  * 总应收 / 总单数
* 优惠金额占比
  * 总优惠金额 / 总应收金额



直接需要计算的字段

* 读取 tbl_orders
* 不需要 Group 直接可以求得
  * 订单的优惠状态, 0 -> 无优惠, 1 -> 有优惠
  * 订单的优惠金额
  * 应收金额 = 优惠金额 + 订单金额
* 需要按照用户 Group 才能求得
  * 优惠订单数量
  * 未优惠订单数量
  * 总单数
* 需要 Group 总优惠金额
* 需要 Group 总应收金额
* 需要使用 Group 后的字段, 优惠订单占比
* 需要使用 Group 后的字段, 平均优惠金额
* 需要使用 Group 后的字段, 平均每单应收
* 需要使用 Group 后的字段, 优惠金额占比



整体处理步骤

1. 计算每一笔订单的字段
2. 按照用户分组, 求得需要分组才能求得的字段
3. 根据分组求得的字段, 再次处理



```scala
import org.apache.spark.sql.functions._
    import spark.implicits._

    // 应收金额
    val receivableAmount = ('couponcodevalue + 'orderamount).cast(DoubleType) as "receivableAmount"
    // 优惠金额
    val discountAmount = 'couponcodevalue.cast(DoubleType) as "discountAmount"
    // 实收金额
    val practicalAmount = 'orderamount.cast(DoubleType) as "practicalAmount"

    // 是否优惠
    val state = when(discountAmount =!= 0.0d, 1)
      .when(discountAmount === 0.0d, 0)
      .as("state")

    // 优惠订单数
    val discountCount = sum('state) as "discountCount"
    // 订单总数
    val totalCount = count('state) as "totalCount"
    // 优惠总额
    val totalDiscountAmount = sum('discountAmount) as "totalDiscountAmount"
    // 应收总额
    val totalReceivableAmount = sum('receivableAmount) as "totalReceivableAmount"

    // 平均优惠金额
    val avgDiscountAmount = ('totalDiscountAmount / 'discountCount) as "avgDiscountAmount"
    // 平均每单应收
    val avgReceivableAmount = ('totalReceivableAmount / 'totalCount) as "avgReceivableAmount"
    // 优惠订单占比
    val discountPercent = ('discountCount / 'totalCount) as "discountPercent"
    // 平均优惠金额占比
    val avgDiscountPercent = (avgDiscountAmount / avgReceivableAmount) as "avgDiscountPercent"
    // 优惠金额占比
    val discountAmountPercent = ('totalDiscountAmount / 'totalReceivableAmount) as "discountAmountPercent"

    // 优惠订单占比 + (平均优惠金额 / 平均每单应收) + 优惠金额占比
    val psmScore = (discountPercent + (avgDiscountPercent / avgReceivableAmount) + discountAmountPercent) as "psm"

    val psmDF = source.select('memberid as "id", receivableAmount, discountAmount, practicalAmount, state)
      .groupBy('id)
      .agg(discountCount, totalCount, totalDiscountAmount, totalReceivableAmount)
      .select('id, psmScore)
      .cache()
```



### 2.2. 训练模型和肘部法则



* 如何确定 K 的值?
  * 通过肘部法则, KMeans
* 肘部法则
  * 根据损失函数, 计算每一个 K 的情况下, 总体上的损失
  * 绘制图形, 找到拐点, 就是合适的 K



```scala
5 -> 0.3204976425868482
6 -> 0.27951512313840254
2 -> 1.270047626457095
7 -> 0.1722912442800976
3 -> 0.8305664904430397
8 -> 0.12552074282727718
4 -> 0.4322790150969663

// https://online.visual-paradigm.com/
```



![image-20200303011446793](assets/image-20200303011446793.png)



```scala
val vectored = new VectorAssembler()
  .setInputCols(Array("psm"))
  .setOutputCol("features")
  .setHandleInvalid("skip")
  .transform(psmDF)

val kArray = Array(2, 3, 4, 5, 6, 7, 8)

val wssseMap = kArray.map(k => {
  val kmeans = new KMeans()
    .setK(k)
    .setMaxIter(10)
    .setPredictionCol("prediction")
    .setFeaturesCol("features")

  val model: KMeansModel = kmeans.fit(vectored)
  val wssse: Double = model.computeCost(vectored)
  (k, wssse)
}).toMap

println(wssseMap)
```



## 3. 购物性别



* 购物性别模型的意义有两种
  1. 通过用户购物的行为, 预测用户性别
  2. 通过用户购物的行为, 判定用户的购物性别偏好



### 3.1. 决策树

* 决策树介绍

  <img src="assets/image-20200303011818258.png" alt="image-20200303011818258" style="zoom: 80%;" />



* 决策树的构建过程

  ![image-20200303012347303](assets/image-20200303012347303.png)



### 3.2. 增加多源基类



* 购物性别的计算需要两个表的数据
  * 商品表
  * 订单表
  * 用户 -> 商品
* 所以需要另外一个基类, 有可以读取多个源的能力
  1. 创建 `MultiSourceModel`
  2. 修改 `readMetaData` 方法, 返回多个 MetaData
  3. 修改 `createSource`, 传入多个 MetaData
  4. 修改 `process` 方法, 传入多个 source



```scala
trait MultiSourceModel {

  val spark: SparkSession = SparkSession.builder()
    .appName("job model")
    .master("local[10]")
    .getOrCreate()

  val config: Config = ConfigFactory.load()

  def startFlow(): Unit = {
    // 1. 访问 MySQL, tbl_basic_tag, 获取四级标签和五级标签的数据
    val (fourTag, fiveTag) = readBasicTag(tagName())

    // 2. 根据四级标签的 ID, 去 tbl_metadata 中查询对应的元数据
    val metaData = readMetaData(fourTag.id)

    // 3. 读取对应的数据库, 拿到源表数据
    val sourceArr = createSource(metaData)

    if (sourceArr.isEmpty) {
      return
    }

    // 4. 计算标签
    val result = process(sourceArr.map(s => s._1), fiveTag, sourceArr.map(s => s._2.outFields))

    // 5. 写入 HBase
    if (result != null) {
      result.show()
      saveUserProfile(result, sourceArr.head._2.outFields)
    }
  }

  def tagName(): String

  def process(source: Array[DataFrame], fiveTags: Array[Tag], outFields: Array[Array[String]]): DataFrame

  /**
   * 通过名称, 取四级标签
   * 通过四级标签, 取五级标签
   * 最终返回四级标签和五级标签
   */
  def readBasicTag(tagName: String): (Tag, Array[Tag]) = {
    import spark.implicits._

    val url = config.getString("jdbc.basic_tag.url")
    val table = config.getString("jdbc.basic_tag.table")

    val basicTagDF = spark.read.jdbc(url, table, new Properties())

    // 筛选四级标签
    val fourTag = basicTagDF.where('name === tagName)
      .as[Tag]
      .collect()
      .head

    // 筛选五级标签
    val fiveTag = basicTagDF.where('pid === fourTag.id)
      .as[Tag]
      .collect()

    (fourTag, fiveTag)
  }

  /**
   * 通过四级标签的 ID, 去元信息表中查询对应的元信息
   */
  def readMetaData(fourTagID: String): Array[MetaData] = {
    // 1. 导入 Spark 相关的隐式转换
    import org.apache.spark.sql.functions._
    import spark.implicits._

    // 2. 读取相关配置
    val url = config.getString("jdbc.meta_data.url")
    val table = config.getString("jdbc.meta_data.table")
    val matchColumn = config.getString("jdbc.meta_data.match_column")

    // 3. 读取并筛选数据
    spark.read
      .jdbc(url, table, new Properties())
      .where(col(matchColumn) === fourTagID)
      .as[MetaData]
      .collect()
  }

  /**
   * 根据元数据, 读取源表所在的数据库, 返回DataFrame数据源
   */
  def createSource(metaData: Array[MetaData]): Array[(DataFrame, CommonMeta)] = {

    def createSource(metaData: MetaData): (DataFrame, CommonMeta) = {
      if (metaData.isHBase) {
        val hbaseMeta = metaData.toHBaseMeta
        val source = ShcUtils.read(hbaseMeta.tableName, hbaseMeta.commonMeta.inFields, hbaseMeta.columnFamily, spark)

        return (source, hbaseMeta.commonMeta)
      }

      if (metaData.isRDBMS) {

      }

      if (metaData.isHDFS) {

      }

      (null, null)
    }

    metaData.map(createSource)
  }

  /**
   * 将结果集写入 HBase 中
   */
  def saveUserProfile(result: DataFrame, outFields: Array[String]): Unit = {
    ShcUtils.writeToHBase(outFields, result, "5")
  }
}
```



### 3.3. 预置标签



* 决策树是一个监督学习算法, 需要先对数据集人工打上标签, 此处简化整体流程, 通过简单的匹配, 先预置所需要的标签



```scala
    import org.apache.spark.sql.functions._
    import spark.implicits._

    val goodsSource = source(0)
    val orderSource = source(1)

    val label = when('ogcolor.equalTo("樱花粉")
        .or('ogcolor.equalTo("白色"))
        .or('ogcolor.equalTo("香槟色"))
        .or('ogcolor.equalTo("香槟金"))
        .or('producttype.equalTo("料理机"))
        .or('producttype.equalTo("挂烫机"))
        .or('producttype.equalTo("吸尘器/除螨仪")), 1)
      .otherwise(0)
      .alias("gender")

    val stage1 = goodsSource.select('cordersn as "ordersn", 'ogcolor as "color", 'producttype, label)
      .join(orderSource, "ordersn")
      .select('memberid as "id", 'color, 'producttype, 'gender)
```



### 3.4. 训练模型



```scala
val colorIndexer = new StringIndexer()
      .setInputCol("color")
      .setOutputCol("color_index")

    val productTypeIndexer = new StringIndexer()
      .setInputCol("producttype")
      .setOutputCol("producttype_index")

    val featureAssembler = new VectorAssembler()
      .setInputCols(Array("color_index", "producttype_index"))
      .setOutputCol("features")

    val featureVectorIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("features_index")
      .setMaxCategories(3)

    val decisionTreeClassifier = new DecisionTreeClassifier()
      .setFeaturesCol("features_index")
      .setLabelCol("gender")
      .setPredictionCol("predict")
      .setMaxDepth(5)
      .setImpurity("gini")

    val pipeline = new Pipeline()
      .setStages(Array(colorIndexer, productTypeIndexer, featureAssembler, featureVectorIndexer, decisionTreeClassifier))

    val Array(trainData, testData) = stage1.randomSplit(Array(0.8, 0.2))

    val model = pipeline.fit(trainData)
    val pTrain = model.transform(trainData)
    val tTrain = model.transform(testData)
```



### 3.5. 测试模型准确率



```
    val accEvaluator = new MulticlassClassificationEvaluator()
      .setPredictionCol("predict")
      .setLabelCol("label")
      .setMetricName("accuracy")//精准度

    val trainAcc: Double = accEvaluator.evaluate(pTrain)
    val testAcc: Double = accEvaluator.evaluate(tTrain)

    println(trainAcc, testAcc)
```

