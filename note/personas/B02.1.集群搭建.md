# 集群搭建

* 目标
  * 能够通过自动化脚本部署一个集群
* 步骤
  1. 为企业设计一个规模合适的集群
  2. 企业中部署和管理集群的工具
  3. 自动创建虚拟机
  4. 自动化部署服务



## 1. 设计一个规模合适的集群

* 目标
  * 给定需求和数据规模, 能够设计一个合适的集群
* 步骤
  1. 资源预估
  2. 选择服务器
  3. 为服务器选择服务(角色)



### 1.1. 资源预估



**明确需求**

| 需求点               | 量    |
| -------------------- | ----- |
| 标签数量             | 150个 |
| 标签计算任务数量     | 150个 |
| 数据抽取相关任务数量 | 10个  |
| 最少支持并发任务数量 | 5个   |
| 日数据增量           | 260G  |



**如果一个Spark任务需要计算260G的数据, 需要260G的内存吗?** 

1. 给出一段 Spark 代码

```scala
rdd1 = sc.readTextFile(...)
rdd2 = rdd1.map(...)
rdd3 = rdd2.flatMap(...)
```

2. 分析执行策略, 简化的逻辑如下

```shell
rdd3 = rdd2.flatMap(...)
rdd3 = rdd1.map(...).flatMap(...)
rdd3 = sc.readTestFile(...).map(...).flatMap(...)
```

3. 按照这个逻辑, 没有必要把所有的数据都加载出来, 再逐个数据集去计算

<img src="assets/image-20200219094535946.png" alt="image-20200219094535946" style="zoom:50%;" />



得出结论, 如果计算 260G 的数据, 可能和计算 60G 的数据, 所需要的内存一样, Spark 会逐个取数据, 逐个计算, 计算完成后抛弃, 再取下一条



**真的是这样吗? 再看一段代码**

1. 给出一段Spark代码, 这段代码多了一个Shuffle算子

```scala
rdd1 = sc.readTextFile()
rdd2 = rdd1.map(...)
rdd3 = rdd2.flatMap(...)
rdd4 = rdd3.reduceByKey(...)
```

2. 分析执行过程

```scala
rdd4 = sc.readTestFile(...).map(...).flatMap(...).reduceByKey(...)
```

3. flatMap 出去的数据可能要汇总一下, 才能流入 reduceByKey

<img src="assets/image-20200219095335338.png" alt="image-20200219095335338" style="zoom:50%;" />



得出结论, 如果计算 260G 的数据, 和计算 60G 的数据, 所需要的内存确实不一样, 有 Shuffle 的情况下要稍微多一些才行



**那么, 如何设计集群规模?**

1. Spark 这样启动

```shell
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 16G \
  --executor-cores 4 \
  --executor-num 10 \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
```

2. `executor-memory`, `executor-cores`, `executor-num` 设置的原则
   1. `executor-memory` 是 `executor-cores` 的 2-4 倍, 取决于 Spark 任务是否有很多计算
   2. 通过 `executor-num` 来控制整个 Job 的内存占用



**所以, 可以得到如下表格**

1. 认定需要160G内存用来读取260G数据
2. CPU和内存的比例大概是1-2和1-4的比例
3. Executor cores一般不会设置很大，2-10个基本够用了

| 点                | 量                                                       |
| ----------------- | -------------------------------------------------------- |
| Executor cores    | 4 cores                                                  |
| Executor num      | 10                                                       |
| Executor memory   | 16 G                                                     |
| Spark parallelism | 128                                                      |
| Driver memory     | 6 G                                                      |
| Total cores       | exenum * execores = 40                                   |
| Total memory      | exenum * exemem = 160                                    |
| Pre job needs     | 40 core + 160 G                                          |
| Cluster           | (40, 160) * 5 * 1.2 = (240,1080) = 240 核心, 1080 G 内存 |



### 1.2. 选择服务器



假设我们公司很有钱, 选择在京东上买新的 Dell 服务器, 选择了一个比较好的机器如下



![20200216173643](assets/20200216173643.png)



配置如下



| 类型 | 型号                 | 容量     | 数量              |
| :--- | :------------------- | :------- | :---------------- |
| CPU  | Intel 至强 E5-2690V4 | 14 cores | 2颗               |
| 内存 | Dell ECC DDR4        | 32 G     | 4条(可扩展至24条) |
| 硬盘 | Dell SAS 3.5英寸     | 4 TB     | 3块(可扩展至8块)  |



所以, 单台服务器可以有 128G 内存, 28 cores, 那我们需要的集群数量如下



| 类型   | 数量           |
| :----- | :------------- |
| Master | 3              |
| Worker | 10             |
| Edge   | 2 (可 1U 低配) |



这样的话, 我们集群的总资源量就如下, 可以看到, 已经非常够用了



| 类型 | 大小      |
| :--- | :-------- |
| CPU  | 280 cores |
| 内存 | 1280 G    |
| 硬盘 | 120 T     |



### 1.3. 分配集群角色



按照如下方式分配是比较推荐的, 而且一般生产级别的大数据集群, 一定是要 HA 的



|                        | Master 1-2 | Master 3 | Work 1-10 | Gateway | Utility 1 |
| :--------------------- | ---------- | -------- | --------- | ------- | --------- |
| **NameNode**           | ✔️          |          |           |         |           |
| **JournalNode**        | ✔️          | ✔️        |           |         |           |
| **FailoverController** | ✔️          |          |           |         |           |
| **ResourceManager**    | ✔️          |          |           |         |           |
| **HMaster**            | ✔️          | ✔️        |           |         |           |
| **Zookeeper**          | ✔️          | ✔️        |           |         |           |
| **JobHistory**         |            | ✔️        |           |         |           |
| **SparkHistory**       |            | ✔️        |           |         |           |
| **DataNode**           |            |          | ✔️         |         |           |
| **NodeManager**        |            |          | ✔️         |         |           |
| **HRegionsServer**     |            |          | ✔️         |         |           |
| **Hue**                |            |          |           | ✔️       |           |
| **Oozie**              |            |          |           | ✔️       |           |
| **HiveServer2**        |            |          |           | ✔️       |           |
| **Flume**              |            |          |           | ✔️       |           |
| **用户画像系统**       |            |          |           | ✔️       |           |
| **ClouderaManager**    |            |          |           |         | ✔️         |
| **ManagementService**  |            |          |           |         | ✔️         |
| **HiveMetastore**      |            |          |           |         | ✔️         |
