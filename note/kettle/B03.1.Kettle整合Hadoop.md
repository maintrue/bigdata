
###  Kettle整合Hadoop

#### Hadoop环境准备

1、查看hadoop的文件系统

- 通过浏览器访问

```html
http://node1:50070/
```

- 通过终端访问

```shell
hadoop fs -ls / # 查看文件
```

2、在hadoop文件系统中创建/hadoop/test目录

```shell
hadoop fs -mkdir -p /hadoop/test  
```

3、在本地创建1.txt

- vim 1.txt

```html
id,name
1,itheima
2,itcast
```

4、上传1.txt到hadoop文件系统的/hadoop/test目录

```shell
hadoop fs -put 1.txt /hadoop/test
```

#### kettle与hahoop环境整合

1、确保Hadoop的环境变量设置好HADOOP_USER_NAME为root

2、从hadoop下载核心配置文件

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/hdfs-site.xml
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/core-site.xml
```

文件会被下载到windows的下载目录

sz命令设置下载到windows的目录：

| linux下载文件到windows                                       |
| ------------------------------------------------------------ |
| ![image-20200205162500695](assets/image-20200205162500695.png) |
| ![image-20200205162639526](assets/image-20200205162639526.png) |



3、把hadoop核心配置文件(hdfs-site.xml和core-site.xml)放入kettle目录

```shell
data-integration\plugins\pentaho-big-data-plugin\hadoop-configurations\cdh514
```

4、修改 `data-integration\plugins\pentaho-big-data-plugin\plugin.properties`文件

- 修改plugin.properties

```shell
active.hadoop.configuration=cdh514
```

| plugin.propeties                                             |
| ------------------------------------------------------------ |
| ![image-20200205162801320](assets/image-20200205162801320.png) |

5、 创建Hadoop clusters

具体步骤如下：

| 创建hadoop cluster                                           |
| ------------------------------------------------------------ |
| ![image-20200205162933991](assets/image-20200205162933991.png) |
| ![image-20200205163018669](assets/image-20200205163018669.png) |
| ![image-20200205163201486](assets/image-20200205163201486.png) |



点击测试结果如图片右侧展示效果说明整合hadoop环境没有问题！！点击确定保存以上操作即可！！

查看链接是否保存：

| hadoop连接保存                                               |
| ------------------------------------------------------------ |
| ![image-20200205163327492](assets/image-20200205163327492.png) |

如果与hadoop链接没有保存后续是无法操作hadoop集群！！

####  Hadoop file input组件

Kettle在Big data分类中提供了一个Hadoop file input 组件用来从hdfs文件系统中读取数据。

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205163533264](assets/image-20200205163533264.png) |





需求：

- 从Hadoop文件系统读取/hadoop/test/1.txt文件，把数据输入到Excel中。



实习步骤：

1、拖入以下组件

<img src="assets/clip_image017.png" align="left" style="border:1px solid #999"/>

2、配置Hadoop File Input组件

指定hdfs的目标路径：

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205164223439](assets/image-20200205164223439.png) |



指定文件内容格式：

| 指定文件内容格式                                             |
| ------------------------------------------------------------ |
| ![image-20200109145528606](assets/image-20200109145528606.png) |



点击字段查看获取字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164439030](assets/image-20200205164439030.png) |



配置excel输出组件：

| excel输出组件配置                                            |
| ------------------------------------------------------------ |
| ![image-20200205164529691](assets/image-20200205164529691.png) |



点击excel输出组件获取字段查看字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164624756](assets/image-20200205164624756.png) |



启动转换任务：

| 执行任务                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164718324](assets/image-20200205164718324.png) |
| ![image-20200205164746715](assets/image-20200205164746715.png) |
| 执行结果                                                     |
| ![image-20200205164818973](assets/image-20200205164818973.png) |



#### Hadoop file output组件

Kettle在Big data分类中提供了一个Hadoop file output 组件用来向hdfs文件系统中保存数据

| hadoop file output组件                                       |
| ------------------------------------------------------------ |
| ![image-20200205163533264](assets/image-20200205163533264-1580983912463.png) |



需求：

- 读取 user.json 把数据写入到hdfs文件系统的的/hadoop/test/2.txt中。

实现步骤：

1、拖入以下组件

| 组件配置                                                     |
| ------------------------------------------------------------ |
| ![image-20200205165008109](assets/image-20200205165008109.png) |

2、配置 JSON 输入组件

| 指定json文件的路径                                           |
| ------------------------------------------------------------ |
| ![image-20200205165120462](assets/image-20200205165120462.png) |
| 配置json input组件读取的字段                                 |
| ![image-20200205165251370](assets/image-20200205165251370.png) |







3、配置Hadoop file output组件

| 指定hdfs目标路径                                             |
| ------------------------------------------------------------ |
| ![image-20200205165351594](assets/image-20200205165351594.png) |
| 指定源文件的属性信息：                                       |
| ![image-20200205165542753](assets/image-20200205165542753.png) |
| ![image-20200205165629082](assets/image-20200205165629082.png) |



- 问题

![image-20200205165908948](../../../../1上课共享资料/day01/1.讲义/assets/image-20200205165908948.png)

错误：用户没有权限

解决：

```shell
# 修改权限
  hadoop fs -chmod -R 777  /
```
