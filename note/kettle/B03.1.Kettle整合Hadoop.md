
###  Kettle整合Hadoop

#### Hadoop环境准备

1、查看hadoop的文件系统

- 通过浏览器访问

```html
http://node1:50070/
```

- 通过终端访问

```shell
hadoop fs -ls / # 查看文件
```

2、在hadoop文件系统中创建/hadoop/test目录

```shell
hadoop fs -mkdir -p /hadoop/test  
```

3、在本地创建1.txt

- vim 1.txt

```html
id,name
1,itheima
2,itcast
```

4、上传1.txt到hadoop文件系统的/hadoop/test目录

```shell
hadoop fs -put 1.txt /hadoop/test
```

#### kettle与hahoop环境整合

1、确保Hadoop的环境变量设置好HADOOP_USER_NAME为root

2、从hadoop下载核心配置文件

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/hdfs-site.xml
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/core-site.xml
```

文件会被下载到windows的下载目录

sz命令设置下载到windows的目录：

| linux下载文件到windows                                       |
| ------------------------------------------------------------ |
| ![image-20200205162500695](https://user-images.githubusercontent.com/75486726/180307122-c7b6e495-b3c5-4378-bded-1e6f83d91634.png) |

3、把hadoop核心配置文件(hdfs-site.xml和core-site.xml)放入kettle目录

```shell
data-integration\plugins\pentaho-big-data-plugin\hadoop-configurations\cdh514
```

4、修改 `data-integration\plugins\pentaho-big-data-plugin\plugin.properties`文件

- 修改plugin.properties

```shell
active.hadoop.configuration=cdh514
```

| plugin.propeties                                             |
| ------------------------------------------------------------ |
| ![image-20200205162801320](https://user-images.githubusercontent.com/75486726/180307208-8dc54f93-e173-4c10-b433-0b4cf0fb6c1c.png) |

5、 创建Hadoop clusters

具体步骤如下：

| 创建hadoop cluster                                           |
| ------------------------------------------------------------ |
| ![image-20200205162933991](https://user-images.githubusercontent.com/75486726/180307246-b59f6884-4860-4726-9692-5182e3412fe0.png) |
| ![image-20200205163018669](https://user-images.githubusercontent.com/75486726/180307273-134657fd-7d81-4ea9-8b4a-12d0d7b34a45.png) |
| ![image-20200205163201486](https://user-images.githubusercontent.com/75486726/180307301-d4897bda-bd18-4ee3-9ce9-740a3b2fe0b4.png) |



点击测试结果如图片右侧展示效果说明整合hadoop环境没有问题！！点击确定保存以上操作即可！！

查看链接是否保存：

| hadoop连接保存                                               |
| ------------------------------------------------------------ |
| ![image-20200205163327492](https://user-images.githubusercontent.com/75486726/180307326-0f94780a-f522-4e7a-8cae-027f0f9621a1.png) |

如果与hadoop链接没有保存后续是无法操作hadoop集群！！

####  Hadoop file input组件

Kettle在Big data分类中提供了一个Hadoop file input 组件用来从hdfs文件系统中读取数据。

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205163533264](https://user-images.githubusercontent.com/75486726/180307373-d2152455-b980-4000-83cc-ad9726356c45.png) |





需求：

- 从Hadoop文件系统读取/hadoop/test/1.txt文件，把数据输入到Excel中。



实习步骤：

1、拖入以下组件

![clip_image017](https://user-images.githubusercontent.com/75486726/180307412-43f47c0a-7bdf-4106-bad7-963337122a74.png)

2、配置Hadoop File Input组件

指定hdfs的目标路径：

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205164223439](https://user-images.githubusercontent.com/75486726/180307445-1c6a2b62-63f2-4275-90cd-13747300d811.png) |



指定文件内容格式：

| 指定文件内容格式                                             |
| ------------------------------------------------------------ |
| ![image-20200109145528606](https://user-images.githubusercontent.com/75486726/180307496-46163e73-ea3d-4d12-a703-9eb61155d963.png) |



点击字段查看获取字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164439030](https://user-images.githubusercontent.com/75486726/180307524-e0e81d7f-7b44-45b9-b4c5-bec907b335ab.png) |



配置excel输出组件：

| excel输出组件配置                                            |
| ------------------------------------------------------------ |
| ![image-20200205164529691](https://user-images.githubusercontent.com/75486726/180307550-456ced2f-21d9-43b1-8dc9-7a89d5f99e06.png) |



点击excel输出组件获取字段查看字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164624756](https://user-images.githubusercontent.com/75486726/180307574-05786532-bc86-4fc1-bc65-f9fe5d73039c.png) |



启动转换任务：

| 执行任务                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164718324](https://user-images.githubusercontent.com/75486726/180307600-eef077e6-13ca-414c-83c2-4636cf169355.png) |
| ![image-20200205164746715](https://user-images.githubusercontent.com/75486726/180307623-4b3f7b0c-05d7-45de-9920-843ff7276606.png) |
| 执行结果                                                     |
| ![image-20200205164818973](https://user-images.githubusercontent.com/75486726/180307655-47018b0e-5ccd-48e5-b487-3f4546f9be84.png) |



#### Hadoop file output组件

Kettle在Big data分类中提供了一个Hadoop file output 组件用来向hdfs文件系统中保存数据

| hadoop file output组件                                       |
| ------------------------------------------------------------ |
| ![image-20200205163533264-1580983912463](https://user-images.githubusercontent.com/75486726/180307690-561b13e6-bedb-4500-9ed3-2e52c6d057e2.png) |



需求：

- 读取 user.json 把数据写入到hdfs文件系统的的/hadoop/test/2.txt中。

实现步骤：

1、拖入以下组件

| 组件配置                                                     |
| ------------------------------------------------------------ |
| ![image-20200205165008109](https://user-images.githubusercontent.com/75486726/180307731-32cf68ac-306f-406b-9bcd-56767cb15252.png) |

2、配置 JSON 输入组件

| 指定json文件的路径                                           |
| ------------------------------------------------------------ |
| ![image-20200205165120462](https://user-images.githubusercontent.com/75486726/180307756-d37fe885-9083-4750-bc53-48e3e52eaf4a.png) |
| 配置json input组件读取的字段                                 |
| ![image-20200205165251370](https://user-images.githubusercontent.com/75486726/180307782-46f258b2-5c16-4dff-a609-8b714a663817.png) |







3、配置Hadoop file output组件

| 指定hdfs目标路径                                             |
| ------------------------------------------------------------ |
| ![image-20200205165351594](https://user-images.githubusercontent.com/75486726/180307812-1fd8e954-6095-48c6-a528-6ad37768ade0.png) |
| 指定源文件的属性信息：                                       |
| ![image-20200205165542753](https://user-images.githubusercontent.com/75486726/180307842-e912bf49-f06b-4f6d-8b8f-c5f547e661bf.png) |
| ![image-20200205165629082](https://user-images.githubusercontent.com/75486726/180307876-0b880d69-8f68-419b-b9a4-ce20c190e455.png) |



- 问题

错误：用户没有权限

解决：

```shell
# 修改权限
  hadoop fs -chmod -R 777  /
```
