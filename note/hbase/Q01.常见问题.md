# 1 常见问题

## 1.1 Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster
1.找到$HADOOP_HOME/etc/mapred-site.xml,增加以下配置
``` 
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
```
2.将配置文件分发到各个节点

3.重新启动YARN集群

## 1.2 Caused by: java.net.ConnectException: Call to node2.itcast.cn/192.168.88.101:16020 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.ConnectTimeoutException: connection timed out: node2.itcast.cn/192.168.88.101:16020
无法连接到HBase，请检查HBase的Master是否正常启动

## 1.3 Starting namenodes on [localhost] ERROR: Attempting to launch hdfs namenode as root ，ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting launch.
解决办法：
是因为缺少用户定义造成的，所以分别编辑开始和关闭脚本
``` 
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
```

在顶部空白处添加内容：
``` 
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```


## 1.4 Starting resourcemanager ERROR: Attempting to launch yarn resourcemanager as root ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting launch. Starting nodemanagers ERROR: Attempting to launch yarn nodemanager as root ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting launch.
``` 
vim sbin/start-yarn.sh
vim sbin/stop-yarn.sh

YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```


## 1.5 Exception in thread "main" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat
解决方案：
``` 
将 hadoop.dll 放到c:/windows/system32文件夹中，重启IDEA，重新运行程序
```


## 1.6 Regions In Transition
错误信息如下：
``` 
2020-05-09 12:14:22,760 WARN  [RS_OPEN_REGION-regionserver/node1:16020-2] handler.AssignRegionHandler: Failed to open region TestTable,00000000000000000006900000,1588444012555.8a72d1ccdadd3b14284a24ec01918023., will report to master
java.io.IOException: Missing table descriptor for TestTable,00000000000000000006900000,1588444012555.8a72d1ccdadd3b14284a24ec01918023.
        at org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler.process(AssignRegionHandler.java:129)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

问题解析
在执行Region Split时，因为系统中断或者HDFS中的Region文件已经被删除。

Region的状态由master跟踪，包括以下状态：

| State  | Description
| --- | ---
| Offline | Region is offline
| Pending Open   | A request to open the region was sent to the server
| Opening | The server has started opening the region
| Open    | The region is open and is fully operational
| Pending Close | A request to close the region has been sent to the server
| Closing | The server has started closing the region
| Closed  | The region is closed
| Splitting   | The server started splitting the region
| Split   | The region has been split by the server

Region在这些状态之间的迁移（transition）可以由master引发，也可以由region server引发。

解决方案
- 使用 hbase hbck 找到哪些Region出现Error
- 使用以下命令将失效的Region删除
``` 
deleteall "hbase:meta","TestTable,00000000000000000005850000,1588444012555.89e1c07384a56c77761e490ae3f34a8d."
```
- 重启hbase即可

## 1.7 Phoenix: Table is read only
``` 
Error: ERROR 505 (42000): Table is read only. (state=42000,code=505)
org.apache.phoenix.schema.ReadOnlyTableException: ERROR 505 (42000): Table is read only.
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1126)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1501)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:2721)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1114)
        at org.apache.phoenix.compile.CreateTableCompiler$1.execute(CreateTableCompiler.java:192)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:408)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:391)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:390)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:378)
        at org.apache.phoenix.jdbc.PhoenixStatement.execute(PhoenixStatement.java:1825)
        at sqlline.Commands.execute(Commands.java:822)
        at sqlline.Commands.sql(Commands.java:732)
        at sqlline.SqlLine.dispatch(SqlLine.java:813)
        at sqlline.SqlLine.begin(SqlLine.java:686)
        at sqlline.SqlLine.start(SqlLine.java:398)
        at sqlline.SqlLine.main(SqlLine.java:291)
```

phoenix连接hbase数据库，创建二级索引报错：Error: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions: Tue Mar 06 10:32:02 CST 2018, null, java.net.SocketTimeoutException: callTimeout

1.修改phoenix的hbase-site.xml配置文件为
``` 
<property>
    <name>phoenix.query.timeoutMs</name>
    <value>1800000</value>
</property>    
<property>
    <name>hbase.regionserver.lease.period</name>
    <value>1200000</value>
</property>
<property>
    <name>hbase.rpc.timeout</name>
    <value>1200000</value>
</property>
<property>
    <name>hbase.client.scanner.caching</name>
    <value>1000</value>
</property>
<property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>1200000</value>
</property>
```

2.设置完以上内容后，重新通过sqlline.py连接hbase