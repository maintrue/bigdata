# 1 Standalone-HA高可用集群模式

![image](https://user-images.githubusercontent.com/75486726/177995469-ea2ad367-dff1-46d5-855f-2a4bde97d18c.png)

从之前的架构中我们可以很明显的发现 JobManager 有明显的单点问题(SPOF，single point of failure)。JobManager 肩负着任务调度以及资源分配，一旦 JobManager 出现意外，其后果可想而知。

在 Zookeeper 的帮助下，一个 Standalone的Flink集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后(如宕机或 Crash)，Zookeeper 会从 Standby 中选一个新的 JobManager 来接管 Flink 集群。

集群规划：
- 服务器: node1(Master + Slave): JobManager + TaskManager
- 服务器: node2(Master + Slave): JobManager + TaskManager
- 服务器: node3(Slave): TaskManager

## 1.1 环境搭建
1.启动ZooKeeper
``` 
zkServer.sh status
zkServer.sh stop
zkServer.sh start
```

2.启动HDFS
``` 
/export/serves/hadoop/sbin/start-dfs.sh
```

3.停止Flink集群
``` 
/export/server/flink/bin/stop-cluster.sh
```


4.修改flink-conf.yaml
``` 
vi /export/server/flink/conf/flink-conf.yaml

state.backend: filesystem
state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints
high-availability: zookeeper
high-availability.storageDir: hdfs://node1:8020/flink/ha/
high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181
```
配置解释
``` 
#开启HA，使用文件系统作为快照存储
state.backend: filesystem

#启用检查点，可以将快照保存到HDFS
state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints

#使用zookeeper搭建高可用
high-availability: zookeeper

# 存储JobManager的元数据到HDFS
high-availability.storageDir: hdfs://node1:8020/flink/ha/

# 配置ZK集群地址
high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181
```

6.修改masters
``` 
vi /export/server/flink/conf/masters
node1:8081
node2:8081
```


7.同步
``` 
scp -r /export/server/flink/conf/flink-conf.yaml node2:/export/server/flink/conf/
scp -r /export/server/flink/conf/flink-conf.yaml node3:/export/server/flink/conf/
scp -r /export/server/flink/conf/masters node2:/export/server/flink/conf/
scp -r /export/server/flink/conf/masters node3:/export/server/flink/conf/
```

8.修改node2上的flink-conf.yaml
``` 
vi /export/server/flink/conf/flink-conf.yaml
jobmanager.rpc.address: node2
```

9.重新启动Flink集群,node1上执行
``` 
/export/server/flink/bin/stop-cluster.sh
/export/server/flink/bin/start-cluster.sh
```

![image](https://user-images.githubusercontent.com/75486726/177995504-33a41be5-3e41-46f9-8295-42d72019e1af.png)

10.使用jps命令查看
```
jps
```

11.查看日志
``` 
cat /export/server/flink/log/flink-root-standalonesession-0-node1.log
```

发现如下错误

![image](https://user-images.githubusercontent.com/75486726/177995542-68b2bfb1-8db8-44a6-b1d1-4a79f6f8a140.png)

因为在Flink1.8版本后,Flink官方提供的安装包里没有整合HDFS的jar

12.下载jar包并在Flink的lib目录下放入该jar包并分发使Flink能够支持对Hadoop的操作
``` 
下载地址
https://flink.apache.org/downloads.html

放入lib目录
cd /export/server/flink/lib

![image](https://user-images.githubusercontent.com/75486726/177995575-39ac30f1-d19c-42c0-bcca-b31defa88b5f.png)

分发
for i in {2..3}; do scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar node$i:$PWD; done
```

13.重新启动Flink集群,node1上执行
``` 
/export/server/flink/bin/start-cluster.sh
```

14.使用jps命令查看,发现三台机器已经ok


## 1.2 测试运行
1.访问WebUI

http://node1:8081/#/job-manager/config
http://node2:8081/#/job-manager/config

2.执行wc
``` 
/export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar
```

3.kill掉其中一个master

4.重新执行wc,还是可以正常执行
``` 
/export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar

```

3.停止集群
``` 
/export/server/flink/bin/stop-cluster.sh
```
