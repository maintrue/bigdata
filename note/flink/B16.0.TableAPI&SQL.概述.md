# 1 Table API & SQL 介绍

## 1.1 为什么需要Table API & SQL

https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/

### 1.1.1 Flink的Table模块包括 Table API 和 SQL
Table API 是一种类SQL的API，通过Table API，用户可以像操作表一样操作数据，非常直观和方便

SQL作为一种声明式语言，有着标准的语法和规范，用户可以不用关心底层实现即可进行数据的处理，非常易于上手

Flink Table API 和 SQL 的实现上有80%左右的代码是公用的。作为一个流批统一的计算引擎，Flink 的 Runtime 层是统一的。

### 1.1.2 Table API & SQL的特点
Flink之所以选择将 Table API & SQL 作为未来的核心 API，是因为其具有一些非常重要的特点：

1. 声明式:属于设定式语言，用户只要表达清楚需求即可，不需要了解底层执行；
2. 高性能:可优化，内置多种查询优化器，这些查询优化器可为 SQL 翻译出最优执行计划；
3. 简单易学:易于理解，不同行业和领域的人都懂，学习成本较低；
4. 标准稳定:语义遵循SQL标准，非常稳定，在数据库 30 多年的历史中，SQL 本身变化较少；
5. 流批统一:可以做到API层面上流与批的统一，相同的SQL逻辑，既可流模式运行，也可批模式运行，Flink底层Runtime本身就是一个流与批统一的引擎

## 1.2 Table API& SQL发展历程

### 1.2.1 架构升级
自 2015 年开始，阿里巴巴开始调研开源流计算引擎，最终决定基于 Flink 打造新一代计算引擎，针对 Flink 存在的不足进行优化和改进，并且在 2019 年初将最终代码开源，也就是Blink。Blink 在原来的 Flink 基础上最显著的一个贡献就是 Flink SQL 的实现。随着版本的不断更新，API 也出现了很多不兼容的地方。

在 Flink 1.9 中，Table 模块迎来了核心架构的升级，引入了阿里巴巴Blink团队贡献的诸多功能

![image](https://user-images.githubusercontent.com/75486726/178263630-3d391449-a2ab-40ea-afd7-880a692443cd.png)

在Flink 1.9 之前，Flink API 层 一直分为DataStream API 和 DataSet API，Table API & SQL 位于 DataStream API 和 DataSet API 之上。可以看处流处理和批处理有各自独立的api (流处理DataStream，批处理DataSet)。而且有不同的执行计划解析过程，codegen过程也完全不一样，完全没有流批一体的概念，面向用户不太友好。

在Flink1.9之后新的架构中，有两个查询处理器：Flink Query Processor，也称作Old Planner和Blink Query Processor，也称作Blink Planner。为了兼容老版本Table及SQL模块，插件化实现了Planner，Flink原有的Flink Planner不变，后期版本会被移除。新增加了Blink Planner，新的代码及特性会在Blink planner模块上实现。批或者流都是通过解析为Stream Transformation来实现的，不像Flink Planner，批是基于Dataset，流是基于DataStream。

### 1.2.2 查询处理器的选择
查询处理器是 Planner 的具体实现，通过parser、optimizer、codegen(代码生成技术)等流程将 Table API & SQL作业转换成 Flink Runtime 可识别的 Transformation DAG，最终由 Flink Runtime 进行作业的调度和执行。

Flink Query Processor查询处理器针对流计算和批处理作业有不同的分支处理，流计算作业底层的 API 是 DataStream API， 批处理作业底层的 API 是 DataSet API

Blink Query Processor查询处理器则实现流批作业接口的统一，底层的 API 都是Transformation，这就意味着我们和Dataset完全没有关系了

Flink1.11之后Blink Query Processor查询处理器已经是默认的了

https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/



### 1.2.3 了解-Blink planner和Flink Planner具体区别
https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html

1. Blink将批处理作业视为流式处理的特殊情况。因此，表和数据集之间的转换也不受支持，批处理作业不会转换为娄媚集程序，而是转换为数据流程序，与流作业相同。
2. Blink planner不支持BatchTableSource。
3. old planner和Blink planne由勺Fi也rableSource实现容
4. 基于字符串的键靡遣选项仅用于Blink planner。
5. PlannerConfig在两个planners中的(CalciteConfig)是不同的。 
6. Blink planner将在TableEnvironment 和 StreamTableEnvironment上将多个汇优化为—个DAG。 旧的计划者总是将每个水槽优化为一个新的DAG,其中所有DAG彼此独立。
7. 旧的计划器现在不支持目录统计，而Blinki十划器支持。

## 1.3 注意
https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html

### 1.3.1 API稳定性

![image](https://user-images.githubusercontent.com/75486726/178263892-fe0b68ef-1a05-4476-be0b-3a996964cd7d.png)

### 1.3.2 性能对比
注意：目前FlinkSQL性能不如SparkSQL，未来FlinkSQL可能会越来越好
下图是Hive、Spark、Flink的SQL执行速度对比：

![image](https://user-images.githubusercontent.com/75486726/178263911-ee985174-01e5-4db7-b64d-5e5c921af654.png)

## 1.4 相关概念

https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/dynamic_tables.html

### 1.4.1 Dynamic Tables & Continuous Queries

在Flink中，它把针对无界流的表称之为Dynamic Table（动态表）。它是Flink Table API和SQL的核心概念。顾名思义，它表示了Table是不断变化的。
我们可以这样来理解，当我们用Flink的API，建立一个表，其实把它理解为建立一个逻辑结构，这个逻辑结构需要映射到数据上去。Flink source源源不断的流入数据，就好比每次都往表上新增一条数据。表中有了数据，我们就可以使用SQL去查询了。要注意一下，流处理中的数据是只有新增的，所以看起来数据会源源不断地添加到表中。
动态表也是一种表，既然是表，就应该能够被查询。我们来回想一下原先我们查询表的场景。
打开编译工具，编写一条SQL语句
将SQL语句放入到mysql的终端执行
查看结果
再编写一条SQL语句
再放入到终端执行
再查看结果
…..如此反复

而针对动态表，Flink的source端肯定是源源不断地会有数据流入，然后我们基于这个数据流建立了一张表，再编写SQL语句查询数据，进行处理。这个SQL语句一定是不断地执行的。而不是只执行一次。注意：针对流处理的SQL绝对不会像批式处理一样，执行一次拿到结果就完了。而是会不停地执行，不断地查询获取结果处理。所以，官方给这种查询方式取了一个名字，叫Continuous Query，中文翻译过来叫连续查询。而且每一次查询出来的数据也是不断变化的。

![image](https://user-images.githubusercontent.com/75486726/178264019-be74d199-21b9-4d1e-b4f0-366686c856ee.png)

这是一个非常简单的示意图。该示意图描述了：我们通过建立动态表和连续查询来实现在无界流中的SQL操作。大家也可以看到，在Continuous上面有一个State，表示查询出来的结果会存储在State中，再下来Flink最终还是使用流来进行处理。
所以，我们可以理解为Flink的Table API和SQL，是一个逻辑模型，通过该逻辑模型可以让我们的数据处理变得更加简单。

![image](https://user-images.githubusercontent.com/75486726/178264166-c5b955e9-81ae-4022-acb9-761557df6c40.png)

![image](https://user-images.githubusercontent.com/75486726/178264177-3448d0a5-14cf-4ea5-b5c8-f069e2aacf16.png)

### 1.4.2 Table to Stream Conversion
表中的Update和Delete
我们前面提到的表示不断地Append，表的数据是一直累加的，因为表示对接Source的，Source是不会有update的。但如果我们编写了一个SQL。这个SQL看起来是这样的：

SELECT user, sum(money) FROM order GROUP BY user;
当执行一条SQL语句之后，这条语句的结果还是一个表，因为在Flink中执行的SQL是Continuous Query，这个表的数据是不断变化的。新创建的表存在Update的情况。仔细看下下面的示例，例如：
第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000
第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500
第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500
….
大家发现了吗，现在数据结果是有Update的。张三一开始是2000，但后面变成了2300。
那还有删除的情况吗？有的。看一下下面这条SQL语句：
SELECT t1.`user`, SUM(t1.`money`) FROM t_order t1
WHERE
NOT EXISTS (SELECT T2.`user`AS TOTAL_MONEY FROM t_order t2 WHERE T2.`user` = T1.`user` GROUP BY t2.`user` HAVING SUM(T2.`money`) > 3000)
GROUP BY t1.`user`GROUP BY t1.`user`

第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000
第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500
第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500
第四条数据，张三,800，继续执行这条SQL语句，结果是，李四,1500
惊不惊喜？意不意外？
因为张三的消费的金额已经超过了3000，所以SQL执行完后，张三是被处理掉了。从数据的角度来看，它不就是被删除了吗？

通过上面的两个示例，给大家演示了，在Flink SQL中，对接Source的表都是Append-only的，不断地增加。执行一些SQL生成的表，这个表可能是要UPDATE的、也可能是要INSERT的。

对表的编码操作
我们前面说到过，表是一种逻辑结构。而Flink中的核心还是Stream。所以，Table最终还是会以Stream方式来继续处理。如果是以Stream方式处理，最终Stream中的数据有可能会写入到其他的外部系统中，例如：将Stream中的数据写入到MySQL中。
我们前面也看到了，表是有可能会UPDATE和DELETE的。那么如果是输出到MySQL中，就要执行UPDATE和DELETE语句了。而DataStream我们在学习Flink的时候就学习过了，DataStream是不能更新、删除事件的。
如果对表的操作是INSERT，这很好办，直接转换输出就好，因为DataStream数据也是不断递增的。但如果一个TABLE中的数据被UPDATE了、或者被DELETE了，如果用流来表达呢？因为流不可变的特征，我们肯定要对这种能够进行UPDATE/DELETE的TABLE做特殊操作。
我们可以针对每一种操作，INSERT/UPDATE/DELETE都用一个或多个经过编码的事件来表示。
例如：针对UPDATE，我们用两个操作来表达，[DELETE] 数据+  [INSERT]数据。也就是先把之前的数据删除，然后再插入一条新的数据。针对DELETE，我们也可以对流中的数据进行编码，[DELETE]数据。
总体来说，我们通过对流数据进行编码，也可以告诉DataStream的下游，[DELETE]表示发出MySQL的DELETE操作，将数据删除。用 [INSERT]表示插入新的数据。

将表转换为三种不同编码方式的流
Flink中的Table API或者SQL支持三种不同的编码方式。分别是：
Append-only流
Retract流
Upsert流

分别来解释下这三种流。

Append-only流
跟INSERT操作对应。这种编码类型的流针对的是只会不断新增的Dynamic Table。这种方式好处理，不需要进行特殊处理，源源不断地往流中发送事件即可。

Retract流
这种流就和Append-only不太一样。上面的只能处理INSERT，如果表会发生DELETE或者UPDATE，Append-only编码方式的流就不合适了。Retract流有几种类型的事件类型：
ADD MESSAGE：这种消息对应的就是INSERT操作。
RETRACT MESSAGE：直译过来叫取消消息。这种消息对应的就是DELETE操作。

我们可以看到通过ADD MESSAGE和RETRACT MESSAGE可以很好的向外部系统表达删除和插入操作。那如何进行UPDATE呢？好办！RETRACT MESSAGE + ADD MESSAGE即可。先把之前的数据进行删除，然后插入一条新的。完美~

![image](https://user-images.githubusercontent.com/75486726/178264254-35fc6d06-dc03-4801-8416-78574681c475.png)

Upsert流
前面我们看到的RETRACT编码方式的流，实现UPDATE是使用DELETE + INSERT模式的。大家想一下：在MySQL中我们更新数据的时候，肯定不会先DELETE掉一条数据，然后再插入一条数据，肯定是直接发出UPDATE语句执行更新。而Upsert编码方式的流，是能够支持Update的，这种效率更高。它同样有两种类型的消息：
UPSERT MESSAGE：这种消息可以表示要对外部系统进行Update或者INSERT操作
DELETE MESSAGE：这种消息表示DELETE操作。
Upsert流是要求必须指定Primary Key的，因为Upsert操作是要有Key的。Upsert流针对UPDATE操作用一个UPSERT MESSAGE就可以描述，所以效率会更高。

![image](https://user-images.githubusercontent.com/75486726/178264290-ac4cc524-4aec-40c5-a7a3-c1cce62aa177.png)
