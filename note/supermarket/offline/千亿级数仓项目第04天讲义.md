# 千亿级数仓第04天讲义

**课程目标**

* 了解用户行为日志数据采集方案

* 掌握flume自定义拦截器开发与使用

* 掌握日志数据ETL处理程序

* 掌握spark程序数据倾斜解决方案


## 数据采集-点击流日志数据

### 网站流量日志数据获取

随着网站在技术和运营上的不断技术发展，人们对数据的要求越来越高，以求实现更加精细的运营来提升网站的质量。所以数据的获取方式也随着网站技术的进步和人们对网站数据需求的加深而不断地发展。从使用发展来看，主要分为2类：

* 网站日志文件（Log files）
* 页面埋点js自定义采集

 

#### 网站日志文件

记录网站日志文件的方式是最原始的数据获取方式。主要在服务端完成，在网站的应用服务器配置相应的写日志的功能就能够实现，很多web应用服务器自带日志的记录功能。如Nginx的access.log日志等。

| 网站请求流程图          |
| ----------------------- |
| ![img](assets/wps1.jpg) |

 

优点

* 获取数据时不需要对页面做相关处理，可以直接开始统计相关请求信息

缺点

* 有些信息无法采集，比如用户在页面端的操作（如点击、ajax的使用等）无法记录
* 限制了一些指标的统计和计算。



#### 页面埋点js自定义采集

自定义采集用户行为数据，通过在页面嵌入自定义的javascript代码来获取用户的访问行为（比如鼠标悬停的位置，点击的页面组件等），然后通过ajax请求到后台记录日志，这种方式所能采集的信息会更加全面。



在实际操作中，有以下几个方面的数据可以自定义的采集：

* **系统特征**
	* 比如所采用的操作系统、浏览器、域名和访问速度等。

* **访问特征**
	* 包括点击的URL、所点击的“页面标签<a>”及标签的属性等。

* **来源特征**
	* 包括来访URL，来访IP等。

* **产品特征**
	* 包括所访问的产品编号、产品类别、产品颜色、产品价格、产品利润、产品数量和特价等级等。



以某电商网站为例，当用户点击相关产品页面时，其自定义采集系统就会收集相关的行为数据，发到后端的服务器，收集的数据日志格式如下：

| 网站埋点采集示例                                             |
| ------------------------------------------------------------ |
| ![image-20200210080223708](assets/image-20200210080223708.png) |

 

```java
https://mercury.jd.com/log.gif?t=search.000001&m=UA-J2011-1&pin=-&uid=15401915286171416477093&sid=15401915286171416477093|5&v=je=0sc=24-bitsr=1536x864ul=zh-cncs=UTF-8dt=iphone xs max - 商品搜索 - 京东hn=search.jd.comfl=-os=winbr=chromebv=70.0.3538.67$wb=1540191529xb=1540531132yb=1540539558zb=5cb=2usc=baiduucp=-umd=organicuct=not setct=1540539573584lt=389tad=-keyword=iphone xs maxev=0ab=0011mtest=group_base,ext_attr_fliter,qpv3,qpt9,qpz7rel_ver=V0700sig=80It1J9QZbpAL74eICv7MjyjnF9YmbJPZT9figy0Mw1of5qw7/hLNdEVuOn3Ui9yHjym3F0CT67flqqHfj0fyI08i8pf8Asn+7thpEDDaJZjrwK/gHpYwQNN2MK6q/GuOZfL8VOsvbLDGo3rpj+R1jMIO4n5hg0Kv6yrwrFLlSA=rel_cat2=653,6880rel_cat3=655,6881logid=1540539562.92430$loc=1-72-2799-
```

### 埋点js自定义采集原理分析

#### 请求流程图

| 请求流程图              |
| ----------------------- |
| ![img](assets/wps3.jpg) |



#### 原理分析

1. 用户的行为会触发浏览器对被统计页面的一个http请求
	* 比如打开某网页。当网页被打开，页面中的埋点javascript代码会被执行

2. 数据收集完成后，js会请求一个后端的数据收集脚本（上图的backend）
	* 这个脚本一般是一个伪装成图片的动态脚本程序，
3. js将收集到的数据通过http参数的方式传递给后端脚本
4. 后端脚本解析参数并按固定格式记录到访问日志，并在http响应中给客户端种植一些用于追踪的cookie的代码



> 埋点
>
> * 在网页中预先加入小段javascript代码
> * 这个代码片段一般会动态创建一个script标签，并将src属性指向一个单独的js文件
> * 此时这个单独的js文件（图中绿色节点）会被浏览器请求到并执行，这个js往往就是真正的数据收集脚本。



#### 埋点js方案实现

* 埋点js自定义采集实现中涉及到大量前端js技术
* 非课程的重点，了解其中原理即可，感兴趣的同学可以查看「<u>埋点js自定义采集实现.md</u>」

### 网站流量日志数据采集Flume采集

在网站流量日志分析场景中，对数据采集部分的可靠性、容错能力要求通常不会非常严苛，需要注意结合语境分析是何种含义的数据采集：

* 对于数据从无到有的过程
	* 结合使用web服务器自带的日志功能、自定义埋点JavaScript采集
	* 收集用户访问网站的行为数据

* 对于数据需要做搬运的操作
	* 使用Flume定制相关的采集方案满足数据采集传输
	
	
### Flume版本选择

针对nginx日志生成场景

* Flume 1.6
	* 无论是Spooling Directory Source和Exec Source均不能满足动态实时收集的需求
* Flume 1.8+
	* 提供了一个非常好用的TaildirSource
	* 使用这个source，可以监控一个目录，并且使用正则表达式匹配该目录中的文件名进行实时收集



Flume采集系统的搭建

1. 在服务器上部署agent节点，修改配置文件

2. 启动agent节点，将采集到的数据汇聚到指定的HDFS目录中



### Flume核心配置

#### tailDirSource配置

核心配置如下

```properties
a1.sources = r1
a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
```



配置说明

* **filegroups**
	* 指定filegroups，可以有多个，以空格分隔；（TailSource可以同时监控tail多个目录中的文件）

* **positionFile**
	* 配置检查点文件的路径，检查点文件会以json格式保存已经tail文件的位置，解决了断点不能续传的缺陷。

* **filegroups.<filegroupName>**
	* 配置每个filegroup的文件绝对路径，文件名可以用正则表达式匹配



通过以上配置，即可监控文件内容的增加和文件的增加。产生和所配置的文件名正则表达式不匹配的文件，则不会被tail。



#### sink配置

本次将日志采集到HDFS中，需要使用HDFSSink文件。HDFSSink需要配置滚动属性。



* 基于hdfs文件副本数
	* 配置项：hdfs.minBlockReplicas
	* 默认值：和hdfs的副本数一致
	* 说明
		* hdfs.minBlockReplicas是为了让flume感知不到hdfs的块复制，这样滚动方式配置（比如时间间隔、文件大小、events数量等）才不会受影响



示例说明：

假如hdfs的副本为3，配置的滚动时间为10秒，那么在第二秒的时候，flume检测到hdfs在复制块，这时候flume就会滚动，这样导致flume的滚动方式受到影响。所以**通常hdfs.minBlockReplicas配置为1**，就检测不到副本的复制了。但是hdfs的实际副本还是3

#### 完整版配置文件

```properties
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source

a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*

# Describe the sink
#指定hdfs sink
a1.sinks.k1.type = hdfs
#hdfs目录，带有时间信息
a1.sinks.k1.hdfs.path = /flume/tailout/%Y-%m-%d/
#生成的hdfs文件名的前缀
a1.sinks.k1.hdfs.filePrefix = events-
#指定滚动时间，默认是30秒，设置为0表示禁用该策略
a1.sinks.k1.hdfs.rollInterval = 0
#指定滚动大小，设置为0表示禁用该策略
a1.sinks.k1.hdfs.rollSize = 200000000
#指定滚动条数
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#副本策略
a1.sinks.k1.hdfs.minBlockReplicas=1
#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
a1.sinks.k1.hdfs.fileType = DataStream

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

#### 启动flume agent采集数据

创建目录

```shell
mkdir -p /var/log/test1/
mkdir -p /var/log/test2/
```



上传测试数据到上面创建的目录！！



flume启动命令

```shell
bin/flume-ng agent --conf-file job/log2hdfs.conf -name a1  -Dflume.root.logger=INFO,console
```

#### 思考问题 hdfs路径是否正确

**问题**

按照上面flume agent的配置文件会出现一种情况，数据存放的路径信息不正确，需要按照日志时间存储。

**解决方式**

需要使用flume拦截器实现此需求，选择使用flume自带拦截器还是自定义拦截器？

| flume官方自带拦截器                                          |
| ------------------------------------------------------------ |
| ![image-20200216165550634](assets/image-20200216165550634.png) |

#### flume自定义拦截器

**实现步骤**

1. 创建maven工程

2. 新建class实现flume提供的Interceptor接口

   * 实现相关方法

     interceptor方法

     定义静态内部类实现Interceptor.Builder接口

3. 打成jar包上传到flume安装目录下lib文件夹中

4. 开发flume agent配置文件引用header中的日期信息

   

具体实现

**创建maven java工程，导入jar包**

```properties
<dependencies>
    <dependency>
        <groupId>org.apache.flume</groupId>
        <artifactId>flume-ng-core</artifactId>
        <version>1.8.0</version>
        <scope>provided</scope>
    </dependency>
</dependencies>
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.0</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
                <encoding>UTF-8</encoding>
            </configuration>
        </plugin>
    </plugins>
</build>

```



 

 

 

**自定义flume的拦截器**

```java
package com.itheima.interceptor;

import org.apache.commons.compress.utils.Charsets;
import org.apache.commons.lang.StringUtils;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class CustomerInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        //获得body的内容
        String eventBody = new String(event.getBody(), Charsets.UTF_8);
        final String[] bodyArr = eventBody.split(" ");
        String time_local = "";
        if (bodyArr.length > 11) {
            time_local = bodyArr[4] ;
        }
        final Map<String, String> headers = event.getHeaders();
        // 添加时间信息 到event的header
        if (StringUtils.isNotBlank(time_local)) {

            headers.put("event_time", time_local);
        } else {
            headers.put("event_time", "unkown");
        }
        event.setHeaders(headers);
        return event;
    }

    @Override
    public List<Event> intercept(List<Event> events) {
        List<Event> out = new ArrayList<Event>();
        for (Event event : events) {
            Event outEvent = intercept(event);
            if (outEvent != null) {
                out.add(outEvent);
            }
        }
        return out;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {

        @Override
        public Interceptor build() {
            return new CustomerInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```



 

 

**打包上传服务器**

将我们的拦截器打成jar包放到flume的lib目录下

| ![img](assets/wps1-1581844124624.jpg)                        |
| ------------------------------------------------------------ |
| ![image-20200216171112113](assets/image-20200216171112113.png) |

 

 

 

**开发flume的配置文件**

开发flume的配置文件

```properties
# Name the components on this agent
 a1.sources = r1
 a1.sinks = k1
 a1.channels = c1

 # Describe/configure the source

 a1.sources.r1.type = TAILDIR
 a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
 a1.sources.r1.filegroups = f1 f2
 a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
 a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*

 #interceptor
 a1.sources.r1.interceptors =i1 
 a1.sources.r1.interceptors.i1.type =com.itheima.interceptor.CustomTimeInterceptor$Builder
 # Describe the sink
 #指定hdfs sink
 a1.sinks.k1.type = hdfs
 #hdfs目录，带有时间信息
 a1.sinks.k1.hdfs.path = /flume/tailout/%{event_time}/
 #生成的hdfs文件名的前缀
 a1.sinks.k1.hdfs.filePrefix = events-
 #指定滚动时间，默认是30秒，设置为0表示禁用该策略
 a1.sinks.k1.hdfs.rollInterval = 0
 #指定滚动大小，设置为0表示禁用该策略
 a1.sinks.k1.hdfs.rollSize = 200000000
 #指定滚动条数
 a1.sinks.k1.hdfs.rollCount = 0
 a1.sinks.k1.hdfs.batchSize = 100
 #副本策略
 a1.sinks.k1.hdfs.minBlockReplicas=1
 #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
 a1.sinks.k1.hdfs.fileType = DataStream

 # Use a channel which buffers events in memory
 a1.channels.c1.type = memory
 a1.channels.c1.capacity = 1000
 a1.channels.c1.transactionCapacity = 100

 # Bind the source and sink to the channel
 a1.sources.r1.channels = c1
 a1.sinks.k1.channel = c1
```



## 数据仓库-ETL处理

### 点击流概念

* **点击流（Click Stream）是指用户在网站上持续访问的轨迹**，注重用户浏览网站的整个流程

* 用户对网站的每次访问包含了一系列的点击动作行为，这些点击行为数据就构成了点击流数据（Click Stream Data），它代表了用户浏览网站的整个流程

* 点击流和网站日志是两个不同的概念
	* 点击流是从用户的角度出发，注重用户浏览网站的整个流程
	* 网站日志是面向整个站点，它包含了用户行为数据、服务器响应数据等众多日志信息
	
* 通过对网站日志的分析可以获得用户的点击流数据

  | 点击流示意图                                                 |
  | ------------------------------------------------------------ |
  | ![image-20200110210412619](assets/image-20200110210412619.png) |

  

### 点击流模型

点击流模型完全是业务模型，相关概念由业务指定而来。由于大量的指标统计从点击流模型中更容易得出，所以在预处理阶段，可以使用spark程序来生成点击流模型的数据。



在点击流模型中，存在着两种模型数据：

* PageViews
* Visits

#### pageviews模型

Pageviews模型数据专注于用户每次会话（session）的识别，以及每次session内访问了几步和每一步的停留时间。

> 在日志数据分析中，通常把前后两条访问记录时间差在30分钟以内算成一次会话。如果超过30分钟，则把下次访问算成新的会话开始

大致步骤如下：

* 在所有访问日志中找出该用户的所有访问记录
* 把该用户所有访问记录按照时间正序排序
* 计算前后两条记录时间差是否为30分钟
* 如果小于30分钟，则是同一会话session的延续
* 如果大于30分钟，则是下一会话session的开始
* 用前后两条记录时间差算出上一步停留时间
* 最后一步和只有一步的 业务默认指定页面停留时间60s



#### visit模型

Visit模型专注于每次会话session内起始、结束的访问情况信息。比如用户在某一个会话session内，进入会话的起始页面和起始时间，会话结束是从哪个页面离开的，离开时间，本次session总共访问了几个页面等信息。

大致步骤如下：

* 在pageviews模型上进行梳理

* 对每一个session内所有访问记录按照时间正序排序

*  第一天的时间页面就是起始时间页面

* 业务指定最后一条记录的时间页面作为离开时间和离开页面

### 数据清洗

#### 主要目的

* 过滤“不合规”数据，清洗无意义的数据

* 格式转换和规整

* 根据后续的统计需求，过滤分离出各种不同主题(不同栏目path)的基础数据。

| 数据预处理              |
| ----------------------- |
| ![img](assets/wps4.png) |

#### 创建日志数据hive-ods层表

执行资料中日志数据-ods层建表语句文件夹中的sql语句，分别创建三张表：

sql语句

```sql



--1．创建ODS层数据表
--1.1．原始日志清洗数据表
drop table if exists itcast_ods.weblog_origin;
create  table itcast_ods.weblog_origin(
valid Boolean,
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_sent string,
http_referer string,
http_user_agent string,
guid string)
partitioned by (dt string)
STORED AS PARQUET TBLPROPERTIES('parquet.compression'='SNAPPY');


--注意事项：
--parquet中字段数据类型要与hive表字段类型保持一致！！
--1.2．点击流模型pageviews表
drop table if exists itcast_ods.click_pageviews;
create EXTERNAL table itcast_ods.click_pageviews(
session string,
remote_addr string,
time_local string,
request string,
visit_step int,
page_staylong string,
http_referer string,
http_user_agent string,
body_bytes_sent string,
status string,
guid string)
partitioned by (dt string)
STORED AS PARQUET TBLPROPERTIES('parquet.compression'='SNAPPY');


--1.3．点击流visit模型表

drop table if exists itcast_ods.click_stream_visit;
create table itcast_ods.click_stream_visit(
guid string,
session   string,
remote_addr string,
inTime    string,
outTime   string,
inPage    string,
outPage   string,
referal   string,
pageVisits  int)
partitioned by (dt string)
STORED AS PARQUET TBLPROPERTIES('parquet.compression'='SNAPPY');
```

#### 日志数据说明

日志数据内容样例：



```txt
f5dd685d-6b83-4e7d-8c37-df8797812075 222.68.172.190 - - 2018-11-01 14:34:57 "GET /images/my.jpg HTTP/1.1" 200 19939 "http://www.angularjs.cn/A00n" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36" 
```



字段解析：

```markdown
1、用户id信息-uid: f5dd685d-6b83-4e7d-8c37-df8797812075
2、访客ip地址：  222.68.172.190
3、访客用户信息：  - -
4、请求时间：2018-11-01 14:34:57
5、请求方式：GET
6、请求的url：/images/my.jpg
7、请求所用协议：HTTP/1.1
8、响应码：200
9、返回的数据流量：19939
10、访客的来源url：http://www.angularjs.cn/A00n
11、访客所用浏览器：Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36


```

**注意：分隔符为空格！**

#### 实现方式-普通版本

使用spark rdd程序对数据进行预处理



编程小技巧

* 如果涉及多属性值数据传递 通常可建立与之对应的bean对象携带数据传递
* 注意写出数据时要保存为parquet格式方便后续数据入hive映射方便。
* 如涉及不符合本次分析的脏数据，往往采用逻辑删除，也就是自定义标记位，比如使用1或者0来表示数据是否有效，而不是直接物理删除

**初步清洗数据**

```scala
package com.itheima.main

import java.util.UUID

import com.itheima.bean.{PageViewsBeanCase, VisitBeanCase, WebLogBean, WeblogBeanCase}
import com.itheima.util.DateUtil
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ListBuffer

object ETLApp {
  def main(args: Array[String]): Unit = {
    //准备sparksession
    val conf = new SparkConf()
    val spark = SparkSession.builder()
      .master("local[*]")
      .config(conf).appName(ETLApp.getClass.getName).getOrCreate()
    //获取上下文
    val sc: SparkContext = spark.sparkContext
    /*
    1:读取日志文件，解析封装为weblogbean对象
    2：过滤掉静态请求资源路径
  
     */
    //flume采集输出的路径
    val textRdd: RDD[String] = sc.textFile("/spark_etl/data/input2/")
    val webLogBeanRdd: RDD[WebLogBean] = textRdd.map(WebLogBean(_))
    //过滤掉不合法的请求
    val filterWeblogBeanRdd: RDD[WebLogBean] = webLogBeanRdd.filter(
      x => {
        x != null && x.valid
      }
    )
    //2：过滤掉静态请求资源路径,哪些是静态的资源路径，准备一个初始规则文件，初始的集合装有规则的路径
    initlizePages
    //使用广播变量广播规则
    val pagesBroadCast: Broadcast[mutable.HashSet[String]] = sc.broadcast(pages)
    val filterStaticWeblogRdd: RDD[WebLogBean] = filterWeblogBeanRdd.filter(
      bean => {
        val request: String = bean.request
        val res: Boolean = pagesBroadCast.value.contains(request) //pages直接如此使用是否好？是driver端还是executor端的？所以为了性能要考虑使用广播变量广播pages规则
        //如果被规则文件包含则过滤掉这个请求
        if (res) {
          false
        } else {
          true
        }
      }
    )
    
    //过滤和清洗已经完成

    import spark.implicits._
    val weblogBeanCaseDataset: Dataset[WeblogBeanCase] = filterStaticWeblogRdd.map(bean => WeblogBeanCase(bean.valid, bean.remote_addr, bean.remote_user, bean.time_local,
      bean.request, bean.status, bean.body_bytes_sent, bean.http_referer, bean.http_user_agent, bean.guid)).toDS()

    //保存为parquet文件
    weblogBeanCaseDataset.write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/weblog_origin/dt=20191101/")



    //停止程序
    sc.stop()
  }

  //我们准备一个静态资源的集合
  // 用来存储网站url分类数据
  val pages = new mutable.HashSet[String]()

  //初始化静态资源路径集合
  def initlizePages(): Unit = {
    pages.add("/about")
    pages.add("/black-ip-list/")
    pages.add("/cassandra-clustor/")
    pages.add("/finance-rhive-repurchase/")
    pages.add("/hadoop-family-roadmap/")
    pages.add("/hadoop-hive-intro/")
    pages.add("/hadoop-zookeeper-intro/")
    pages.add("/hadoop-mahout-roadmap/")
  }
}


```

**生成pageview模型**

```scala
//pageview模型生成
    //按照用户id分组（uid,iterator(用户的所有访问记录)）
    val uidWeblogRdd: RDD[(String, Iterable[WebLogBean])] = filterStaticWeblogRdd.groupBy(x => x.guid)


    //排序
    val pageviewBeanCaseRdd: RDD[PageViewsBeanCase] = uidWeblogRdd.flatMap(
      item => {
        val uid = item._1
        //按照时间排序
        val sortWeblogBeanList: List[WebLogBean] = item._2.toList.sortBy(_.time_local)
        //两两比较，生成sessionid,step,staylong(停留时长),如何计算停留时长：遍历的时候都是计算上一条的停留时长，sessionid,step信息
        //只有一条记录：sessionid,step,staylong:60s
        val pageViewBeanCaseList: ListBuffer[PageViewsBeanCase] = new ListBuffer[PageViewsBeanCase]()
        import scala.util.control.Breaks._
        var sessionid: String = UUID.randomUUID().toString
        var step = 1
        var page_staylong = 60000
        breakable {
          for (num <- 0 until (sortWeblogBeanList.size)) {
            //一条访问记录
            val bean: WebLogBean = sortWeblogBeanList(num)
            //如果只由一条记录
            if (sortWeblogBeanList.size == 1) {
              val pageViewsBeanCase = PageViewsBeanCase(sessionid, bean.remote_addr, bean.time_local,
                bean.request, step, page_staylong,
                bean.http_referer, bean.http_user_agent, bean.body_bytes_sent, bean.status, bean.guid)
              //之前输出，现在则需要保存起来最后一起输出
              pageViewBeanCaseList += pageViewsBeanCase
              //重新生成sessionid
              sessionid = UUID.randomUUID().toString
              //跳出循环
              break
            }
            //数量不止一条，本条来计算上一条的时长
            breakable {
              if (num == 0) {
                //continue:中止本次，进入下次循环
                break()
              }
              //不是第一条数据，获取到上一条记录
              val lastBean: WebLogBean = sortWeblogBeanList(num - 1)
              //判断的是两个bean对象的差值
              val timeDiff: Long = DateUtil.getTimeDiff(lastBean.time_local, bean.time_local)
              //毫秒值是否小于30分钟
              if (timeDiff <= 30 * 60 * 1000) {
                //属于同个session，你们俩共用一个sessionid,输出上一条
                val pageViewsBeanCase = PageViewsBeanCase(sessionid, lastBean.remote_addr, lastBean.time_local, lastBean.request,
                  step, timeDiff, lastBean.http_referer, lastBean.http_user_agent, lastBean.body_bytes_sent,
                  lastBean.status, lastBean.guid
                )
                //添加到集合中
                pageViewBeanCaseList += pageViewsBeanCase
                //sessionid是否重新生成：不需要，step如何处理？
                step += 1

              } else {
                //不属于一个sessionid,如何处理？不管是否属于同个会话，我们输出的都是上一条记录
                //属于同个session，你们俩共用一个sessionid,输出上一条
                val pageViewsBeanCase = PageViewsBeanCase(sessionid, lastBean.remote_addr, lastBean.time_local, lastBean.request,
                  step, page_staylong, lastBean.http_referer,
                  lastBean.http_user_agent, lastBean.body_bytes_sent,
                  lastBean.status, lastBean.guid
                )
                //添加到集合中
                pageViewBeanCaseList += pageViewsBeanCase
                //sessionid是否重新生成：需要，step如何处理？
                sessionid = UUID.randomUUID().toString
                //step重置为1
                step = 1
              }

              //最后一条需要我们控制输出
              if (num == sortWeblogBeanList.size - 1) {

                val pageViewsBeanCase = PageViewsBeanCase(sessionid, bean.remote_addr, bean.time_local,
                  bean.request, step, page_staylong,
                  bean.http_referer, bean.http_user_agent, bean.body_bytes_sent, bean.status, bean.guid)
                //之前输出，现在则需要保存起来最后一起输出
                pageViewBeanCaseList += pageViewsBeanCase
                //重新生成sessionid
                sessionid = UUID.randomUUID().toString
              }
            }

          }
        }

        pageViewBeanCaseList
      }
    )

    pageviewBeanCaseRdd.toDS().write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/click_pageviews/dt=20191101/")

```

生成visit模型

```scala
 //生成visit模型，汇总每个会话的总步长，时长等信息
    //按照sessionid分组,(sessionid,iterable(pageviewbeancase))
    val sessionidRdd: RDD[(String, Iterable[PageViewsBeanCase])] = pageviewBeanCaseRdd.groupBy(_.session)

    val visitBeanCaseRdd: RDD[VisitBeanCase] = sessionidRdd.map(
      item => {
        //sessionid
        val sessionid: String = item._1
        //sessionid对应的访问记录
        val cases: List[PageViewsBeanCase] = item._2.toList.sortBy(_.time_local)
        VisitBeanCase(cases(0).guid, sessionid, cases(0).remote_addr, cases(0).time_local, cases(cases.size - 1).time_local,
          cases(0).request, cases(cases.size - 1).request, cases(0).htp_referer, cases.size
        )
      }
    )

    visitBeanCaseRdd.toDS().write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/click_stream_visit/dt=20191101/")
```



**完整版参考代码:**

``` scala
package com.itheima.main

import java.util.UUID

import com.itheima.bean.{PageViewsBeanCase, VisitBeanCase, WebLogBean, WeblogBeanCase}
import com.itheima.util.DateUtil
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ListBuffer

object ETLApp {
  def main(args: Array[String]): Unit = {
    //准备sparksession
    val conf = new SparkConf()
    val spark = SparkSession.builder()
      .master("local[*]")
      .config(conf).appName(ETLApp.getClass.getName).getOrCreate()
    //获取上下文
    val sc: SparkContext = spark.sparkContext
    /*
    1:读取日志文件，解析封装为weblogbean对象
    2：过滤掉静态请求资源路径
    3：按照用户id分组，生成sessionid
    4：生成visit模型
     */
    //flume采集输出的路径
    val textRdd: RDD[String] = sc.textFile("/spark_etl/data/input2/")
    val webLogBeanRdd: RDD[WebLogBean] = textRdd.map(WebLogBean(_))
    //过滤掉不合法的请求
    val filterWeblogBeanRdd: RDD[WebLogBean] = webLogBeanRdd.filter(
      x => {
        x != null && x.valid
      }
    )
    //2：过滤掉静态请求资源路径,哪些是静态的资源路径，准备一个初始规则文件，初始的集合装有规则的路径
    initlizePages
    //使用广播变量广播规则
    val pagesBroadCast: Broadcast[mutable.HashSet[String]] = sc.broadcast(pages)
    val filterStaticWeblogRdd: RDD[WebLogBean] = filterWeblogBeanRdd.filter(
      bean => {
        val request: String = bean.request
        val res: Boolean = pagesBroadCast.value.contains(request) //pages直接如此使用是否好？是driver端还是executor端的？所以为了性能要考虑使用广播变量广播pages规则
        //如果被规则文件包含则过滤掉这个请求
        if (res) {
          false
        } else {
          true
        }
      }
    )

    //过滤和清洗已经完成

    import spark.implicits._
    val weblogBeanCaseDataset: Dataset[WeblogBeanCase] = filterStaticWeblogRdd.map(bean => WeblogBeanCase(bean.valid, bean.remote_addr, bean.remote_user, bean.time_local,
      bean.request, bean.status, bean.body_bytes_sent, bean.http_referer, bean.http_user_agent, bean.guid)).toDS()

    //保存为parquet文件
    weblogBeanCaseDataset.write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/weblog_origin/dt=20191101/")

    //pageview模型生成
    //按照用户id分组（uid,iterator(用户的所有访问记录)）
    val uidWeblogRdd: RDD[(String, Iterable[WebLogBean])] = filterStaticWeblogRdd.groupBy(x => x.guid)


    //排序
    val pageviewBeanCaseRdd: RDD[PageViewsBeanCase] = uidWeblogRdd.flatMap(
      item => {
        val uid = item._1
        //按照时间排序
        val sortWeblogBeanList: List[WebLogBean] = item._2.toList.sortBy(_.time_local)
        //两两比较，生成sessionid,step,staylong(停留时长),如何计算停留时长：遍历的时候都是计算上一条的停留时长，sessionid,step信息
        //只有一条记录：sessionid,step,staylong:60s
        val pageViewBeanCaseList: ListBuffer[PageViewsBeanCase] = new ListBuffer[PageViewsBeanCase]()
        import scala.util.control.Breaks._
        var sessionid: String = UUID.randomUUID().toString
        var step = 1
        var page_staylong = 60000
        breakable {
          for (num <- 0 until (sortWeblogBeanList.size)) {
            //一条访问记录
            val bean: WebLogBean = sortWeblogBeanList(num)
            //如果只由一条记录
            if (sortWeblogBeanList.size == 1) {
              val pageViewsBeanCase = PageViewsBeanCase(sessionid, bean.remote_addr, bean.time_local,
                bean.request, step, page_staylong,
                bean.http_referer, bean.http_user_agent, bean.body_bytes_sent, bean.status, bean.guid)
              //之前输出，现在则需要保存起来最后一起输出
              pageViewBeanCaseList += pageViewsBeanCase
              //重新生成sessionid
              sessionid = UUID.randomUUID().toString
              //跳出循环
              break
            }
            //数量不止一条，本条来计算上一条的时长
            breakable {
              if (num == 0) {
                //continue:中止本次，进入下次循环
                break()
              }
              //不是第一条数据，获取到上一条记录
              val lastBean: WebLogBean = sortWeblogBeanList(num - 1)
              //判断的是两个bean对象的差值
              val timeDiff: Long = DateUtil.getTimeDiff(lastBean.time_local, bean.time_local)
              //毫秒值是否小于30分钟
              if (timeDiff <= 30 * 60 * 1000) {
                //属于同个session，你们俩共用一个sessionid,输出上一条
                val pageViewsBeanCase = PageViewsBeanCase(sessionid, lastBean.remote_addr, lastBean.time_local, lastBean.request,
                  step, timeDiff, lastBean.http_referer, lastBean.http_user_agent, lastBean.body_bytes_sent,
                  lastBean.status, lastBean.guid
                )
                //添加到集合中
                pageViewBeanCaseList += pageViewsBeanCase
                //sessionid是否重新生成：不需要，step如何处理？
                step += 1

              } else {
                //不属于一个sessionid,如何处理？不管是否属于同个会话，我们输出的都是上一条记录
                //属于同个session，你们俩共用一个sessionid,输出上一条
                val pageViewsBeanCase = PageViewsBeanCase(sessionid, lastBean.remote_addr, lastBean.time_local, lastBean.request,
                  step, page_staylong, lastBean.http_referer,
                  lastBean.http_user_agent, lastBean.body_bytes_sent,
                  lastBean.status, lastBean.guid
                )
                //添加到集合中
                pageViewBeanCaseList += pageViewsBeanCase
                //sessionid是否重新生成：需要，step如何处理？
                sessionid = UUID.randomUUID().toString
                //step重置为1
                step = 1
              }

              //最后一条需要我们控制输出
              if (num == sortWeblogBeanList.size - 1) {

                val pageViewsBeanCase = PageViewsBeanCase(sessionid, bean.remote_addr, bean.time_local,
                  bean.request, step, page_staylong,
                  bean.http_referer, bean.http_user_agent, bean.body_bytes_sent, bean.status, bean.guid)
                //之前输出，现在则需要保存起来最后一起输出
                pageViewBeanCaseList += pageViewsBeanCase
                //重新生成sessionid
                sessionid = UUID.randomUUID().toString
              }
            }

          }
        }

        pageViewBeanCaseList
      }
    )

    pageviewBeanCaseRdd.toDS().write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/click_pageviews/dt=20191101/")


    //生成visit模型，汇总每个会话的总步长，时长等信息
    //按照sessionid分组,(sessionid,iterable(pageviewbeancase))
    val sessionidRdd: RDD[(String, Iterable[PageViewsBeanCase])] = pageviewBeanCaseRdd.groupBy(_.session)

    val visitBeanCaseRdd: RDD[VisitBeanCase] = sessionidRdd.map(
      item => {
        //sessionid
        val sessionid: String = item._1
        //sessionid对应的访问记录
        val cases: List[PageViewsBeanCase] = item._2.toList.sortBy(_.time_local)
        VisitBeanCase(cases(0).guid, sessionid, cases(0).remote_addr, cases(0).time_local, cases(cases.size - 1).time_local,
          cases(0).request, cases(cases.size - 1).request, cases(0).htp_referer, cases.size
        )
      }
    )

    visitBeanCaseRdd.toDS().write.mode("overwrite")
      .parquet("/user/hive/warehouse/itcast_ods.db/click_stream_visit/dt=20191101/")


    //停止程序
    sc.stop()
  }

  //我们准备一个静态资源的集合
  // 用来存储网站url分类数据
  val pages = new mutable.HashSet[String]()

  //初始化静态资源路径集合
  def initlizePages(): Unit = {
    pages.add("/about")
    pages.add("/black-ip-list/")
    pages.add("/cassandra-clustor/")
    pages.add("/finance-rhive-repurchase/")
    pages.add("/hadoop-family-roadmap/")
    pages.add("/hadoop-hive-intro/")
    pages.add("/hadoop-zookeeper-intro/")
    pages.add("/hadoop-mahout-roadmap/")
  }
}


```

#### 实现方式-数据倾斜版本



**解决spark程序倾斜整体思路**

* 通过Spark UI找到产生倾斜的shuffle算子

* 分析产生倾斜的原因

* 对倾斜的key进行处理 

  * 对key增加随机数

  * 增加并行度

  * 单独处理倾斜的key

  * rangepartioner+key转换处理

* 编写代码进行验证

  

**本次倾斜解决具体思路**

* 转换key:guid-->guid+time_local,

* 利用sortbykey对guid+time_local进行全局排序

* 使用rangepartioner按照key的范围进行划分（保证每个分区数据大致均匀）

* 第一次生成sessionid,使用累加器收集边界处的数据

* 处理边界处数据获取正确的sessionid



**完整版代码参考**

```scala
package com.itheima.service

import java.util
import java.util.UUID

import com.itheima.bean.{PageViewsBeanCase, WebLogBean}
import com.itheima.util.DateUtil
import org.apache.spark.RangePartitioner
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.util.CollectionAccumulator

import scala.collection.mutable
import scala.collection.mutable.ListBuffer

class PageViewService {

}

object PageViewService {


  def savePageViewToHdfs(filterStaticWeblogRdd: RDD[WebLogBean]) = {
    /*
    1：把key由原来uid变为uid+timeLocal字段
    2：使用rangepartitioner进行均匀分区
     */
    //key:uid+timeLocal ,value:weblogbean
    val uidTimeRdd: RDD[(String, WebLogBean)] = filterStaticWeblogRdd.map(bean => (bean.guid + "&" + bean.time_local, bean))
    //为了保证key全局有序使用sortbykey进行排序
    val sortedUidTimeRdd: RDD[(String, WebLogBean)] = uidTimeRdd.sortByKey()
    //使用rangepartitioner均匀分区
    val rangeRdd: RDD[(String, WebLogBean)] = sortedUidTimeRdd.partitionBy(new RangePartitioner(100, sortedUidTimeRdd))


    //使用累加器收集每个分区的首尾记录
    val spark: SparkSession = SparkSession.getDefaultSession.get
    //使用集合类型累加器收集分区的首尾数据
    val headTailList: CollectionAccumulator[(String, String)] = spark.sparkContext.collectionAccumulator[(String, String)]("headTailList")
    //对每个分区使用mappartitionwithindex算子进行sessionid和步长信息的生成
    val questionSessionRdd: RDD[(WebLogBean, String, Int, Long)] = generateSessionid(rangeRdd, headTailList)
    //累加器中有数据必须触发计算,使用累加器一定要注意重复计算的问题
    //对rdd数据进行cache防止重复计算
    questionSessionRdd.cache()
    questionSessionRdd.count()
    val headTailListValue: util.List[(String, String)] = headTailList.value

    //保存数据
    questionSessionRdd.saveAsTextFile("/questionSessionRdd")
    //根据累加器中的边界数据判断哪些分区的边界存在问题
    //方便在累加器数据中获取指定分区的数据，我们把累加器数据结构调整为map类型：key:index+"&first/last",value: 记录的数据
    import collection.JavaConverters._
    val buffer: mutable.Buffer[(String, String)] = headTailListValue.asScala
    //转为一个可变map，方便更新其中的数据
    val map: mutable.HashMap[String, String] = mutable.HashMap(buffer.toMap.toSeq: _*) //map装有原来累加器中的数据
    //根据首尾数据判断边界问题得到需要修复的正确数据
    val correctMap: mutable.HashMap[String, String] = processBoundaryMap(map)


    //广播正确的map数据到每个executor
    val questionBroadCast: Broadcast[mutable.HashMap[String, String]] = spark.sparkContext.broadcast(correctMap)
    //经过修复过后的正确的rdd数据，（uidtime,sessionid,step,staylong）
    val correctRdd: RDD[(WebLogBean, String, Int, Long)] =
      repairBoundarySession(questionSessionRdd, questionBroadCast)

    val pageviewRdd: RDD[PageViewsBeanCase] = correctRdd.map(
      t => {


        PageViewsBeanCase(
          t._2, t._1.remote_addr, t._1.time_local, t._1.request, t._3, t._4,
          t._1.http_referer, t._1.http_user_agent, t._1.body_bytes_sent, t._1.status, t._1.guid
        )
      }
    )
    pageviewRdd.saveAsTextFile("/pageviewrddtxt")

  }


  //修复我们rdd边界处的数据
  def repairBoundarySession(uidTimeSessionStepLongRdd: RDD[( WebLogBean, String, Int, Long)],
                            questionBroadCast: Broadcast[mutable.HashMap[String, String]]) = {
    //key:index/first/last,value:last-->timediff,first-->correctsessionid+correctstep+quesitonsessionid
    val questionMap: mutable.HashMap[String, String] = questionBroadCast.value
    val correctRdd: RDD[(WebLogBean, String, Int, Long)] = uidTimeSessionStepLongRdd.mapPartitionsWithIndex(
      (index, iter) => {
        //uid&time,sessionid,step,staylong
        var orginList = iter.toList
        val firstLine: String = questionMap.getOrElse(index + "&first", "")
        val lastLine: String = questionMap.getOrElse(index + "&last", "")
        if (lastLine != "") {
          //当前这个分区最后一条数据他的停留时长需要修改
          val buffer: mutable.Buffer[(WebLogBean, String, Int, Long)] = orginList.toBuffer
          val lastTuple: (WebLogBean, String, Int, Long) = buffer.remove(buffer.size - 1) //只修改停留时长
          buffer += ((lastTuple._1, lastTuple._2, lastTuple._3, lastLine.toLong))
          orginList = buffer.toList
        }

        if (firstLine != "") {
          //分区第一条数据有问题，则需要修改：按照错误的sessionid找到所有需要修改的数据，改正sessionid和step
          val firstArr: Array[String] = firstLine.split("&")
          val tuples: List[(WebLogBean, String, Int, Long)] = orginList.map {
            t => {
              if (t._2.equals(firstArr(2))) {
                //错误的sessionid,修改为正确的sessionid和步长
                (t._1, firstArr(0), firstArr(1).toInt + t._3.toInt, t._4)
              } else {
                t
              }
            }
          }
          orginList=tuples
        }
        orginList.iterator
      }
    )
    correctRdd

  }

  //根据首尾数据找到有问题边界数据，以及修改的正确数据
  def processBoundaryMap(map: mutable.HashMap[String, String]) = {
    //定义一个map接收有问题分区需要修改的正确数据：key:index+"&first/last", value需要修改的正确数据
    val correctMap: mutable.HashMap[String, String] = new mutable.HashMap[String, String]()
    //遍历首尾记录map找到有问题的边界
    for (num <- 1 until (map.size / 2)) {
      //保证等于分区数据
      //获取num分区对应的首尾记录
      val numFirstMsg: String = map.get(num + "&first").get //uid+time+sessionid
      val numLastMsg: String = map.get(num + "&last").get //uid+time+sessionid+step+partition.size(分区数量)
      //获取到上一个分区的最后一条数据
      val lastPartLastMsg: String = map.get((num - 1) + "&last").get
      //判断当前分区与上个分区是否存在边界问题
      val numLastArr: Array[String] = numLastMsg.split("&")

      val lastPartLastArr: Array[String] = lastPartLastMsg.split("&")
      val numFirstArr: Array[String] = numFirstMsg.split("&")
      //判断是否同个用户
      if (lastPartLastArr(0).equals(numFirstArr(0))) {
        //判断时间差
        val timediff = DateUtil.getTimeDiff(lastPartLastArr(1), numFirstArr(1))
        if (timediff < 30 * 60 * 1000) {
          //说明当前分区第一条数据与上个分区最后一条属于同个会话
          //上个分区记录需要修改的正确的停留时长数据
          correctMap.put((num - 1) + "&last", timediff.toString)
          //当前分区的第一条数据（有可能是多条数据严谨来说应该是当前分区的第一个session的数据）需要修改的数据
          //sessionid:与上个分区最后一条数据的sessionid保持一致,step:应该是上个分区最后一条记录的step+1
          if (lastPartLastArr.size > 5) {
            //正确的sessionid+正确的step+错误的sessionid
            correctMap.put(num + "&first", lastPartLastArr(lastPartLastArr.size - 2) + "&"
              + lastPartLastArr(lastPartLastArr.size - 1) + "&" + numFirstArr(2))
          } else {

            correctMap.put(num + "&first", lastPartLastArr(2) + "&" + lastPartLastArr(3) + "&" + numFirstArr(2))
          }
          //判断当前整个分区是否属于同个会话，属于同个会话则更新map中当前分区对应的最后一条数据的sessionid和ste数据

          if (numFirstArr(2).equals(numLastArr(2))) {
            //说明是同个会话，存在了会话穿透多个分区的现象
            //更新最后一条数据的step和sessionid信息
            //numlastMsg +正确的sessionid（上个分区的最后一条数据的sessionid）+正确的步长step(上个分区最后一条数据的步长+
            // 当前分区的数量)
            if (lastPartLastArr.size > 5) {
              map.put(num + "&last", numLastMsg + "&" + lastPartLastArr(lastPartLastArr.size - 2) + "&" +
                (lastPartLastArr(lastPartLastArr.size - 1).toInt + numLastArr(4).toInt))

            } else {
              //uid+time+sessionid+step+partition.size(分区数量)+sessionid+step
              map.put(num + "&last", numLastMsg + "&" + lastPartLastArr(2) + "&" + (lastPartLastArr(3).toInt + numLastArr(4).toInt))
            }

          }
        }
      }


    }
    correctMap
  }

  //对均匀分区的rdd生成sessionid，使用累加器收集每个分区的首尾数据
  def generateSessionid(rangeRdd: RDD[(String, WebLogBean)],
                        headTailList: CollectionAccumulator[(String, String)]) = {


    //使用mappartitionwithindex算子
    val sessionidStepPageRdd: RDD[(WebLogBean, String, Int, Long)] = rangeRdd.mapPartitionsWithIndex {
      (index, iter) => {

        //iter-->list,list集合中依然是按照key有序分布的
        val list: List[(String, WebLogBean)] = iter.toList
        //准备一个list集合接收每条记录生成的sessionid的信息:数据内容：weblogbean,sessionid,step,pagestaylong
        val resultTupleList: ListBuffer[(WebLogBean, String, Int, Long)] = new ListBuffer[(WebLogBean, String, Int, Long)]()
        //准备sessionid，step，pagestaylong
        var sessionid = UUID.randomUUID().toString
        var step = 1
        var pagestaylong: Long = 60000
        //遍历list集合进行两两比较判断是否是同个用户以及时间是否小于30分钟
        import scala.util.control.Breaks._
        breakable {
          for (num <- 0 until (list.size)) {
            //取出当前遍历的数据
            val currentTuple: (String, WebLogBean) = list(num)
            //累加器收集第一条数据
            if (num == 0) {
              //把数据装入累加器中:key:分区编号+"&"+first/last,value:uid+time,sessionid
              headTailList.add((index + "&first", currentTuple._1 + "&" + sessionid))
            }
            //判断只有一条数据的情况
            if (list.size == 1) {
              //当前分区只有一条数据,不需要生成pageviewbeancase类型的数据,
              //添加数据到resulttuplelist中
              resultTupleList += ((currentTuple._2, sessionid, step, pagestaylong))
              //重新生成sessionid
              sessionid = UUID.randomUUID().toString
              //中止循环
              break()
            }

            //判断不止有一条数据的情况
            //第一条数据我们跳过从第二条开始遍历，
            //实现第一条数据continue的效果
            breakable {
              if (num == 0) {
                //说明是第一条
                break()
              }
              //从第二条开始判断
              //获取到上一条的数据然后两两比较   //生成sessionid需要uid和time字段即可生成
              val lastTuple: (String, WebLogBean) = list(num - 1)
              val currentUidTime: String = currentTuple._1
              val lastUidTime: String = lastTuple._1
              //取出uid和time
              //uid+"&"+time_local
              val currentUidTimeArr: Array[String] = currentUidTime.split("&")
              val lastUidTimeArr: Array[String] = lastUidTime.split("&")

              //计算时间差
              val timeDiff = DateUtil.getTimeDiff(lastUidTimeArr(1), currentUidTimeArr(1))
              //是不是同个用户
              if (lastUidTimeArr(0).equals(currentUidTimeArr(0)) && timeDiff < 30 * 60 * 1000) {
                //说明两条记录是同个session，保存上一条数据：sessionid,step,timediff
                resultTupleList += ((lastTuple._2, sessionid, step, timeDiff))
                //sessionid和step如何处置，sessionid无需重新生成，step必须要加1
                step += 1
              } else {
                //说明两条记录是不同的会话
                resultTupleList += ((lastTuple._2, sessionid, step, pagestaylong))
                //sessionid,step
                sessionid = UUID.randomUUID().toString
                //step重置
                step = 1
              }
              //考虑最后一条数据的输出问题
              if (num == list.size - 1) {
                //需要保存最后一条数据
                resultTupleList += ((currentTuple._2, sessionid, step, pagestaylong))
                //使用累加器收集最后一条数据,key:index+"&"last/first,value:uid+time+sessionid+step+partition.size
                headTailList.add((index + "&last", currentTuple._1 + "&" + sessionid + "&" + step + "&" + list.size))
                //sessionid
                sessionid = UUID.randomUUID().toString
              }

            }

          }

        }

        resultTupleList.toIterator
      }
    }
    sessionidStepPageRdd


  }

}

```

