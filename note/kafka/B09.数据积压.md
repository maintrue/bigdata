# 1 数据积压

Kafka消费者消费数据的速度是非常快的，但如果由于处理Kafka消息时，由于有一些外部IO、或者是产生网络拥堵，就会造成Kafka中的数据积压（或称为数据堆积）。如果数据一直积压，会导致数据出来的实时性受到较大影响。

## 1.1 使用Kafka-Eagle查看数据积压情况

![image](https://user-images.githubusercontent.com/75486726/180817196-e06a85b5-5125-4ad3-9182-519d5a946313.png)

![image](https://user-images.githubusercontent.com/75486726/180817242-d9c162d9-3f03-4aaf-a3be-888c0c1d565d.png)

![image](https://user-images.githubusercontent.com/75486726/180817307-700066ff-27b3-46d4-963b-b1c0ec700e7e.png)

![image](https://user-images.githubusercontent.com/75486726/180817354-7ab8ec75-dee9-444c-bd6a-594bd5dd345f.png)

## 1.2 解决数据积压问题
当Kafka出现数据积压问题时，首先要找到数据积压的原因。以下是在企业中出现数据积压的几个类场景。

### 1.2.1 数据写入MySQL失败
**问题描述**

某日运维人员找到开发人员，说某个topic的一个分区发生数据积压，这个topic非常重要，而且开始有用户投诉。运维非常紧张，赶紧重启了这台机器。重启之后，还是无济于事。

**问题分析**

消费这个topic的代码比较简单，主要就是消费topic数据，然后进行判断在进行数据库操作。运维通过kafka-eagle找到积压的topic，发现该topic的某个分区积压了几十万条的消息。

最后，通过查看日志发现，由于数据写入到MySQL中报错，导致消费分区的offset一自没有提交，所以数据积压严重。

### 1.2.2 因为网络延迟消费失败
**问题描述**

基于Kafka开发的系统平稳运行了两个月，突然某天发现某个topic中的消息出现数据积压，大概有几万条消息没有被消费。

**问题分析**

通过查看应用程序日志发现，有大量的消费超时失败。后查明原因，因为当天网络抖动，通过查看Kafka的消费者超时配置为50ms，随后，将消费的时间修改为500ms后问题解决。
