# 1 集群规划
| 服务器IP          | 192.168.174.100 | 192.168.174.110 | 192.168.174.120 |
| ----------------- | --------------- | --------------- | --------------- |
| 主机名             | node01          | node02          | node03          |
| NameNode          | 是              | 否              | 否              |
| SecondaryNameNode | 是              | 否              | 否              |
| dataNode          | 是              | 是              | 是              |
| ResourceManager   | 是              | 否              | 否              |
| NodeManager       | 是              | 是              | 是              |


# 2 上传apache hadoop包并解压
``` 
cd /export/softwares
tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/
```

# 3 修改配置文件

## 3.1 修改core-site.xml
第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi core-site.xml

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://server1:8020</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/servers/hadoop-2.7.5/hadoopDatas/tempDatas</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>4096</value>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>10080</value>
    </property>
</configuration>
```


## 3.2 修改hdfs-site.xml
第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi hdfs-site.xml

<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>server1:50090</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>server1:50070</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2</value>
    </property>
    <property>
        <name>dfs.namenode.edits.dir</name>
        <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/snn/name</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.edits.dir</name>
        <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hdfs-sockets/dn</value>
    </property>
    <property>
        <name>dfs.client.file-block-storage-locations.timeout.millis</name>
        <value>10000</value>
    </property>
    <property>
        <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
        <value>true</value>
    </property>
</configuration>
```

## 3.3 修改hadoop-env.sh
第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi hadoop-env.sh

export JAVA_HOME=/export/servers/jdk1.8.0_141
```


## 3.4 修改mapred-site.xml
第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi mapred-site.xml

<configuration>
    <property>
        <name>mapreduce.job.ubertask.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>server1:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>server1:19888</value>
    </property>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

## 3.5 修改yarn-site.xml
第一台机器执行以下命令
``` 
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>server1</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>20480</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
    </property>
</configuration>
```

## 3.6 修改mapred-env.sh
第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi mapred-env.sh

export JAVA_HOME=/export/servers/jdk1.8.0_141
```

## 3.7 修改slaves
修改slaves文件，然后将安装包发送到其他机器，重新启动集群即可

第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/etc/hadoop
vi slaves

node01
node02
node03
```

第一台机器执行以下命令
``` 
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2
mkdir  -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/edits
mkdir  -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/name
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits
mkdir -p /var/run/hdfs-sockets
```

安装包的分发,第一台机器执行以下命令
``` 
cd /export/servers/
scp -r hadoop-2.7.5 node02:$PWD
scp -r hadoop-2.7.5 node03:$PWD
```

# 4 配置hadoop的环境变量
三台机器都要进行配置hadoop的环境变量,三台机器执行以下命令
``` 
vi /etc/profile

export HADOOP_HOME=/export/servers/hadoop-2.7.5
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
source /etc/profile
```

# 5 启动集群
要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个模块。

注意：首次启动 HDFS 时，必须对其进行格式化操作。 本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。
``` 
hdfs namenode -format
或者
hadoop namenode –format
```

准备启动,第一台机器执行以下命令
``` 
cd /export/servers/hadoop-2.7.5/
bin/hdfs namenode -format
sbin/start-dfs.sh
sbin/start-yarn.sh
sbin/mr-jobhistory-daemon.sh start historyserver
```

三个端口查看界面

http://node01:50070/explorer.html#/ 查看hdfs

http://node01:8088/cluster 查看yarn集群

http://node01:19888/jobhistory 查看历史完成的任务

有时候hadoop起不来,查看日志报没有这个文件夹给创建一个
``` 
mkdir /var/run/hdfs-sockets
```


