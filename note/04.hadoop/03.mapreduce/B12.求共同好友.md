# 1 案例求共同好友

## 1.1 需求分析
以下是qq的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）
``` 
A:B,C,D,F,E,O
B:A,C,E,K
C:A,B,D,E,I
D:A,E,F,L
E:B,C,D,M,L
F:A,B,C,D,E,O,M
G:A,C,D,E,F
H:A,C,D,E,O
I:A,O
J:B,O
K:A,C,D
L:D,E,F
M:E,F,G
O:A,H,I,J
```
求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？

## 1.2 实现步骤

### 1.2.1 Mapper类

输出的bean要写个成员变量！！！
``` 
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;
public class FriendsMapper extends Mapper<LongWritable, Text,FriendsBean,Text> {
    private FriendsBean keyout = new FriendsBean();
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] split = value.toString().split(":");
        String mine = split[0];
        String[] friends = (mine+","+split[1]).split(",");
        for(int i=0;i<friends.length;i++){
            for(int j=i+1;j<friends.length;j++){
                String f1 = friends[i];
                String f2 = friends[j];
                if(f1.compareTo(f2) < 0){
                    keyout.setF1(f1);
                    keyout.setF2(f2);
                    context.write(keyout,new Text(mine));
                }else{
                    keyout.setF1(f2);
                    keyout.setF2(f1);
                    context.write(keyout,new Text(mine));
                }
            }
        }
    }
}
```

### 1.2.2 Reducer类
``` 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;
public class FriendsReducer extends Reducer<FriendsBean,Text,FriendsBean,Text> {
    @Override
    protected void reduce(FriendsBean key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        String sameFriends = "";
        for(Text value:values){
            if(!key.toString().contains(value.toString())){
                sameFriends += value;
            }
        }
        if(!"".equals(sameFriends)){
            context.write(key,new Text(sameFriends));
        }
    }
}
```

### 1.2.3 FriensBean
血的教训！！！

必须要实现WritableComparable接口！！！

必须要实现compareTo方法！！！

``` 
import org.apache.hadoop.io.WritableComparable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
public class FriendsBean implements WritableComparable<FriendsBean> {
    private String f1;
    private String f2;
    public FriendsBean(){}
    public FriendsBean(String f1, String f2) {
        this.f1 = f1;
        this.f2 = f2;
    }
    public String getF1() {
        return f1;
    }
    public void setF1(String f1) {
        this.f1 = f1;
    }
    public String getF2() {
        return f2;
    }
    public void setF2(String f2) {
        this.f2 = f2;
    }
    @Override
    public void write(DataOutput dataOutput) throws IOException {
        dataOutput.writeUTF(f1);
        dataOutput.writeUTF(f2);
    }
    @Override
    public void readFields(DataInput dataInput) throws IOException {
        this.f1 = dataInput.readUTF();
        this.f2 = dataInput.readUTF();
    }
    @Override
    public String toString() {
        return this.f1+this.f2;
    }
    @Override
    public int compareTo(FriendsBean o) {
        int i = this.f1.compareTo(o.getF1());
        if(i == 0){
            return this.f2.compareTo(o.getF2());
        }
        return i;
    }
}
```

### 1.2.4 JobMain
``` 
import com.main.constant.HadoopConf;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
public class FriendsMain extends Configured implements Tool {
    @Override
    public int run(String[] strings) throws Exception {
        Job job = Job.getInstance(new Configuration(), FriendsMain.class.getSimpleName());
        job.setJarByClass(FriendsMain.class);
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job,new Path(HadoopConf.FILE_PATH+"/friends/in"));
        job.setMapperClass(FriendsMapper.class);
        job.setMapOutputKeyClass(FriendsBean.class);
        job.setMapOutputValueClass(Text.class);
        job.setReducerClass(FriendsReducer.class);
        job.setOutputKeyClass(FriendsBean.class);
        job.setOutputValueClass(Text.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job,new Path(HadoopConf.FILE_PATH+"/friends/out"));
        boolean b = job.waitForCompletion(true);
        return b?0:1;
    }
    public static void main(String[] args) throws Exception {
        int run = ToolRunner.run(new Configuration(), new FriendsMain(), args);
        System.exit(run);
    }
}
```
