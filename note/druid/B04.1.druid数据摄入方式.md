# druid加载数据实战

### 4.1.1 批量（离线）摄取

#### 4.1.1.1 以索引服务方式摄取

index-service

![1558782367657](E:\大数据双元视频-druid\druid\day01\课件\1558782367657.png)

可以通过index-service的方式批量摄取数据，我们需要向overlord提交一个索引任务，overlord接受任务，通过zk将任务信息分配给middleManger,middlemanager领取任务后创建Peon进程，通过zk向overlord汇报任务状态；

案例:ad_event.json

```
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"guangzhou","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T05:01.00Z","city":"beijing","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T01:03.00Z","city":"guangzhou","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"beijing","platform":"mobile","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T03:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"guangzhou","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T05:03.00Z","city":"beijing","platform":"mobile","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"guangzhou","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T03:03.00Z","city":"beijing","platform":"mobile","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T05:01.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"mobile","click":"1"}
{"timestamp":"2018-12-01T01:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"guangzhou","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"mobile","click":"0"}
{"timestamp":"2018-12-01T01:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"pc","click":"1"}
{"timestamp":"2018-12-01T01:01.00Z","city":"guangzhou","platform":"pc","click":"0"}
{"timestamp":"2018-12-01T01:03.00Z","city":"beijing","platform":"mobile","click":"0"}
{"timestamp":"2018-12-02T01:01.00Z","city":"shanghai","platform":"pc","click":"0"}
{"timestamp":"2018-12-02T02:03.00Z","city":"beijing","platform":"pc","click":"0"}
```

本地批量索引任务规则文件

```
{
    "type": "index",
    "spec": {
        "ioConfig": {
            "type": "index",
            "firehose": {
                "type": "local",
                "baseDir": "/data/",
                "filter": "ad_event.json"
            }
        },
        "dataSchema": {
            "dataSource": "ad_event_local",
            "granularitySpec": {
                "type": "uniform",
                "segmentGranularity": "day",
                "queryGranularity": "hour",
                "intervals": [
                    "2018-12-01/2018-12-03"
                ]
            },
            "parser": {
                "type": "String",
                "parseSpec": {
                    "format": "json",
                    "dimensionsSpec": {
                        "dimensions": [
                            "city",
                            "platform"
                        ]
                    },
                    "timestampSpec": {
                        "format": "auto",
                        "column": "timestamp"
                    }
                }
            },
            "metricsSpec": [
                {
                    "name": "count",
                    "type": "count"
                },
                {
                    "name": "click",
                    "type": "longSum",
                    "fieldName": "click"
                }
            ]
        },
        "tuningConfig": {
            "type": "index",
            "partitionsSpec": {
                "type": "hashed",
                "targetPartitionSize": 5000000
            }
        }
    }
}
```

提交本地批量索引任务

```
curl -X 'POST' -H 'Content-Type:application/json' -d @index-local.json hp101:8090/druid/indexer/v1/task
```

注意：要保证数据文件在middlemanager所在节点！

查询规则文件

```
{
    "queryType":"timeseries",
    "dataSource":"ad_event_local",
    "granularity":{"type": "period", "period": "PT1H", "timeZone": "Asia/Shanghai"},
    "aggregations":[
        {
            "type":"longSum",
            "name":"click",
            "fieldName":"click"
        },{
            "type":"longSum",
            "name":"pv",
            "fieldName":"count"
        }
    ],
    "intervals":["2018-06-02/2019-06-06"]
}
```

提交查询

```
curl -X 'POST'  -H'Content-Type: application/json'  -d @query-adtest.json http://hp103:8082/druid/v2/?pretty
```



#### 4.1.1.2 以MR任务方式摄-HadoopDruidIndexer

使用HadoopDruidIndexer加载批量数据时，会启动MapReduce任务，将数据生成segments文件，存放在HDFS上，同时向Druid metastore中写入segments元数据。Coordinator Node监控元数据中有新增的segments，会将指令写入Zookeeper，而Historical Node监控到Zookeeper中的指令之后，从HDFS上下载segments文件到本地。之后，该批量数据便可从Druid中查询。

数据文件

```
{"time":"2015-09-12T00:47:00.496Z","channel":"#ca.wikipedia","cityName":null,"comment":"Robot inserta {{Commonscat}} que enllaça amb [[commons:category:Rallicula]]","countryIsoCode":null,"countryName":null,"isAnonymous":false,"isMinor":true,"isNew":false,"isRobot":true,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Rallicula","regionIsoCode":null,"regionName":null,"user":"PereBot","delta":17,"added":17,"deleted":0}
{"time":"2015-09-12T00:47:05.474Z","channel":"#en.wikipedia","cityName":"Auburn","comment":"/* Status of peremptory norms under international law */ fixed spelling of 'Wimbledon'","countryIsoCode":"AU","countryName":"Australia","isAnonymous":true,"isMinor":false,"isNew":false,"isRobot":false,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Peremptory norm","regionIsoCode":"NSW","regionName":"New South Wales","user":"60.225.66.142","delta":0,"added":0,"deleted":0}
{"time":"2015-09-12T00:47:08.770Z","channel":"#vi.wikipedia","cityName":null,"comment":"fix Lỗi CS1: ngày tháng","countryIsoCode":null,"countryName":null,"isAnonymous":false,"isMinor":true,"isNew":false,"isRobot":true,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Apamea abruzzorum","regionIsoCode":null,"regionName":null,"user":"Cheers!-bot","delta":18,"added":18,"deleted":0}
{"time":"2015-09-12T00:47:11.862Z","channel":"#vi.wikipedia","cityName":null,"comment":"clean up using [[Project:AWB|AWB]]","countryIsoCode":null,"countryName":null,"isAnonymous":false,"isMinor":false,"isNew":false,"isRobot":true,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Atractus flammigerus","regionIsoCode":null,"regionName":null,"user":"ThitxongkhoiAWB","delta":18,"added":18,"deleted":0}
{"time":"2015-09-12T00:47:13.987Z","channel":"#vi.wikipedia","cityName":null,"comment":"clean up using [[Project:AWB|AWB]]","countryIsoCode":null,"countryName":null,"isAnonymous":false,"isMinor":false,"isNew":false,"isRobot":true,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Agama mossambica","regionIsoCode":null,"regionName":null,"user":"ThitxongkhoiAWB","delta":18,"added":18,"deleted":0}
{"time":"2015-09-12T00:47:17.009Z","channel":"#ca.wikipedia","cityName":null,"comment":"/* Imperi Austrohongarès */","countryIsoCode":null,"countryName":null,"isAnonymous":false,"isMinor":false,"isNew":false,"isRobot":false,"isUnpatrolled":false,"metroCode":null,"namespace":"Main","page":"Campanya dels Balcans (1914-1918)","regionIsoCode":null,"regionName":null,"user":"Jaumellecha","delta":-20,"added":0,"deleted":20}
```



index-hadoop规则文件

```
{
    "type": "index_hadoop",
    "spec": {
        "dataSchema": {
            "dataSource": "wikiticker",
            "parser": {
                "type": "hadoopyString",
                "parseSpec": {
                    "format": "json",
                    "dimensionsSpec": {
                        "dimensions": [
                            "channel",
                            "cityName",
                            "comment",
                            "countryIsoCode",
                            "countryName",
                            "isAnonymous",
                            "isMinor",
                            "isNew",
                            "isRobot",
                            "isUnpatrolled",
                            "metroCode",
                            "namespace",
                            "page",
                            "regionIsoCode",
                            "regionName",
                            "user"
                        ]
                    },
                    "timestampSpec": {
                        "format": "auto",
                        "column": "time"
                    }
                }
            },
            "metricsSpec": [
                {
                    "name": "count",
                    "type": "count"
                },
                {
                    "name": "added",
                    "type": "longSum",
                    "fieldName": "added"
                },
                {
                    "name": "deleted",
                    "type": "longSum",
                    "fieldName": "deleted"
                },
                {
                    "name": "delta",
                    "type": "longSum",
                    "fieldName": "delta"
                }
            ],
            "granularitySpec": {
                "type": "uniform",
                "segmentGranularity": "day",
                "queryGranularity": "none",
                "intervals": [
                    "2015-09-12/2015-09-13"
                ],
                "rollup": false
            }
        },
        "ioConfig": {
            "type": "hadoop",
            "inputSpec": {
                "type": "static",
                "paths": "/wikiticker-2015-09-12-sampled.json"
            }
        },
        "tuningConfig": {
            "type": "hadoop",
            "partitionsSpec": {
                "type": "hashed",
                "targetPartitionSize": 5000000
            },
            "jobProperties": {
                "fs.default.name": "hdfs://hp101:9000",
                "fs.defaultFS": "hdfs://hp101:9000",
                "dfs.datanode.address": "hp102",
                "dfs.client.use.datanode.hostname": "true",
                "dfs.datanode.use.datanode.hostname": "true",
                "yarn.resourcemanager.hostname": "hp102",
                "yarn.nodemanager.vmem-check-enabled": "false",
                "mapreduce.map.java.opts": "-Duser.timezone=UTC -Dfile.encoding=UTF-8",
                "mapreduce.job.user.classpath.first": "true",
                "mapreduce.reduce.java.opts": "-Duser.timezone=UTC -Dfile.encoding=UTF-8",
                "mapreduce.map.memory.mb": 1024,
                "mapreduce.reduce.memory.mb": 1024
            }
        }
    },
    "hadoopDependencyCoordinates": [
        "org.apache.hadoop:hadoop-client:2.7.3"
    ]
}
```

注意：hadoopdependencyCoordinate：指定为你当前hadoop集群版本，如果与druid依赖版本不一致，需要使用命令去下载你的集群版本对应的hadoop-client.要保证数据文件存储在hdfs上并且路径正确。

提交hadoop-druid索引任务

```
curl -X 'POST' -H 'Content-Type:application/json' -d @hadoop-index.json hp101:8090/druid/indexer/v1/task
```

查询规则文件

```
{
  "queryType" : "topN",
  "dataSource" : "wikiticker",
  "intervals" : ["2015-09-12/2015-09-13"],
  "granularity" : "day",
  "dimension" : "page",
  "metric" : "edits",
  "threshold" : 25,
  "aggregations" : [
    {
      "type" : "longSum",
      "name" : "edits",
      "fieldName" : "count"
    }
  ]
}
```

提交查询任务：

```
curl -X 'POST'  -H'Content-Type: application/json'  -d @query-wikiticker.json http://hp103:8082/druid/v2/?pretty
```



### 4.1.2 流式（实时）摄取

####  Kafka-indexing-service（官方推荐使用）

之前的流式摄取方式realtimenode ，tranquility与kafka-indexing-service对比，该种方式的优势有哪些？

realTime node存在单点问题，并且不能有效扩展，官方是不建议生产环境使用的，新版本已经完全弃用。

Tranqulity方式：需要启动一个扩展程序改程序从数据源（kafka等）读取数据后推送给druid集群，但是这种方式存在一些弊端，由于其架构设计中存在一个时间窗口的概念，很容易出现数据到来时与窗口不匹配导致数据被丢弃，所以会造成数据丢失，所以官方建议如果使用Tranqulity方式实时摄取数据时需要使用离线数据导入每天修正数据否则可能计算结果有误。

Kafka-indexing-service方式：

1：引入SuperVisor，用于管理实时任务(peon)的生命周期，包括任务的启动，停止，副本管理，失败任务恢复等

2：实时任务主动消费kafka数据，不需要想Tranqulity维护一个推送程序，

3：实时任务使用kafka低阶api,自己保存kafka offset，提升了数据的可靠性，

4：不丢弃延时数据；

Supervisor就是overlord节点启动的一个线程，该线程负责一个实时摄取任务的完整过程，创建新的任务，以及补足副本数量等，

如何做到不丢弃延迟数据？

一个实时任务不在只是生成一个时间范围的segment,而是根据收到的数据的事件时间来确定自己属于哪个segment,如果所属segment已经创建，则新建一个segment的分片将数据写入该分片中实现延迟数据的追加。

配置文件修改：/export/servers/druid010/conf/druid/_common/common.runtime.properties

druid.extensions.loadList=["druid-kafka-indexing-service",  "druid-datasketches", "mysql-metadata-storage","druid-hdfs-storage"]

案例：

```
{
    "type": "kafka",
    "dataSchema": {
        "dataSource": "kafkatest",
        "parser": {
            "type": "string",
            "parseSpec": {
                "format": "json",
                "timestampSpec": {
                    "column": "timestamp",
                    "format": "auto"
                },
                "dimensionsSpec": {
                    "dimensions": [
                        "city",
                        "platform"
                    ]
                }
            }
        },
        "metricsSpec": [
            {
                "name": "count",
                "type": "count"
            },
            {
                "name": "click",
                "type": "longSum",
                "fieldName": "click"
            }
        ],
        "granularitySpec": {
            "type": "uniform",
            "segmentGranularity": "DAY",
            "queryGranularity": "NONE",
            "rollup": false
        }
    },
    "tuningConfig": {
        "type": "kafka",
        "reportParseExceptions": false
    },
    "ioConfig": {
        "topic": "adtest",
        "replicas": 2,
        "taskDuration": "PT10M",
        "completionTimeout": "PT20M",
        "consumerProperties": {
            "bootstrap.servers": "hp101:9092,hp102:9092"
        }
    }
}
```



提交kafka索引任务

```
curl -X POST -H 'Content-Type: application/json' -d @kafka_test_index.json http://hp101:8090/druid/indexer/v1/supervisor
```



启动kafka控制台版本生产者

```
 bin/kafka-console-producer.sh --topic adtest --broker-list hp101:9092
```

发送数据：

```
{"timestamp":"1559736155453","city":"beijing","platform":"pc","click":"0"}
```



查询规则文件

```
{
    "queryType":"timeseries",
    "dataSource":"kafkatest",
    "granularity":{"type": "period", "period": "P1D", "timeZone": "Asia/Shanghai"},
    "aggregations":[
        {
            "type":"longSum",
            "name":"click",
            "fieldName":"click"
        },{
            "type":"longSum",
            "name":"pv",
            "fieldName":"count"
        }
    ],
    "intervals":["2018-06-02/2019-06-06"]
}
```

提交查询

```
curl -X 'POST'  -H'Content-Type: application/json'  -d @query-kafkatest.json http://hp103:8082/druid/v2/?pretty
```

