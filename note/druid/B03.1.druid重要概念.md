---
typora-copy-images-to: assets
typora-root-url: assets
---

### 3.1.1 roll-up聚合

druid可以汇总原始数据。汇总是就是把选定的相同维度的数据进行聚合操作，可减少存储的大小。 druid要求数据具有如下要求，数据分为三部分：时间戳，维度列，指标列

- **Timestamp列**: 所有的查询都以时间为中心。
- **Dimension列（维度）**: Dimensions对应事件的维度,通常用于筛选过滤数据。
- **Metric列（度量）**: Metrics是用于聚合和计算的列。通常是数字,并且支持count、sum等聚合操作。

比如：

{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024}
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133}
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780}
{"timestamp":"2018-01-01T01:02:14Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":38,"bytes":6289}
{"timestamp":"2018-01-01T01:02:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":377,"bytes":359971}
{"timestamp":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":49,"bytes":10204}
{"timestamp":"2018-01-02T21:33:14Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":38,"bytes":6289}
{"timestamp":"2018-01-02T21:33:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":123,"bytes":93999}
{"timestamp":"2018-01-02T21:35:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":12,"bytes":2818}

如果该数据被druid加载并且我们打开了聚合功能，聚合的要求是packets和bytes对应的数据进行累加，并且对条数进行计数（具体设置操作后续加载数据时讲解），聚合后的数据如下：

```
┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐
│ __time                   │ bytes  │ count │ dstIP   │ packets │ srcIP   │
├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤
│ 2018-01-01T01:01:00.000Z │  35937 │     3 │ 2.2.2.2 │     286 │ 1.1.1.1 │
│ 2018-01-01T01:02:00.000Z │ 366260 │     2 │ 2.2.2.2 │     415 │ 1.1.1.1 │
│ 2018-01-01T01:03:00.000Z │  10204 │     1 │ 2.2.2.2 │      49 │ 1.1.1.1 │
│ 2018-01-02T21:33:00.000Z │ 100288 │     2 │ 8.8.8.8 │     161 │ 7.7.7.7 │
│ 2018-01-02T21:35:00.000Z │   2818 │     1 │ 8.8.8.8 │      12 │ 7.7.7.7 │
└──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘
```

### 3.2.2 列式存储与行式存储

列式存储(Columnar or column-based)是相对于传统关系型数据库的行式存储(Row-basedstorage)来说的。简单来说两者的区别就是如何组织表：

![1558927757344](https://user-images.githubusercontent.com/75486726/178937621-0647ca7a-ca33-415b-b396-0b0b960e5bb0.png)

如图所示，上图就是两种不同的存储模式在底层的表现，

思考：基于上述不同的存储模式对于查询来说会有哪些不一样之处呢？列式存储为何对OLAP来说更有优势，行式存储对于OLTP更适合呢？

行式存储对于 OLTP 场景是很自然的：大多数操作都以实体（Entity）为单位，即大多为增删改查一整行记录，显然把一行数据存在物理上相邻的位置是个很好的选择。 

对于 OLAP 场景，一个典型的查询需要遍历整个表，进行分组、排序、聚合等操作，这样一来按行存储的优势就不复存在了。更糟糕的是，分析型 SQL 常常不会用到所有的列，而仅仅对其中某些感兴趣的列做运算，那一行中无关的列也不得不参与扫描，因此列式存储更适合这种场景。

### 3.2.3 Datasource和segments

Druid的数据被保存在datasource里面， DataSource类似于关系型数据库中的table。所有的DataSource是按照时间来分片的，每个时间区间范围被称为一个chunk（比如当你的DataSource是按天来分片的，一天就是一个chunk）。在chunk内部，数据被进一步分片成一个或多个segment。所有的segment是一个单独的文件，通常一个segment会包含数百万行数据。segment和chunk的关系示意图如下： 

![trunk与datasource关系图](https://user-images.githubusercontent.com/75486726/178937708-cb71bb1b-b398-4c44-b9d5-08139b8d906e.png)

 Segment是Druid中最基本的数据存储单元，采用列式(columnar)存储某一个时间间隔(interval)内某一个数据源(dataSource)的部分数据所对应的所有维度值、度量值、时间维度以及索引。

#### **Segment存储结构**

Segment逻辑名称形如“datasource_intervalStart_intervalEnd_version_partitionNum”，

dataSource：数据源；

intervalStart、intervalEnd：时间间隔的起止，使用ISO-8601格式；

version：版本号，使用导入的系统时间；

partitionNumber：分区编号，在每个时间间隔内，根据数据量的大小一个Segment内部可能会有多个分区，

数据以segments(段)的形式存储就是druid的分片，Segments是自包含容器，包括基于列的压缩,以及这些列的索引，对应下面要介绍的存储的descriptor.json和index.zip。Druid只需要清楚如何扫描这些segments就可以查询。



```
我们验证我们单机版导入的数据查看：cd /export/servers/druid/var/druid/segments/wikiticker/2015-09-12T00:00:00.000Z_2015-09-13T00:00:00.000Z/2019-05-16T15:50:40.729Z/0
目录下有两个文件：

```

```
-rw-r--r--. 1 root root     685 5月  16 11:51 descriptor.json
-rw-r--r--. 1 root root 2465463 5月  16 11:51 index.zip
```

descriptor.json保存的信息：

```
{
    "dataSource": "wikiticker", //数据源
    "interval": "2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z",//时间区间
    "version": "2019-05-16T15:50:40.729Z",//版本号，修改时间
    "loadSpec": {//存储路径
        "type": "local",
        "path": "/export/servers/druid/var/druid/segments/wikiticker/2015-09-12T00:00:00.000Z_2015-09-13T00:00:00.000Z/2019-05-16T15:50:40.729Z/0/index.zip"
    },
    "dimensions": "channel,cityName,comment,countryIsoCode,countryName,isAnonymous,isMinor,isNew,isRobot,isUnpatrolled,metroCode,namespace,page,regionIsoCode,regionName,user",//维度列
    "metrics": "count,added,deleted,delta,user_unique",//指标列
    "shardSpec": {//分片信息
        "type": "none"
    },
    "binaryVersion": 9,//数据格式的版本号，druid内部使用的，新老版本由差别
    "size": 5537818,//index.zip解压后的文件大小
    "identifier": "wikiticker_2015-09-12T00:00:00.000Z_2015-09-13T00:00:00.000Z_2019-05-16T15:50:40.729Z"
}

```

index.zip文件中包括了该segment中的数据。它包含了以下三类文件：

version.bin：Segment内部标识数据结构的版本号

xxxxx.smoosh：xxxxx从0开始编号，最大为2G，是为了满足Java内存文件映射MapperByteBuffer限制。就是druid存储的聚合后的数据以及索引数据。

meta.smoosh文件

![1558511825182](https://user-images.githubusercontent.com/75486726/178937960-81a43335-5a05-47a2-acc9-2261a66c1c7c.png)

meta.smoosh: 该文件用于记录该segment数据的元信息。文件格式为csv。包含两部分：

第一行为文件头：

其中v1代表segment的版本号，2147483647(2GB)为xxxxx.smoosh文件的最大大小。1表示smoosh文件的数量，即index.zip文件中xxxxx.smoosh文件的个数。

从第二行开始为文件体：

每行都是4列，column表示列名，smooshid表示分片编号，从0开始编号，对于包含多个xxxxx.smoosh文件的index.zip，meta.smooth文件会记载多个xxxxx.smoosh文件中每列的元信息。startPos为该列在第smooshid分片中的开始位置，endPos为其结束位置。

#### 位图索引

![1558513542175](https://user-images.githubusercontent.com/75486726/178938085-ab2cc3e9-1498-46c5-9f76-15aec8d8ac66.png)

第一列为时间。Appkey和area都是维度列。value为metric列。
Druid会在导入阶段对数据进行Rollup，将维度相同组合的数据进行聚合处理。Rollup会使用其设定的聚合器进行聚合。

按天聚合后的数据如下（聚合后的DataSource为AD_areauser):

![1558513964340](https://user-images.githubusercontent.com/75486726/178938167-3993225c-fae8-4787-b4dd-0c973396b2ba.png)

对于聚合后的数据，如何对表建立索引，快速的进行查找？
我们知道数据库常用的索引是利用B+树建立联合索引，但是在以上数据中，比如按area进行查找，由于该列的基数非常低，这样无论该表有多少行，B+数的叶子结点都比较少，所以查找索引的效率很低，并不一定比得上全表扫描。
那么如何通过建立索引来解决这类问题呢？答案是建立位图索引。

索引如下所示：

![1558516446315](https://user-images.githubusercontent.com/75486726/178938243-afa31e8b-59b6-4f92-9b98-5b7be5a28504.png)

其实索引位图可以看作是HashMap<String, Bitmap>。该map中的key就是维度的取值，value就是该表中对应的行是否有该维度的值。

以SQL查询为例：
1）boolean条件查询：

Select sum(value) from AD_areauser where time=’2017-10-11’and Appkey in (‘appkey1’,’appkey2’) and area=’北京’

首先根据时间段定位到segment，然后根据Appkey=’appkey2’ and area=’北京’查到各自的bitmap：
(appkey1(1000) or appkey2(0110)) and 北京(1100) = (1100)
也就是说，符合条件的列是第一行和第二行，这两行的metric（value）的和为26.

2）group by 查询：
select area, sum(value) from AD_areauser where time=’2017-10-11’and Appkey in (‘appkey1’,’appkey2’) group by area

该查询与上面的查询不同之处在于将符合条件的列
appkey1(1000) or appkey2(0110) = (1110)

取出来，然后在内存中做分组聚合。结果为：北京：26， 上海：13.

