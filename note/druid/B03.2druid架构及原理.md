## 3.3 druid架构及原理

### 3.3.1druid的架构

![](E:\大数据双元视频-druid\druid\day01\课件\assets\druid架构图.png)

主要的节点包括（PS: Druid 的所有功能都在同一个软件包中，通过不同的命令启动）：

- **Coordinator** 节点：负责集群 Segment 的管理和发布，并确保 Segment 在 Historical 集群中的负载均衡

- **Broker** 节点：负责从客户端接收查询请求，并将查询请求转发给 Historical 节点和 MiddleManager 节点。Broker 节点需要感知 Segment 信息在集群上的分布

- **Historical** 节点：负责按照规则加载Segment并提供历史数据的查询

- **Router**(可选) 节点：可选节点，在 Broker 集群之上的API网关，有了 Router 节点 Broker 不在是单点服务了，提高了并发查询的能力

- **Indexing Service**

  Indexing Service顾名思义就是指索引服务，在索引服务生成segment的过程中，由Overlord Node接收加载任务，然后生成索引任务（Index Service）并将任务分发给多个MiddleManager节点，MiddleManager节点根据索引协议生成多个Peon，Peon将完成数据的索引任务并生成segment，并将segment提交到分布式存储里面（一般是HDFS），然后Coordinator节点感知到segment生成，给Historical节点分发下载任务，Historical节点从分布式存储里面下载segment到本地。

  **Overlord**

  Overlord Node负责segment生成的任务，并提供任务的状态信息，当然原理跟上面类似，也在Zookeeper中对应的目录下，由实际执行任务的最小单位在Zookeeper中同步更新任务信息，类似于回调函数的执行过程。跟Coordinator Node一样，它在集群里面一般只存在一个，如果存在多个Overlord Node，Zookeeper会根据选举算法（一致性算法避免脑裂）产生一个Leader，其余的当Follower，当Leader遇到问题宕机时，Zookeeper会在Follower中再次选取一个Leader，从而维持集群生成segment服务的正常运行。Overlord Node会将任务分发给MiddleManager Node，由MiddleManager Node负责具体的segment生成任务。

  **MiddleManager**

  Overlord Node会将任务分发给MiddleManager Node，所以MiddleManager Node会在Zookeeper中感知到新的索引任务。一但感知到新的索引任务，会创建Peon（segment具体执行者，也是索引过程的最小单位）来具体执行索引任务，一个MiddleManager Node会运行很多个Peon的实例。

  **Peon**

  Peon（segment生成任务的具体执行者，也是索引过程的最小单位），所有的Peon都会在Zookeeper对应的目录中实时更新自己的任务状态。

  简化版：

  coordinator-Master节点，管理集群的数据视图，segment的load与drop;

  historical :历史节点，负责历史窗口内数据的查询；

  broker:查询节点，查询拆分，结果汇聚；

  indexing service-一套实时/批量数据导入任务的调度服务

  ​	overlord-调度服务的master节点，负责接收任务，管理任务状态；

  ​	middleManager-worker节点，接收任务启动任务；

  ​	peon-实际的任务进程

  

  #### 外部依赖

  深度存储服务(Deep storage)：深度存储服务是能够被每个Druid服务都能访问的共享文件系统，一般是分布式对象存储服务，用于存放Druid所有摄入的数据。比如S3、HDFS或网络文件系统。

  元数据存储(Metadata store)：元数据存储服务主要用于存储Druid中的一些元数据，比如segment的相关信息。一般是传统的RDMS，比如Mysql。

  Zookeeper：用于内部服务发现、协调和leader选举的。