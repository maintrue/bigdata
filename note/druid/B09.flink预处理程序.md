flink预处理程序

1:创建样例类来封装数据

AdClicklog

```
package com.itheima.realprocess.bean

import com.alibaba.fastjson.JSON
import org.apache.commons.lang3.StringUtils

case class AdClickLog(
                       city:String,
                       ad_compaign: String,
                       ad_media: String,
                       ad_source: String,
                       corpuin: String,
                       device_type: String,
                       host: String,
                       t_id: String,
                       user_id: String,
                       click_user_id: String,
                       timestamp: String
                     )

object AdClickLog {

  def apply(json: String) = {
    //使用FastJSON的JSON.parseObject方法将JSON字符串构建一个ClickLog实例对象
    //{\"ad_compaign\":\"风系列\",\"ad_media\":\"m1\",\"ad_source\":\"s2\",\"corpuin\":18,
    // \"device_type\":\"mobile\",\"host\":\"google\",\"is_new\":1,\"t_id\":5,\"timestamp\":1559092939197}
    val jsonObject = JSON.parseObject(json)
    var click_user_id = jsonObject.getString("click_user_id")
    if (StringUtils.isBlank(click_user_id)) {
      click_user_id = "null"
    }
    new AdClickLog(
      jsonObject.getString("city"),
      jsonObject.getString("ad_compaign"),
      jsonObject.getString("ad_media"),
      jsonObject.getString("ad_source"),
      jsonObject.getString("corpuin"),
      jsonObject.getString("device_type"),
      jsonObject.getString("host"),
      jsonObject.getString("t_id"),
      jsonObject.getString("user_id"),
      click_user_id,
      jsonObject.getString("timestamp")
    )
  }
}
```

准备好修改GalobalUtil,获取消费的topic,添加flink消费的topic,以及生产消息的topic,在GlobalConfig中获取topic;

先去配置文件中添加topic信息，然后在GlobalConfigUtils中添加获取方法

flink的任务程序

```
package com.itheima.realprocess

import java.text.SimpleDateFormat
import java.util.Properties

import com.alibaba.fastjson.JSON
import com.itheima.realprocess.bean.AdClickLog
import com.itheima.realprocess.task._
import com.itheima.realprocess.util.GlobalConfigUtil
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.api.scala._
import org.apache.flink.runtime.state.filesystem.FsStateBackend
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.watermark.Watermark
import org.apache.flink.streaming.api.{CheckpointingMode, TimeCharacteristic}
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer09, FlinkKafkaProducer09}
import org.json4s.DefaultFormats
import org.json4s.native.Serialization.write

object AdApp {
  def main(args: Array[String]): Unit = {
    // 创建main方法，获取StreamExecutionEnvironment运行环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    // 设置流处理的时间为EventTime，使用数据发生的时间来进行数据处理
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    // 将Flink默认的开发环境并行度设置为1
    env.setParallelism(1)

    // 保证程序长时间运行的安全性进行checkpoint操作
    //
    // 5秒启动一次checkpoint
    env.enableCheckpointing(5000)
    // 设置checkpoint只checkpoint一次
    env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)
    // 设置两次checkpoint的最小时间间隔
    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(1000)
    // checkpoint超时的时长
    env.getCheckpointConfig.setCheckpointTimeout(60000)
    // 允许的最大checkpoint并行度
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)
    // 当程序关闭的时，触发额外的checkpoint
    env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)
    // 设置checkpoint的地址
    env.setStateBackend(new FsStateBackend("hdfs://hp101:9000/flink-checkpoints/"))

    //
    // 整合Kafka
    //
    val properties = new Properties()
    properties.setProperty("bootstrap.servers", GlobalConfigUtil.bootstrapServers)
    properties.setProperty("zookeeper.connect", GlobalConfigUtil.zookeeperConnect)
    properties.setProperty("group.id", GlobalConfigUtil.groupId)
    properties.setProperty("enable.auto.commit", GlobalConfigUtil.enableAutoCommit)
    properties.setProperty("auto.commit.interval.ms", GlobalConfigUtil.autoCommitIntervalMs)

    // 配置下次重新消费的话，从哪里开始消费
    // latest：从上一次提交的offset位置开始的
    // earlist：从头开始进行（重复消费数据）
    properties.setProperty("auto.offset.reset", GlobalConfigUtil.autoOffsetReset)
    // 配置序列化和反序列化
    properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")

    val consumer: FlinkKafkaConsumer09[String] = new FlinkKafkaConsumer09[String](
      GlobalConfigUtil.adTopic,
      new SimpleStringSchema(),
      properties
    )

    val kafkaDataStream: DataStream[String] = env.addSource(consumer)
    // 使用map算子，将kafka中消费到的数据
    val messageDataStream = kafkaDataStream.map {
      msgJson =>
        // 使用FastJSON转换为JSON对象
        val jsonObject = JSON.parseObject(msgJson)
        val count = jsonObject.getLong("count")
        val message = jsonObject.getString("message")
        val timestamp = jsonObject.getLong("timestamp")
        print(message)
        // 将JSON的数据解析封装到Message样例类中
        // 将数据封装到ClickLog样例类
        AdClickLog(message)
    }
    messageDataStream.print()
    // 添加flink的水印处理 , 允许得最大延迟时间是2S
    val watermarkDataStream: DataStream[AdClickLog] = messageDataStream.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[AdClickLog] {
      var currentTimestamp: Long = 0L
      val maxDelayTime = 2000L
      val df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ")

      var watermark: Watermark = _

      // 获取当前的水印
      override def getCurrentWatermark = {
        watermark = new Watermark(currentTimestamp - maxDelayTime)
        watermark
      }

      // 时间戳抽取操作
      override def extractTimestamp(t: AdClickLog, l: Long) = {
        val timeStamp = t.timeStamp
        currentTimestamp = Math.max(df.parse(timeStamp).getTime, currentTimestamp)
        currentTimestamp
      }
    })
    // 在App中调用预处理任务的process方法，并打印测试
    val clicklogWideDataStream = PreprocessTask.processAd(watermarkDataStream)
    val value: DataStream[String] = clicklogWideDataStream.map {
      x =>
        implicit val formats = DefaultFormats
        val jsonString = write(x)
        jsonString
    }
    value.addSink(
      new FlinkKafkaProducer09[String](
        GlobalConfigUtil.bootstrapServers,
        GlobalConfigUtil.adProcessTopic,
        new SimpleStringSchema()
      )
    )
    //    clicklogWideDataStream.addSink()

    env.execute("RealProcessApp")
  }
}

```

添加process方法对数据进行预处理，主要是判断用户是否为新用户，以及添加点击次数字段

```
package com.itheima.realprocess.task

import com.itheima.realprocess.bean.{AdClickLog, AdClickLogWide, ClickLogWide, Message}
import com.itheima.realprocess.util.HBaseUtil
import org.apache.commons.lang3.StringUtils
import org.apache.commons.lang3.time.FastDateFormat
import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.scala.DataStream

/**
  * 预处理的任务
  */
object PreprocessTask {

  // 包装isNew字段的样例类
  case class IsNewWrapper(isNew: Int, isHourNew: Int, isDayNew: Int, isMonthNew: Int)

  //处理广告点击数据，增加isNew字段
  def processAd(watermarkDataStream: DataStream[AdClickLog]) = {
    watermarkDataStream.map {
      msg =>
        val isNew = analysisNew(msg)
        var click_cnt = 0
        if (!msg.click_user_id.equalsIgnoreCase("null")) {
          click_cnt = 1
        }
        AdClickLogWide(
          msg.city,
          msg.ad_compaign,
          msg.ad_media,
          msg.ad_source,
          msg.corpuin,
          msg.device_type,
          msg.host,
          msg.t_id,
          msg.user_id,
          msg.click_user_id,
          msg.timeStamp,
          isNew,
          click_cnt
        )
    }
  }

  /*
  判断用户是否为新用户
   */
  def analysisNew(adlog: AdClickLog) = {
    //先把要拓宽的字段isNew、isHourNew、isDayNew、isMonthNew都创建出来，初始化为0
    var isNew = 0
    // 封装操作hbase需要的字段
    val tableName = "user_history"
    val rowkey = adlog.user_id
    val cfName = "info"
    val userIdColName = "userId" // 用户ID
    //从hbase查询rowkey为userid:channlid查询user_history中userid列的数据
    val userIdInHBase = HBaseUtil.getData(tableName, rowkey, cfName, userIdColName)
    //判断userid列数据是否为空
    if (StringUtils.isBlank(userIdInHBase)) {
      //如果为空
      //设置isNew字段为1，表示是新用户，
      isNew = 1
      //将该用户数据添加到user_history表中
      HBaseUtil.putMapData(tableName, rowkey, cfName, Map(
        userIdColName -> adlog.user_id
      ))
    }
    isNew
  }
  
}

```

